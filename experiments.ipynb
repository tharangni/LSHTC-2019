{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/gcdart/MulticlassClassifier/blob/master/src/ml/LogisticRegression.java\n",
    "# https://www.kaggle.com/c/lshtc/discussion/6911#38233 - preprocessing: multilabels comma should not have spaces\n",
    "# https://www.kaggle.com/c/lshtc/discussion/14048 - dataset statistics\n",
    "## reading the LWIKI, SWIKI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import igraph as ig\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm #always use this instead of `import tqdm`\n",
    "# from sklearn.datasets import fetch_rcv1\n",
    "from scipy.sparse import *\n",
    "# np.random.seed(123)\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from collections import OrderedDict\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import preprocessing, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Memory\n",
    "from sklearn.datasets import load_svmlight_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = Memory(\"./mycache\")\n",
    "@mem.cache\n",
    "def get_data(filename):\n",
    "    \n",
    "    fname = str(Path(filename))\n",
    "    fe, ex = os.path.splitext(fname) \n",
    "\n",
    "    try:\n",
    "        data = load_svmlight_file(fname, multilabel=True)\n",
    "    except:\n",
    "        # Required: if the input data isn't in the correct libsvm format\n",
    "        outfile = str(Path(\"{}_small{}\".format(fe, ex)))\n",
    "#         outfile = str(Path(\"{}_remapped{}\".format(fe, ex)))\n",
    "        if not os.path.isfile(outfile):\n",
    "            logging.info(\"Remapping data to LibSVM format...\")\n",
    "            f = preprocess_libsvm(fname, outfile)\n",
    "        else:\n",
    "            logging.info(\"Using already remapped data...\")\n",
    "            f = outfile\n",
    "        data = load_svmlight_file(f, multilabel=True)\n",
    "        \n",
    "    return data[0], data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_libsvm(input_file, output_file):\n",
    "    # converts file to the required libsvm format.\n",
    "    # this is very brute force but can be made faster [IMPROVE]\n",
    "\n",
    "    file = open(output_file, \"w+\")\n",
    "    with open(input_file, \"r\") as f:\n",
    "        head = [next(f) for x in range(500)] # retrieve only `n` docs\n",
    "        for i, line in enumerate(tqdm(head)): # change to f/head depending on your needs\n",
    "            instance = line.strip().split()\n",
    "            labels = instance[0]\n",
    "            doc_dict = OrderedDict()\n",
    "            temp_dict = {}\n",
    "            temp_string = ''\n",
    "\n",
    "            for pair in instance[1:]:\n",
    "                feat = pair.split(\":\")\n",
    "                if int(feat[0]) not in temp_dict:\n",
    "                    temp_dict[int(feat[0])] = int(feat[1])\n",
    "\n",
    "            for key in sorted(temp_dict.keys()):\n",
    "                doc_dict[key] = temp_dict[key]\n",
    "\n",
    "            for feat, tf in doc_dict.items():\n",
    "                temp_string = temp_string + \"{}:{} \".format(feat, tf)        \n",
    "            file.write(\"{} {}\\n\".format(labels, temp_string))\n",
    "        file.close()\n",
    "\n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_extractor(labels):\n",
    "\n",
    "    leaf_labels = set()\n",
    "    labels_per_doc = []\n",
    "\n",
    "    for i in labels:\n",
    "        labels_per_doc.append(len(i))\n",
    "        for j in i:\n",
    "            leaf_labels.add(int(j))\n",
    "    \n",
    "    return leaf_labels, labels_per_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hier(filename):\n",
    "    \n",
    "    N = set()\n",
    "    pi = set()\n",
    "    T = set()\n",
    "    \n",
    "    with open(filename, \"r\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            words = line.strip().split()\n",
    "            pi.add(int(words[0])) #adding parent node\n",
    "            T.add(int(words[-1])) #adding ALL leaf nodes in the hierarchy\n",
    "            for w in words:\n",
    "                N.add(int(w))\n",
    "\n",
    "    return N, pi, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = Memory(\"./mycache\")\n",
    "@mem.cache\n",
    "def rr_reader(filename):\n",
    "    '''\n",
    "    create a dataframe from the data-label pair\n",
    "    '''\n",
    "\n",
    "    num_entries = 200000\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    with open(filename, \"r\") as f:\n",
    "#         head = [next(f) for x in range(num_entries)] # retrieve only `n` docs\n",
    "        for i, line in enumerate(tqdm(f)): # change to f/head depending on your needs\n",
    "            instance = line.strip().split()\n",
    "            labels = instance[0]\n",
    "            doc_dict = OrderedDict()\n",
    "            temp_dict = {}\n",
    "\n",
    "            for pair in instance[1:]:\n",
    "                feat = pair.split(\":\")\n",
    "                if int(feat[0]) not in temp_dict:\n",
    "                    temp_dict[int(feat[0])] = int(feat[1])\n",
    "\n",
    "            for key in sorted(temp_dict.keys()):\n",
    "                doc_dict[key] = temp_dict[key]\n",
    "                \n",
    "            temp_df = pd.DataFrame(data = [ labels, doc_dict ]).T\n",
    "            df = df.append(temp_df, ignore_index=True)\n",
    "    \n",
    "    df.columns = [\"labels\", \"feat_tf\"]\n",
    "    df[\"labels\"] = df[\"labels\"].apply( lambda x: list(map(int, x.split(\",\")))  )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = rr_reader(\"swiki/data/train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dta, labls = get_data(\"swiki/data/train_remapped.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df = df.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x2085164 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 114 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dta[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: fucking hell- represent documents in an embedding space -.-\n",
    "# after that we can take `each x_i` -.-\n",
    "def data_parser(dataset, dta):\n",
    "    # embeds the doc to a D-dim space\n",
    "    #create doc_vector for each instance\n",
    "#     tfidfer = TfidfVectorizer()\n",
    "    idx = list(dataset.index)\n",
    "    df_x = []\n",
    "#     for ix in tqdm(idx):\n",
    "#         twoDarray = list(dataset.loc[ix, \"feat_tf\"].items())        \n",
    "#         doc_items = []\n",
    "#         for item in twoDarray:\n",
    "#             if item[0] not in doc_items:\n",
    "#                 if item[1] > 1:\n",
    "#                     for _ in range(item[1]):\n",
    "#                         doc_items.append(str(item[0]))\n",
    "#                 else:\n",
    "#                     doc_items.append(str(item[0]))\n",
    "\n",
    "#         tfidfer.fit(doc_items)\n",
    "#         df_x.append(tfidfer.transform(doc_items))\n",
    "#             df_x.append(doc_items)\n",
    "    for ix in tqdm(idx):\n",
    "        df_x.append(dta[ix].todense())\n",
    "    dg_x = np.stack(df_x)\n",
    "    return dg_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 4155.33it/s]\n"
     ]
    }
   ],
   "source": [
    "ff = data_parser(small_df, dta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2085164)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fff = csr_matrix(ff, shape = (ff.shape[0], 64), dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = np.sum(fff, axis = 1) #row wise sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg = np.random.randn(fff.shape[0], fff.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64),)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(any(fff[:,4]) != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(fff[63,:].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo better R^d representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nn, pii, Ti = read_hier(\"swiki/data/cat_hier.txt\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def user_func(x):\n",
    "    sol = 0\n",
    "    for i in whatiwant:\n",
    "        for j in x:\n",
    "            if i == j:\n",
    "                sol = j\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "    return sol"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df[\"partition\"] = df[\"labels\"].apply(lambda x: user_func(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def largest_feat_n(df):\n",
    "    return max(df[\"feat_tf\"].apply(lambda x: len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = small_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whatiwant = [14661, 71999, 292915, 188756, 131368, 130762, 352578, 395447, 27512, 157031, 33692, 13402, 393382, 390846, 395447, 276114]\n",
    "whatiwant = [14661, 52361, 401434,316934, 369064]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in whatiwant:\n",
    "    d[str(j)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>feat_tf</th>\n",
       "      <th>14661</th>\n",
       "      <th>52361</th>\n",
       "      <th>401434</th>\n",
       "      <th>316934</th>\n",
       "      <th>369064</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>264656</th>\n",
       "      <td>[258850, 78599]</td>\n",
       "      <td>{173875: 1, 177591: 1, 184602: 1, 297238: 1, 3...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247765</th>\n",
       "      <td>[170514]</td>\n",
       "      <td>{170340: 3, 235307: 1, 923907: 1, 1012717: 1, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194584</th>\n",
       "      <td>[156147, 93043]</td>\n",
       "      <td>{4288: 1, 5995: 1, 8032: 1, 40818: 1, 68505: 1...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281678</th>\n",
       "      <td>[288225]</td>\n",
       "      <td>{170340: 2, 339085: 1, 341434: 1, 345419: 1, 3...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393537</th>\n",
       "      <td>[87256]</td>\n",
       "      <td>{3744: 1, 69606: 1, 78034: 1, 150969: 2, 17387...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 labels                                            feat_tf  \\\n",
       "264656  [258850, 78599]  {173875: 1, 177591: 1, 184602: 1, 297238: 1, 3...   \n",
       "247765         [170514]  {170340: 3, 235307: 1, 923907: 1, 1012717: 1, ...   \n",
       "194584  [156147, 93043]  {4288: 1, 5995: 1, 8032: 1, 40818: 1, 68505: 1...   \n",
       "281678         [288225]  {170340: 2, 339085: 1, 341434: 1, 345419: 1, 3...   \n",
       "393537          [87256]  {3744: 1, 69606: 1, 78034: 1, 150969: 2, 17387...   \n",
       "\n",
       "        14661  52361  401434  316934  369064  \n",
       "264656      0      0       0       0       0  \n",
       "247765      0      0       0       0       0  \n",
       "194584      0      0       0       0       0  \n",
       "281678      0      0       0       0       0  \n",
       "393537      0      0       0       0       0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in d.index:\n",
    "    for j in whatiwant:\n",
    "        if j in d.loc[i, \"labels\"]:\n",
    "            d.loc[i, str(j)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14661 1\n",
      "52361 0\n",
      "401434 0\n",
      "316934 0\n",
      "369064 0\n"
     ]
    }
   ],
   "source": [
    "# num of docs per label (id)\n",
    "for j in whatiwant:\n",
    "    print(j, sum(d[str(j)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset = d[d[\"14661\"]==1]\n",
    "subset = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>feat_tf</th>\n",
       "      <th>14661</th>\n",
       "      <th>52361</th>\n",
       "      <th>401434</th>\n",
       "      <th>316934</th>\n",
       "      <th>369064</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>275764</th>\n",
       "      <td>[140871]</td>\n",
       "      <td>{32372: 3, 42120: 2, 42881: 1, 367780: 1, 7763...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          labels                                            feat_tf  14661  \\\n",
       "275764  [140871]  {32372: 3, 42120: 2, 42881: 1, 367780: 1, 7763...      0   \n",
       "\n",
       "        52361  401434  316934  369064  \n",
       "275764      0       0       0       0  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, _ = get_data(\"swiki/data/train_remapped.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "454867"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(list(subset.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = []\n",
    "for i in list(subset.index):\n",
    "    try:\n",
    "        ll.append(train_data[int(i)].toarray())\n",
    "    except:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "niu = np.concatenate( ll, axis=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2085164)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "niu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, test_x, test_y = train_test_split(niu, subset, random_state=42, test_size=0.30, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70, 2085164), (30, 2085164), (70, 7), (30, 7))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape, train_y.shape, test_x.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_x\n",
    "y_train = test_x.drop(labels = [\"labels\", \"feat_tf\"], axis = 1)\n",
    "\n",
    "x_test = train_y\n",
    "y_test = test_y.drop(labels = [\"labels\", \"feat_tf\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>14661</th>\n",
       "      <th>52361</th>\n",
       "      <th>401434</th>\n",
       "      <th>316934</th>\n",
       "      <th>369064</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>103069</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        14661  52361  401434  316934  369064\n",
       "103069      0      0       0       0       0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SGDClassifier()\n",
    "# lb = preprocessing.LabelBinarizer(sparse_output=True)\n",
    "# mb = preprocessing.MultiLabelBinarizer(sparse_output=True)\n",
    "# le = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=None,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "       power_t=0.5, random_state=None, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# SGD Classifier\n",
    "for category in whatiwant[1:]:\n",
    "    print('**Processing {} tag...**'.format(category))\n",
    "    \n",
    "    # Training sgd model on train data\n",
    "    clf.fit(x_train, y_train[str(category)])\n",
    "    \n",
    "    # calculating test accuracy\n",
    "    prediction = clf.predict(x_test)\n",
    "    print('Test set Micro F1 is {}'.format(f1_score(y_test[str(category)], prediction, average=\"micro\")))\n",
    "    print('Test set Precision is {}'.format(precision_score(y_test[str(category)], prediction, average=\"macro\")))\n",
    "    print('Test set Recall is {}'.format(recall_score(y_test[str(category)], prediction, average=\"micro\")))\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Using pipeline for applying logistic regression and one vs rest classifier\n",
    "LogReg_pipeline = Pipeline([\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1)),\n",
    "            ])\n",
    "for category in whatiwant[1:]:\n",
    "    print('**Processing {} tag...**'.format(category))\n",
    "    \n",
    "    # Training logistic regression model on train data\n",
    "    LogReg_pipeline.fit(x_train, y_train[str(category)])\n",
    "    \n",
    "    # calculating test accuracy\n",
    "    prediction = LogReg_pipeline.predict(x_test)\n",
    "    print('Test accuracy is {}'.format(accuracy_score(y_test[str(category)], prediction)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50312 50311\n",
      "{2143406}\n"
     ]
    }
   ],
   "source": [
    "print(len(Nn), len(Ti)) # there is one node without any parent\n",
    "print(Nn.difference(Ti)) # this is the node: this is probably the root node i guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13808"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    " def lookup_table(filename, subset):\n",
    "        \n",
    "    p2c_table = {}\n",
    "    c2p_table = {}\n",
    "    node2id = OrderedDict()\n",
    "    id2node = OrderedDict()\n",
    "    i = 0\n",
    "    \n",
    "    with open(filename, \"r\") as f:\n",
    "        if not subset:\n",
    "            head = f\n",
    "        elif isinstance(subset, int):\n",
    "            head = [next(f) for x in range(subset)] # retrieve only `n` docs\n",
    "        else:\n",
    "            raise ValueError(\"Incorrect subset type. Enter only False (boolean) or int. Encountered {} type.\".format(type(subset)))\n",
    "        for _, line in enumerate(tqdm(head)):\n",
    "            split_line = line.strip().split()\n",
    "            parent_node = int(split_line[0])\n",
    "            child_node = list(map(int, split_line[1:]))\n",
    "            \n",
    "            # map to the respective dicts -> parent:child relationship\n",
    "            # parent2child lookup table\n",
    "            if parent_node not in p2c_table:\n",
    "                p2c_table[parent_node] = [child_node[0]]\n",
    "            else:\n",
    "                p2c_table[parent_node].append(child_node[0])\n",
    "                \n",
    "            #child2parent lookup table\n",
    "            if child_node[0] not in c2p_table:\n",
    "                c2p_table[child_node[0]] = [parent_node]\n",
    "            else:\n",
    "                c2p_table[child_node[0]].append(parent_node)\n",
    "                \n",
    "            # map parent/child node to a node<->id\n",
    "            if parent_node not in node2id:\n",
    "                p_id = i\n",
    "                node2id[parent_node] = p_id\n",
    "                id2node[p_id] = parent_node\n",
    "                i+=1\n",
    "            else:\n",
    "                p_id = node2id[parent_node]\n",
    "                \n",
    "            if child_node[0] not in node2id:\n",
    "                c_id = i\n",
    "                node2id[child_node[0]] = c_id\n",
    "                id2node[c_id] = child_node[0]      \n",
    "                i+=1\n",
    "            else:\n",
    "                c_id = node2id[child_node[0]]\n",
    "\n",
    "    pi_parents = set(p2c_table.keys())        \n",
    "    T_leaves = (c2p_table.keys() - p2c_table.keys()) \n",
    "    N_all_nodes = pi_parents.union(T_leaves)\n",
    "    \n",
    "    return p2c_table, c2p_table, node2id, id2node, list(pi_parents), list(T_leaves), list(N_all_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchy2graph(p2c_table, node2id):\n",
    "\n",
    "    edges = []\n",
    "    for parent, children in p2c_table.items():\n",
    "        p_id = node2id[parent]\n",
    "        for child in children:\n",
    "            c_id = node2id[child]\n",
    "            edges.append((p_id, c_id))\n",
    "    vertices = [k for k, v in node2id.items()]\n",
    "    g = ig.Graph(n=len(node2id), edges=edges, directed=True, vertex_attrs={\"name\": vertices})\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightparameter(N_all_nodes, size_n):\n",
    "    \n",
    "    # randomly initialize weights for all nodes instead of taking user input [assumption]\n",
    "    w_all_n = {}\n",
    "    for n in N_all_nodes:\n",
    "        temp_rand = np.random.randn(size_n,1)\n",
    "        if n not in w_all_n:\n",
    "            w_all_n[n] = temp_rand\n",
    "            \n",
    "#         do reverse mapping?\n",
    "    return w_all_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"image.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_gradient(w_node, x, data):\n",
    "    \n",
    "    w_n = w_all_n[w_node]\n",
    "    w_pi_n = w_all_n[c2p[w_node][0]]\n",
    "    \n",
    "    C = 1 # by default\n",
    "    one = w_n - w_pi_n\n",
    "    norm_w = np.linalg.norm(one, 2, keepdims = False)\n",
    "    g = []\n",
    "    # g = g.reshape((-1,1))\n",
    "    y_in = -1\n",
    "    \n",
    "    for i, j in enumerate(data.index):\n",
    "        if w_node in data.loc[j, \"labels\"]:\n",
    "            y_in = 1\n",
    "            print(\"yes\", w_node, data.loc[j, \"labels\"])\n",
    "    \n",
    "        # eqn 3.9\n",
    "        xx =  x[i].reshape((-1, 1))\n",
    "        y_w_x = y_in * np.dot(np.transpose(w_n), xx)\n",
    "\n",
    "        y_x = y_in * xx\n",
    "\n",
    "        two = C*(1/(1+np.exp(y_w_x)))*y_x\n",
    "\n",
    "        assert(one.shape == two.shape)\n",
    "        g.append(one - two)\n",
    "\n",
    "        # eqn 3.8\n",
    "        obj = C*np.log(1+np.exp(-y_w_x))\n",
    "\n",
    "        min_wn = norm_w + obj\n",
    "    \n",
    "    return min_wn      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.function_gradient(w_node, x, g, data, lmbda)>"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_lr(data, w_node, lmbda, eps, maxfn, fff):\n",
    "    m = 5\n",
    "    f = 0\n",
    "    xtol = 1e-30\n",
    "    maxfn = 0\n",
    "    w_n = w_all_n[w_node]\n",
    "    init_x = [2, 2]\n",
    "    largest_n = largest_feat_n(data)\n",
    "    n = max(len(w_n), largest_n)\n",
    "    \n",
    "    x = fff\n",
    "    g = np.zeros(x.shape)\n",
    "\n",
    "    while(maxfn < 10):\n",
    "        \n",
    "        w_n = w_all_n[w_node]\n",
    "        w_pi_n = w_all_n[c2p[w_node][0]]\n",
    "        \n",
    "        f = function_gradient(w_node, x, g, data, lmbda)\n",
    "#         try:\n",
    "        optimize.minimize(function_gradient, init_x, args=(w_n), method='L-BFGS-B', tol=xtol, options={'disp': True, 'eps': eps })\n",
    "#         except:\n",
    "#             print(\"didn't converge\")\n",
    "            \n",
    "        maxfn +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "65333it [00:00, 276100.81it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "p2c, c2p, n2i, i2n, pi, T, N = lookup_table(\"swiki/data/cat_hier.txt\", subset = False)\n",
    "p2c_s, c2p_s, n2i_s, i2n_s, pi_s, T_s, N_s = lookup_table(\"swiki/data/cat_hier.txt\", subset = 9)\n",
    "w_all_n = weightparameter(N, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_n = largest_feat_n(small_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64,)"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gg[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2255744\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-344-b25551275ff9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mmaxfn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mw_n\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw_all_n\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mlr_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobjective_lr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msmall_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlmbda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m##\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;31m#         g = function_g() ##\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"over\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-336-bc0cca96a5b9>\u001b[0m in \u001b[0;36mobjective_lr\u001b[1;34m(data, w_node, lmbda, eps, maxfn, fff)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunction_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw_node\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlmbda\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m#         try:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0moptimize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'L-BFGS-B'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'disp'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'eps'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0meps\u001b[0m \u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;31m#         except:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m#             print(\"didn't converge\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\harshasivajit\\documents\\master-ai\\venv\\lib\\site-packages\\scipy\\optimize\\_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    599\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'l-bfgs-b'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[1;32m--> 601\u001b[1;33m                                 callback=callback, **options)\n\u001b[0m\u001b[0;32m    602\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tnc'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n",
      "\u001b[1;32mc:\\users\\harshasivajit\\documents\\master-ai\\venv\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[0;32m    333\u001b[0m             \u001b[1;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m             \u001b[1;31m# Overwrite f and g:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m             \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    336\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb'NEW_X'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m             \u001b[1;31m# new iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\harshasivajit\\documents\\master-ai\\venv\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    278\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mjac\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m             \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_approx_fprime_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\harshasivajit\\documents\\master-ai\\venv\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[1;34m(*wrapper_args)\u001b[0m\n\u001b[0;32m    298\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": [
    "# result_w = np.zeros((len(N_s),1))\n",
    "# while(1):\n",
    "result_w = {}\n",
    "\n",
    "for n in N_s:    \n",
    "    print(n)\n",
    "    # check if the node has a parent\n",
    "    if n not in c2p:\n",
    "        w_pi_n = 0 # if the node has no parent then it's the root node. assign it w=0\n",
    "    else:\n",
    "        w_pi_n = w_all_n[n] \n",
    "    \n",
    "    # if n is not leaf node\n",
    "    if n not in T_s:\n",
    "        # update w_n using eqn 3.3 or 4\n",
    "        mod_C_n = len(p2c[n]) # |C_n|\n",
    "        sum_w_c = 0 \n",
    "        for c in p2c[n]:\n",
    "            sum_w_c += w_all_n[c]\n",
    "            \n",
    "        if n not in result_w:\n",
    "            result_w[n] = 1/(mod_C_n + 1) * (w_pi_n + sum_w_c)\n",
    "    # else: n is a leaf node\n",
    "    else:\n",
    "        #optimize using lbfgs eqn 3.8, 3.9 or 8\n",
    "        lmbda = 1\n",
    "        eps = 1e-4\n",
    "        maxfn = 10\n",
    "        w_n = w_all_n[n]\n",
    "        lr_function = objective_lr(small_df, n, lmbda, eps, maxfn, gg) ##\n",
    "#         g = function_g() ##\n",
    "        print(\"over\")\n",
    "    print(\"ove\")\n",
    "print(\"ov\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "g = hierarchy2graph(p2c_s, n2i_s)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(g)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "layout = g.layout(\"kk\")\n",
    "g.vs[\"label\"] = g.vs[\"name\"]\n",
    "ig.plot(g, layout = layout)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, x_test = train_test_split(train_data, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pi), len(T), len(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
