{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/gcdart/MulticlassClassifier/blob/master/src/ml/LogisticRegression.java\n",
    "# https://www.kaggle.com/c/lshtc/discussion/6911#38233 - preprocessing: multilabels comma should not have spaces\n",
    "# https://www.kaggle.com/c/lshtc/discussion/14048 - dataset statistics\n",
    "## reading the LWIKI, SWIKI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import igraph as ig\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm #always use this instead of `import tqdm`\n",
    "# from sklearn.datasets import fetch_rcv1\n",
    "from scipy.sparse import *\n",
    "# np.random.seed(123)\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from collections import OrderedDict\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import preprocessing, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Memory\n",
    "from sklearn.datasets import load_svmlight_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = Memory(\"./mycache\")\n",
    "@mem.cache\n",
    "def get_data(filename):\n",
    "    \n",
    "    fname = str(Path(filename))\n",
    "    fe, ex = os.path.splitext(fname) \n",
    "\n",
    "    try:\n",
    "        data = load_svmlight_file(fname, n_features = 2085162 , multilabel=True, offset = 100000, length = 2800000)\n",
    "    except:\n",
    "        # Required: if the input data isn't in the correct libsvm format\n",
    "        outfile = str(Path(\"{}_small{}\".format(fe, ex)))\n",
    "#         outfile = str(Path(\"{}_remapped{}\".format(fe, ex)))\n",
    "        if not os.path.isfile(outfile):\n",
    "            logging.info(\"Remapping data to LibSVM format...\")\n",
    "            f = preprocess_libsvm(fname, outfile)\n",
    "        else:\n",
    "            logging.info(\"Using already remapped data...\")\n",
    "            f = outfile\n",
    "        data = load_svmlight_file(f, multilabel=True, n_features = 2085162 , offset = 100000, length = 2800000)\n",
    "        \n",
    "    return data[0], data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_libsvm(input_file, output_file):\n",
    "    # converts file to the required libsvm format.\n",
    "    # this is very brute force but can be made faster [IMPROVE]\n",
    "\n",
    "    file = open(output_file, \"w+\")\n",
    "    with open(input_file, \"r\") as f:\n",
    "        head = [next(f) for x in range(500)] # retrieve only `n` docs\n",
    "        for i, line in enumerate(tqdm(head)): # change to f/head depending on your needs\n",
    "            instance = line.strip().split()\n",
    "            labels = instance[0]\n",
    "            doc_dict = OrderedDict()\n",
    "            temp_dict = {}\n",
    "            temp_string = ''\n",
    "\n",
    "            for pair in instance[1:]:\n",
    "                feat = pair.split(\":\")\n",
    "                if int(feat[0]) not in temp_dict:\n",
    "                    temp_dict[int(feat[0])] = int(feat[1])\n",
    "\n",
    "            for key in sorted(temp_dict.keys()):\n",
    "                doc_dict[key] = temp_dict[key]\n",
    "\n",
    "            for feat, tf in doc_dict.items():\n",
    "                temp_string = temp_string + \"{}:{} \".format(feat, tf)        \n",
    "            file.write(\"{} {}\\n\".format(labels, temp_string))\n",
    "        file.close()\n",
    "\n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_extractor(labels):\n",
    "\n",
    "    leaf_labels = set()\n",
    "    labels_per_doc = []\n",
    "\n",
    "    for i in labels:\n",
    "        labels_per_doc.append(len(i))\n",
    "        for j in i:\n",
    "            leaf_labels.add(int(j))\n",
    "    \n",
    "    return leaf_labels, labels_per_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hier(filename):\n",
    "    \n",
    "    N = set()\n",
    "    pi = set()\n",
    "    T = set()\n",
    "    \n",
    "    with open(filename, \"r\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            words = line.strip().split()\n",
    "            pi.add(int(words[0])) #adding parent node\n",
    "            T.add(int(words[-1])) #adding ALL leaf nodes in the hierarchy\n",
    "            for w in words:\n",
    "                N.add(int(w))\n",
    "\n",
    "    return N, pi, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = Memory(\"./mycache\")\n",
    "@mem.cache\n",
    "def rr_reader(filename):\n",
    "    '''\n",
    "    create a dataframe from the data-label pair\n",
    "    '''\n",
    "\n",
    "    num_entries = 200000\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    with open(filename, \"r\") as f:\n",
    "#         head = [next(f) for x in range(num_entries)] # retrieve only `n` docs\n",
    "        for i, line in enumerate(tqdm(f)): # change to f/head depending on your needs\n",
    "            instance = line.strip().split()\n",
    "            labels = instance[0]\n",
    "            doc_dict = OrderedDict()\n",
    "            temp_dict = {}\n",
    "\n",
    "            for pair in instance[1:]:\n",
    "                feat = pair.split(\":\")\n",
    "                if int(feat[0]) not in temp_dict:\n",
    "                    temp_dict[int(feat[0])] = int(feat[1])\n",
    "\n",
    "            for key in sorted(temp_dict.keys()):\n",
    "                doc_dict[key] = temp_dict[key]\n",
    "                \n",
    "            temp_df = pd.DataFrame(data = [ labels, doc_dict ]).T\n",
    "            df = df.append(temp_df, ignore_index=True)\n",
    "    \n",
    "    df.columns = [\"labels\", \"feat_tf\"]\n",
    "    df[\"labels\"] = df[\"labels\"].apply( lambda x: list(map(int, x.split(\",\")))  )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = rr_reader(\"swiki/data/train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "[Memory] Calling __main__-C%3A-Users-harshasivajit-Documents-master-ai-rr13-__ipython-input__.get_data...\n",
      "get_data('swiki/data/train.txt')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using already remapped data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________get_data - 0.0s, 0.0min\n"
     ]
    }
   ],
   "source": [
    "dta, labls = get_data(\"swiki/data/train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df = df.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(348, 2085162)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: fucking hell- represent documents in an embedding space -.-\n",
    "# after that we can take `each x_i` -.-\n",
    "def data_parser(dataset, dta):\n",
    "    # embeds the doc to a D-dim space\n",
    "    #create doc_vector for each instance\n",
    "#     tfidfer = TfidfVectorizer()\n",
    "    idx = list(dataset.index)\n",
    "    df_x = []\n",
    "#     for ix in tqdm(idx):\n",
    "#         twoDarray = list(dataset.loc[ix, \"feat_tf\"].items())        \n",
    "#         doc_items = []\n",
    "#         for item in twoDarray:\n",
    "#             if item[0] not in doc_items:\n",
    "#                 if item[1] > 1:\n",
    "#                     for _ in range(item[1]):\n",
    "#                         doc_items.append(str(item[0]))\n",
    "#                 else:\n",
    "#                     doc_items.append(str(item[0]))\n",
    "\n",
    "#         tfidfer.fit(doc_items)\n",
    "#         df_x.append(tfidfer.transform(doc_items))\n",
    "#             df_x.append(doc_items)\n",
    "    for ix in tqdm(idx):\n",
    "        df_x.append(dta[ix].todense())\n",
    "    dg_x = np.stack(df_x)\n",
    "    return dg_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ff = data_parser(small_df, dta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fff = csr_matrix(ff, shape = (ff.shape[0], 64), dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = np.sum(fff, axis = 1) #row wise sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg = np.random.randn(fff.shape[0], fff.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(any(fff[:,4]) != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fff[63,:].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo better R^d representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nn, pii, Ti = read_hier(\"swiki/data/cat_hier.txt\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def user_func(x):\n",
    "    sol = 0\n",
    "    for i in whatiwant:\n",
    "        for j in x:\n",
    "            if i == j:\n",
    "                sol = j\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "    return sol"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df[\"partition\"] = df[\"labels\"].apply(lambda x: user_func(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def largest_feat_n(df):\n",
    "    return max(df[\"feat_tf\"].apply(lambda x: len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = small_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whatiwant = [14661, 71999, 292915, 188756, 131368, 130762, 352578, 395447, 27512, 157031, 33692, 13402, 393382, 390846, 395447, 276114]\n",
    "whatiwant = [14661, 52361, 401434,316934, 369064]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in whatiwant:\n",
    "    d[str(j)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in d.index:\n",
    "    for j in whatiwant:\n",
    "        if j in d.loc[i, \"labels\"]:\n",
    "            d.loc[i, str(j)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num of docs per label (id)\n",
    "for j in whatiwant:\n",
    "    print(j, sum(d[str(j)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset = d[d[\"14661\"]==1]\n",
    "subset = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, _ = get_data(\"swiki/data/train_remapped.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(list(subset.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = []\n",
    "for i in list(subset.index):\n",
    "    try:\n",
    "        ll.append(train_data[int(i)].toarray())\n",
    "    except:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niu = np.concatenate( ll, axis=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, test_x, test_y = train_test_split(niu, subset, random_state=42, test_size=0.30, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.shape, train_y.shape, test_x.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_x\n",
    "y_train = test_x.drop(labels = [\"labels\", \"feat_tf\"], axis = 1)\n",
    "\n",
    "x_test = train_y\n",
    "y_test = test_y.drop(labels = [\"labels\", \"feat_tf\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SGDClassifier()\n",
    "# lb = preprocessing.LabelBinarizer(sparse_output=True)\n",
    "# mb = preprocessing.MultiLabelBinarizer(sparse_output=True)\n",
    "# le = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# SGD Classifier\n",
    "for category in whatiwant[1:]:\n",
    "    print('**Processing {} tag...**'.format(category))\n",
    "    \n",
    "    # Training sgd model on train data\n",
    "    clf.fit(x_train, y_train[str(category)])\n",
    "    \n",
    "    # calculating test accuracy\n",
    "    prediction = clf.predict(x_test)\n",
    "    print('Test set Micro F1 is {}'.format(f1_score(y_test[str(category)], prediction, average=\"micro\")))\n",
    "    print('Test set Precision is {}'.format(precision_score(y_test[str(category)], prediction, average=\"macro\")))\n",
    "    print('Test set Recall is {}'.format(recall_score(y_test[str(category)], prediction, average=\"micro\")))\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Using pipeline for applying logistic regression and one vs rest classifier\n",
    "LogReg_pipeline = Pipeline([\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1)),\n",
    "            ])\n",
    "for category in whatiwant[1:]:\n",
    "    print('**Processing {} tag...**'.format(category))\n",
    "    \n",
    "    # Training logistic regression model on train data\n",
    "    LogReg_pipeline.fit(x_train, y_train[str(category)])\n",
    "    \n",
    "    # calculating test accuracy\n",
    "    prediction = LogReg_pipeline.predict(x_test)\n",
    "    print('Test accuracy is {}'.format(accuracy_score(y_test[str(category)], prediction)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Nn), len(Ti)) # there is one node without any parent\n",
    "print(Nn.difference(Ti)) # this is the node: this is probably the root node i guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(pii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    " def lookup_table(filename, subset):\n",
    "        \n",
    "    p2c_table = {}\n",
    "    c2p_table = {}\n",
    "    node2id = OrderedDict()\n",
    "    id2node = OrderedDict()\n",
    "    i = 0\n",
    "    \n",
    "    with open(filename, \"r\") as f:\n",
    "        if not subset:\n",
    "            head = f\n",
    "        elif isinstance(subset, int):\n",
    "            head = [next(f) for x in range(subset)] # retrieve only `n` docs\n",
    "        else:\n",
    "            raise ValueError(\"Incorrect subset type. Enter only False (boolean) or int. Encountered {} type.\".format(type(subset)))\n",
    "        for _, line in enumerate(tqdm(head)):\n",
    "            split_line = line.strip().split()\n",
    "            parent_node = int(split_line[0])\n",
    "            child_node = list(map(int, split_line[1:]))\n",
    "            \n",
    "            # map to the respective dicts -> parent:child relationship\n",
    "            # parent2child lookup table\n",
    "            if parent_node not in p2c_table:\n",
    "                p2c_table[parent_node] = [child_node[0]]\n",
    "            else:\n",
    "                p2c_table[parent_node].append(child_node[0])\n",
    "                \n",
    "            #child2parent lookup table\n",
    "            if child_node[0] not in c2p_table:\n",
    "                c2p_table[child_node[0]] = [parent_node]\n",
    "            else:\n",
    "                c2p_table[child_node[0]].append(parent_node)\n",
    "                \n",
    "            # map parent/child node to a node<->id\n",
    "            if parent_node not in node2id:\n",
    "                p_id = i\n",
    "                node2id[parent_node] = p_id\n",
    "                id2node[p_id] = parent_node\n",
    "                i+=1\n",
    "            else:\n",
    "                p_id = node2id[parent_node]\n",
    "                \n",
    "            if child_node[0] not in node2id:\n",
    "                c_id = i\n",
    "                node2id[child_node[0]] = c_id\n",
    "                id2node[c_id] = child_node[0]      \n",
    "                i+=1\n",
    "            else:\n",
    "                c_id = node2id[child_node[0]]\n",
    "\n",
    "    pi_parents = set(p2c_table.keys())        \n",
    "    T_leaves = (c2p_table.keys() - p2c_table.keys()) \n",
    "    N_all_nodes = pi_parents.union(T_leaves)\n",
    "    \n",
    "    return p2c_table, c2p_table, node2id, id2node, list(pi_parents), list(T_leaves), list(N_all_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchy2graph(p2c_table, node2id):\n",
    "\n",
    "    edges = []\n",
    "    for parent, children in p2c_table.items():\n",
    "        p_id = node2id[parent]\n",
    "        for child in children:\n",
    "            c_id = node2id[child]\n",
    "            edges.append((p_id, c_id))\n",
    "    vertices = [k for k, v in node2id.items()]\n",
    "    g = ig.Graph(n=len(node2id), edges=edges, directed=True, vertex_attrs={\"name\": vertices})\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightparameter(N_all_nodes, size_n):\n",
    "    \n",
    "    # randomly initialize weights for all nodes instead of taking user input [assumption]\n",
    "    w_all_n = {}\n",
    "    rev_mapping_w = {}\n",
    "    for n in N_all_nodes:\n",
    "        temp_rand = np.random.randn(size_n,1)\n",
    "        if n not in w_all_n:\n",
    "            w_all_n[n] = temp_rand\n",
    "        if temp_rand not in rev_mapping_w:\n",
    "            rev_mapping_w[temp_rand] = n\n",
    "    return w_all_n, rev_mapping_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"image.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_gradient(w_n, w_node, x, data):\n",
    "    \n",
    "    w_pi_n = w_all_n[c2p[w_node][0]]\n",
    "    \n",
    "    C = 1 # by default\n",
    "    one = w_n - w_pi_n\n",
    "    norm_w = np.linalg.norm(x = one, ord = 'fro', keepdims = False)\n",
    "    \n",
    "#     g = []\n",
    "#     g = g.reshape((-1,1))\n",
    "    y_in = -1 * np.ones((w_n.shape))\n",
    "\n",
    "    for i, j in enumerate(data.index):\n",
    "        if w_node in data.loc[j, \"labels\"]:\n",
    "            y_in[i] = 1\n",
    "            print(\"yes\", w_node, data.loc[j, \"labels\"])\n",
    "\n",
    "    y_w_x = np.dot(y_in, np.dot(np.transpose(w_n), np.transpose(x)))\n",
    "\n",
    "    # eqn 3.8\n",
    "    obj = C * np.sum(np.log(1+np.exp(-y_w_x)), axis = 1, keepdims = True)\n",
    "    assert w_n.shape == obj.shape\n",
    "    \n",
    "#     y_x = np.dot(y_in.T, x.T)\n",
    "#     two = C*(1/(1+np.exp(y_w_x)))*y_x\n",
    "#     assert(one.shape == two.shape)\n",
    "#     g.append(one - two)\n",
    "\n",
    "    min_wn = norm_w + obj\n",
    "    \n",
    "    return min_wn      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.function_gradient(w_n, w_node, x, data)>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fff' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-5686a4cef8ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfff\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'fff' is not defined"
     ]
    }
   ],
   "source": [
    "fff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_lr(data, w_node, lmbda, eps, maxfn, fff):\n",
    "    m = 5\n",
    "    f = 0\n",
    "    xtol = 1e-30\n",
    "    w_n = w_all_n[w_node]\n",
    "    maxfn = 0\n",
    "    init_x = [2, 2]\n",
    "    largest_n = largest_feat_n(data)\n",
    "    n = max(len(w_n), largest_n)\n",
    "    \n",
    "    x = fff\n",
    "    g = np.zeros(x.shape)\n",
    "    while(maxfn < 10):\n",
    "        \n",
    "#         f = function_gradient(w_n, w_node, x, data)\n",
    "#         try:\n",
    "        optimize.minimize(fun = function_gradient, x0 = init_x, args=(w_n), method='L-BFGS-B', tol=xtol, options={'disp': True, 'eps': eps })\n",
    "#         except:\n",
    "#             print(\"didn't converge\")\n",
    "            \n",
    "        maxfn +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2445705"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "65333it [00:00, 215051.92it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-95-04481a7af405>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mp2c\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc2p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn2i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi2n\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlookup_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"swiki/data/cat_hier.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mp2c_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc2p_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn2i_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi2n_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpi_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlookup_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"swiki/data/cat_hier.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mw_all_n\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrev_all_n\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweightparameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-91-5950af939d52>\u001b[0m in \u001b[0;36mweightparameter\u001b[1;34m(N_all_nodes, size_n)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mw_all_n\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mw_all_n\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp_rand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mtemp_rand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrev_mapping_w\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m             \u001b[0mrev_mapping_w\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtemp_rand\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mw_all_n\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrev_mapping_w\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "p2c, c2p, n2i, i2n, pi, T, N = lookup_table(\"swiki/data/cat_hier.txt\", subset = False)\n",
    "p2c_s, c2p_s, n2i_s, i2n_s, pi_s, T_s, N_s = lookup_table(\"swiki/data/cat_hier.txt\", subset = 9)\n",
    "w_all_n, rev_all_n = weightparameter(N, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_n = largest_feat_n(small_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# result_w = np.zeros((len(N_s),1))\n",
    "# while(1):\n",
    "result_w = {}\n",
    "\n",
    "for n in N_s:    \n",
    "    print(n)\n",
    "    # check if the node has a parent\n",
    "    if n not in c2p:\n",
    "        w_pi_n = 0 # if the node has no parent then it's the root node. assign it w=0\n",
    "    else:\n",
    "        w_pi_n = w_all_n[n] \n",
    "    \n",
    "    # if n is not leaf node\n",
    "    if n not in T_s:\n",
    "        # update w_n using eqn 3.3 or 4\n",
    "        mod_C_n = len(p2c[n]) # |C_n|\n",
    "        sum_w_c = 0 \n",
    "        for c in p2c[n]:\n",
    "            sum_w_c += w_all_n[c]\n",
    "            \n",
    "        if n not in result_w:\n",
    "            result_w[n] = 1/(mod_C_n + 1) * (w_pi_n + sum_w_c)\n",
    "    # else: n is a leaf node\n",
    "    else:\n",
    "        #optimize using lbfgs eqn 3.8, 3.9 or 8\n",
    "        lmbda = 1\n",
    "        eps = 1e-4\n",
    "        maxfn = 10\n",
    "        w_n = w_all_n[n]\n",
    "        lr_function = objective_lr(small_df, n, lmbda, eps, maxfn, gg) ##\n",
    "#         g = function_g() ##\n",
    "        print(\"over\")\n",
    "    print(\"ove\")\n",
    "print(\"ov\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "g = hierarchy2graph(p2c_s, n2i_s)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(g)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "layout = g.layout(\"kk\")\n",
    "g.vs[\"label\"] = g.vs[\"name\"]\n",
    "ig.plot(g, layout = layout)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, x_test = train_test_split(train_data, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pi), len(T), len(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
