{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/gcdart/MulticlassClassifier/blob/master/src/ml/LogisticRegression.java\n",
    "# https://www.kaggle.com/c/lshtc/discussion/6911#38233 - preprocessing: multilabels comma should not have spaces\n",
    "# https://www.kaggle.com/c/lshtc/discussion/14048 - dataset statistics\n",
    "## reading the LWIKI, SWIKI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import igraph as ig\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm #always use this instead of `import tqdm`\n",
    "# from sklearn.datasets import fetch_rcv1\n",
    "\n",
    "# np.random.seed(123)\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from collections import OrderedDict\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import preprocessing, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Memory\n",
    "from sklearn.datasets import load_svmlight_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = Memory(\"./mycache\")\n",
    "@mem.cache\n",
    "def get_data(filename):\n",
    "    \n",
    "    fname = str(Path(filename))\n",
    "    fe, ex = os.path.splitext(fname) \n",
    "\n",
    "    try:\n",
    "        data = load_svmlight_file(fname, multilabel=True)\n",
    "    except:\n",
    "        # Required: if the input data isn't in the correct libsvm format\n",
    "        outfile = str(Path(\"{}_small{}\".format(fe, ex)))\n",
    "#         outfile = str(Path(\"{}_remapped{}\".format(fe, ex)))\n",
    "        if not os.path.isfile(outfile):\n",
    "            logging.info(\"Remapping data to LibSVM format...\")\n",
    "            f = preprocess_libsvm(fname, outfile)\n",
    "        else:\n",
    "            logging.info(\"Using already remapped data...\")\n",
    "            f = outfile\n",
    "        data = load_svmlight_file(f, multilabel=True)\n",
    "        \n",
    "    return data[0], data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_libsvm(input_file, output_file):\n",
    "    # converts file to the required libsvm format.\n",
    "    # this is very brute force but can be made faster [IMPROVE]\n",
    "\n",
    "    file = open(output_file, \"w+\")\n",
    "    with open(input_file, \"r\") as f:\n",
    "        head = [next(f) for x in range(500)] # retrieve only `n` docs\n",
    "        for i, line in enumerate(tqdm(head)): # change to f/head depending on your needs\n",
    "            instance = line.strip().split()\n",
    "            labels = instance[0]\n",
    "            doc_dict = OrderedDict()\n",
    "            temp_dict = {}\n",
    "            temp_string = ''\n",
    "\n",
    "            for pair in instance[1:]:\n",
    "                feat = pair.split(\":\")\n",
    "                if int(feat[0]) not in temp_dict:\n",
    "                    temp_dict[int(feat[0])] = int(feat[1])\n",
    "\n",
    "            for key in sorted(temp_dict.keys()):\n",
    "                doc_dict[key] = temp_dict[key]\n",
    "\n",
    "            for feat, tf in doc_dict.items():\n",
    "                temp_string = temp_string + \"{}:{} \".format(feat, tf)        \n",
    "            file.write(\"{} {}\\n\".format(labels, temp_string))\n",
    "        file.close()\n",
    "\n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_extractor(labels):\n",
    "\n",
    "    leaf_labels = set()\n",
    "    labels_per_doc = []\n",
    "\n",
    "    for i in labels:\n",
    "        labels_per_doc.append(len(i))\n",
    "        for j in i:\n",
    "            leaf_labels.add(int(j))\n",
    "    \n",
    "    return leaf_labels, labels_per_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hier(filename):\n",
    "    \n",
    "    N = set()\n",
    "    pi = set()\n",
    "    T = set()\n",
    "    \n",
    "    with open(filename, \"r\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            words = line.strip().split()\n",
    "            pi.add(int(words[0])) #adding parent node\n",
    "            T.add(int(words[-1])) #adding ALL leaf nodes in the hierarchy\n",
    "            for w in words:\n",
    "                N.add(int(w))\n",
    "\n",
    "    return N, pi, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = Memory(\"./mycache\")\n",
    "@mem.cache\n",
    "def rr_reader(filename):\n",
    "    '''\n",
    "    create a dataframe from the data-label pair\n",
    "    '''\n",
    "\n",
    "    num_entries = 200000\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    with open(filename, \"r\") as f:\n",
    "#         head = [next(f) for x in range(num_entries)] # retrieve only `n` docs\n",
    "        for i, line in enumerate(tqdm(f)): # change to f/head depending on your needs\n",
    "            instance = line.strip().split()\n",
    "            labels = instance[0]\n",
    "            doc_dict = OrderedDict()\n",
    "            temp_dict = {}\n",
    "\n",
    "            for pair in instance[1:]:\n",
    "                feat = pair.split(\":\")\n",
    "                if int(feat[0]) not in temp_dict:\n",
    "                    temp_dict[int(feat[0])] = int(feat[1])\n",
    "\n",
    "            for key in sorted(temp_dict.keys()):\n",
    "                doc_dict[key] = temp_dict[key]\n",
    "                \n",
    "            temp_df = pd.DataFrame(data = [ labels, doc_dict ]).T\n",
    "            df = df.append(temp_df, ignore_index=True)\n",
    "    \n",
    "    df.columns = [\"labels\", \"feat_tf\"]\n",
    "    df[\"labels\"] = df[\"labels\"].apply( lambda x: list(map(int, x.split(\",\")))  )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "df = rr_reader(\"swiki/data/train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df = rr_reader(\"swiki/data/train_small.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nn, pii, Ti = read_hier(\"swiki/data/cat_hier.txt\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def user_func(x):\n",
    "    sol = 0\n",
    "    for i in whatiwant:\n",
    "        for j in x:\n",
    "            if i == j:\n",
    "                sol = j\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "    return sol"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df[\"partition\"] = df[\"labels\"].apply(lambda x: user_func(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def largest_feat_n(df):\n",
    "    return max(df[\"feat_tf\"].apply(lambda x: len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = small_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whatiwant = [14661, 71999, 292915, 188756, 131368, 130762, 352578, 395447, 27512, 157031, 33692, 13402, 393382, 390846, 395447, 276114]\n",
    "whatiwant = [14661, 71999, 292915, 188756, 131368]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in whatiwant:\n",
    "    d[str(j)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>feat_tf</th>\n",
       "      <th>14661</th>\n",
       "      <th>71999</th>\n",
       "      <th>292915</th>\n",
       "      <th>188756</th>\n",
       "      <th>131368</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[33692, 13402, 393382]</td>\n",
       "      <td>{624: 3, 4288: 1, 14403: 1, 54278: 1, 62619: 1...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[130762]</td>\n",
       "      <td>{120505: 1, 173442: 1, 554009: 1, 634374: 1, 6...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[352578, 395447, 27512, 157031]</td>\n",
       "      <td>{62483: 1, 73429: 1, 138155: 1, 160218: 1, 165...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[390846, 395447, 276114]</td>\n",
       "      <td>{200187: 1, 207596: 1, 343448: 1, 359544: 1, 4...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[14661, 71999, 292915, 188756, 131368]</td>\n",
       "      <td>{2906: 1, 4288: 1, 17471: 3, 56146: 1, 94588: ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   labels  \\\n",
       "0                  [33692, 13402, 393382]   \n",
       "1                                [130762]   \n",
       "2         [352578, 395447, 27512, 157031]   \n",
       "3                [390846, 395447, 276114]   \n",
       "4  [14661, 71999, 292915, 188756, 131368]   \n",
       "\n",
       "                                             feat_tf  14661  71999  292915  \\\n",
       "0  {624: 3, 4288: 1, 14403: 1, 54278: 1, 62619: 1...      0      0       0   \n",
       "1  {120505: 1, 173442: 1, 554009: 1, 634374: 1, 6...      0      0       0   \n",
       "2  {62483: 1, 73429: 1, 138155: 1, 160218: 1, 165...      0      0       0   \n",
       "3  {200187: 1, 207596: 1, 343448: 1, 359544: 1, 4...      0      0       0   \n",
       "4  {2906: 1, 4288: 1, 17471: 3, 56146: 1, 94588: ...      0      0       0   \n",
       "\n",
       "   188756  131368  \n",
       "0       0       0  \n",
       "1       0       0  \n",
       "2       0       0  \n",
       "3       0       0  \n",
       "4       0       0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(d)):\n",
    "    for j in whatiwant:\n",
    "        if j in d.loc[i, \"labels\"]:\n",
    "            d.loc[i, str(j)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14661 3\n",
      "71999 2\n",
      "292915 1\n",
      "188756 1\n",
      "131368 1\n"
     ]
    }
   ],
   "source": [
    "# num of docs per label (id)\n",
    "for j in whatiwant:\n",
    "    print(j, sum(d[str(j)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = d[d[\"14661\"]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>feat_tf</th>\n",
       "      <th>14661</th>\n",
       "      <th>71999</th>\n",
       "      <th>292915</th>\n",
       "      <th>188756</th>\n",
       "      <th>131368</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[392501, 14661, 347803]</td>\n",
       "      <td>{14403: 1, 15132: 2, 58638: 1, 85149: 1, 11593...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     labels  \\\n",
       "47  [392501, 14661, 347803]   \n",
       "\n",
       "                                              feat_tf  14661  71999  292915  \\\n",
       "47  {14403: 1, 15132: 2, 58638: 1, 85149: 1, 11593...      1      0       0   \n",
       "\n",
       "    188756  131368  \n",
       "47       0       0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, _ = get_data(\"swiki/data/train_remapped.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(list(subset.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = []\n",
    "for i in list(subset.index):\n",
    "    try:\n",
    "        ll.append(train_data[int(i)].toarray())\n",
    "    except:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "niu = np.concatenate( ll, axis=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2085164)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "niu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, test_x, test_y = train_test_split(niu, subset, random_state=42, test_size=0.30, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 2085164), (1, 2085164), (2, 7), (1, 7))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape, train_y.shape, test_x.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_x\n",
    "y_train = test_x.drop(labels = [\"labels\", \"feat_tf\"], axis = 1)\n",
    "\n",
    "x_test = train_y\n",
    "y_test = test_y.drop(labels = [\"labels\", \"feat_tf\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>14661</th>\n",
       "      <th>71999</th>\n",
       "      <th>292915</th>\n",
       "      <th>188756</th>\n",
       "      <th>131368</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    14661  71999  292915  188756  131368\n",
       "47      1      0       0       0       0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SGDClassifier()\n",
    "# lb = preprocessing.LabelBinarizer(sparse_output=True)\n",
    "# mb = preprocessing.MultiLabelBinarizer(sparse_output=True)\n",
    "# le = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=None,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "       power_t=0.5, random_state=None, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Processing 71999 tag...**\n",
      "Test set Micro F1 is 1.0\n",
      "Test set Precision is 1.0\n",
      "Test set Recall is 1.0\n",
      "\n",
      "\n",
      "**Processing 292915 tag...**\n",
      "Test set Micro F1 is 1.0\n",
      "Test set Precision is 1.0\n",
      "Test set Recall is 1.0\n",
      "\n",
      "\n",
      "**Processing 188756 tag...**\n",
      "Test set Micro F1 is 1.0\n",
      "Test set Precision is 1.0\n",
      "Test set Recall is 1.0\n",
      "\n",
      "\n",
      "**Processing 131368 tag...**\n",
      "Test set Micro F1 is 1.0\n",
      "Test set Precision is 1.0\n",
      "Test set Recall is 1.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SGD Classifier\n",
    "for category in whatiwant[1:]:\n",
    "    print('**Processing {} tag...**'.format(category))\n",
    "    \n",
    "    # Training sgd model on train data\n",
    "    clf.fit(x_train, y_train[str(category)])\n",
    "    \n",
    "    # calculating test accuracy\n",
    "    prediction = clf.predict(x_test)\n",
    "    print('Test set Micro F1 is {}'.format(f1_score(y_test[str(category)], prediction, average=\"micro\")))\n",
    "    print('Test set Precision is {}'.format(precision_score(y_test[str(category)], prediction, average=\"macro\")))\n",
    "    print('Test set Recall is {}'.format(recall_score(y_test[str(category)], prediction, average=\"micro\")))\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Using pipeline for applying logistic regression and one vs rest classifier\n",
    "LogReg_pipeline = Pipeline([\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1)),\n",
    "            ])\n",
    "for category in whatiwant[1:]:\n",
    "    print('**Processing {} tag...**'.format(category))\n",
    "    \n",
    "    # Training logistic regression model on train data\n",
    "    LogReg_pipeline.fit(x_train, y_train[str(category)])\n",
    "    \n",
    "    # calculating test accuracy\n",
    "    prediction = LogReg_pipeline.predict(x_test)\n",
    "    print('Test accuracy is {}'.format(accuracy_score(y_test[str(category)], prediction)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50312 50311\n",
      "{2143406}\n"
     ]
    }
   ],
   "source": [
    "print(len(Nn), len(Ti)) # there is one node without any parent\n",
    "print(Nn.difference(Ti)) # this is the node: this is probably the root node i guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13808"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    " def lookup_table(filename, subset):\n",
    "        \n",
    "    p2c_table = {}\n",
    "    c2p_table = {}\n",
    "    node2id = OrderedDict()\n",
    "    id2node = OrderedDict()\n",
    "    i = 0\n",
    "    \n",
    "    with open(filename, \"r\") as f:\n",
    "        if not subset:\n",
    "            head = f\n",
    "        elif isinstance(subset, int):\n",
    "            head = [next(f) for x in range(subset)] # retrieve only `n` docs\n",
    "        else:\n",
    "            raise ValueError(\"Incorrect subset type. Enter only False (boolean) or int. Encountered {} type.\".format(type(subset)))\n",
    "        for _, line in enumerate(tqdm(head)):\n",
    "            split_line = line.strip().split()\n",
    "            parent_node = int(split_line[0])\n",
    "            child_node = list(map(int, split_line[1:]))\n",
    "            \n",
    "            # map to the respective dicts -> parent:child relationship\n",
    "            # parent2child lookup table\n",
    "            if parent_node not in p2c_table:\n",
    "                p2c_table[parent_node] = [child_node[0]]\n",
    "            else:\n",
    "                p2c_table[parent_node].append(child_node[0])\n",
    "                \n",
    "            #child2parent lookup table\n",
    "            if child_node[0] not in c2p_table:\n",
    "                c2p_table[child_node[0]] = [parent_node]\n",
    "            else:\n",
    "                c2p_table[child_node[0]].append(parent_node)\n",
    "                \n",
    "            # map parent/child node to a node<->id\n",
    "            if parent_node not in node2id:\n",
    "                p_id = i\n",
    "                node2id[parent_node] = p_id\n",
    "                id2node[p_id] = parent_node\n",
    "                i+=1\n",
    "            else:\n",
    "                p_id = node2id[parent_node]\n",
    "                \n",
    "            if child_node[0] not in node2id:\n",
    "                c_id = i\n",
    "                node2id[child_node[0]] = c_id\n",
    "                id2node[c_id] = child_node[0]      \n",
    "                i+=1\n",
    "            else:\n",
    "                c_id = node2id[child_node[0]]\n",
    "\n",
    "    pi_parents = set(p2c_table.keys())        \n",
    "    T_leaves = (c2p_table.keys() - p2c_table.keys()) \n",
    "    N_all_nodes = pi_parents.union(T_leaves)\n",
    "    \n",
    "    return p2c_table, c2p_table, node2id, id2node, list(pi_parents), list(T_leaves), list(N_all_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchy2graph(p2c_table, node2id):\n",
    "\n",
    "    edges = []\n",
    "    for parent, children in p2c_table.items():\n",
    "        p_id = node2id[parent]\n",
    "        for child in children:\n",
    "            c_id = node2id[child]\n",
    "            edges.append((p_id, c_id))\n",
    "    vertices = [k for k, v in node2id.items()]\n",
    "    g = ig.Graph(n=len(node2id), edges=edges, directed=True, vertex_attrs={\"name\": vertices})\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightparameter(N_all_nodes, size_n):\n",
    "    \n",
    "    # randomly initialize weights for all nodes instead of taking user input [assumption]\n",
    "    w_all_n = {}\n",
    "    for n in N_all_nodes:\n",
    "        temp_rand = np.random.randn(size_n,)\n",
    "        if n not in w_all_n:\n",
    "            w_all_n[n] = temp_rand\n",
    "    return w_all_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"image.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_gradient(w_node, x, g, data, lmbda):\n",
    "    \n",
    "    w_n = w_all_n[w_node]\n",
    "    print(c2p_s[w_node][0])\n",
    "    w_pi_n = w_all_n[c2p_s[w_node][0]]\n",
    "    \n",
    "    C = 1 # by default\n",
    "    y_in = np.ones((len(data),))*(-1)\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        if w_node in data.loc[i, \"labels\"]:\n",
    "            y_in[i] = 1\n",
    "            print(w_node, data.loc[i, \"labels\"])\n",
    "    \n",
    "    # eqn 3.9\n",
    "    one = w_n - w_pi_n\n",
    "       \n",
    "    print(y_in.shape, w_n.shape)\n",
    "    assert y_in.shape == x.shape\n",
    "    y_w_x = y_in * w_n.T * x\n",
    "    y_x = y_in * x\n",
    "    \n",
    "    two = C*(1/(1+np.exp(y_w_x)))*y_x\n",
    "    assert(w_n.shape == two.shape)\n",
    "    g = one - two\n",
    "    \n",
    "    # eqn 3.8\n",
    "    norm_w = np.linalg.norm(one, order = 2, keepdims = False)\n",
    "    obj = C*np.log(1+np.exp(-y_w_x))\n",
    "    min_wn = norm_w + obj\n",
    "    \n",
    "    return min_wn       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_lr(data, w_node, lmbda, eps, maxfn):\n",
    "    m = 5\n",
    "    f = 0\n",
    "    xtol = 1e-30\n",
    "    iprint = [0, 1]\n",
    "    iflag = [0]\n",
    "    \n",
    "    w_n = w_all_n[w_node]\n",
    "    largest_n = largest_feat_n(data)\n",
    "    n = max(len(w_n), largest_n)\n",
    "    \n",
    "    x = np.stack(w_n)\n",
    "    g = np.zeros(x.shape)\n",
    "    \n",
    "    while(maxfn > 0):\n",
    "        f = function_gradient(w_node, x, g, data, lmbda)\n",
    "        # do lbfgs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "g = hierarchy2graph(p2c_s, n2i_s)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(g)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "layout = g.layout(\"kk\")\n",
    "g.vs[\"label\"] = g.vs[\"name\"]\n",
    "ig.plot(g, layout = layout)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, x_test = train_test_split(train_data, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "65333it [00:00, 254526.43it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "p2c, c2p, n2i, i2n, pi, T, N = lookup_table(\"swiki/data/cat_hier.txt\", subset = False)\n",
    "p2c_s, c2p_s, n2i_s, i2n_s, pi_s, T_s, N_s = lookup_table(\"swiki/data/cat_hier.txt\", subset = 10)\n",
    "w_all_n = weightparameter(N_s, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13808, 36504, 50312)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pi), len(T), len(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_n = largest_feat_n(small_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "[ 0.01262107 -0.96049248  0.80851146 -0.7210829   0.91202167 -0.27584985\n",
      "  0.69628285  0.23157     1.17538187  0.90821097]\n",
      "[ 0.01262107 -0.96049248  0.80851146 -0.7210829   0.91202167 -0.27584985\n",
      "  0.69628285  0.23157     1.17538187  0.90821097]\n",
      "2143406\n",
      "(500,) (10,)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-95-941f5461f003>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mw_n\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw_all_n\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mlr_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobjective_lr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msmall_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlmbda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxfn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m##\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;31m#         g = function_g() ##\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"over\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-82-fe7dd017824e>\u001b[0m in \u001b[0;36mobjective_lr\u001b[1;34m(data, w_node, lmbda, eps, maxfn)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mwhile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaxfn\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunction_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw_node\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlmbda\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[1;31m# do lbfgs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-94-05fedbe864f4>\u001b[0m in \u001b[0;36mfunction_gradient\u001b[1;34m(w_node, x, g, data, lmbda)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_in\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_n\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[0my_in\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0my_w_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_in\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mw_n\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0my_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_in\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result_w = np.zeros([len(N_s),])\n",
    "# while(1):\n",
    "result_w = {}\n",
    "\n",
    "for n in N_s:    \n",
    "    # check if the node has a parent\n",
    "    if n not in c2p_s:\n",
    "        w_pi_n = 0 # if the node has no parent then it's the root node. assign it w=0\n",
    "    else:\n",
    "        w_pi_n = w_all_n[n] \n",
    "    \n",
    "    # if n is not leaf node\n",
    "    if n not in T_s:\n",
    "        # update w_n using eqn 3.3 or 4\n",
    "        mod_C_n = len(p2c_s[n]) # |C_n|\n",
    "        sum_w_c = 0 \n",
    "        for c in p2c_s[n]:\n",
    "            sum_w_c += w_all_n[c]\n",
    "            \n",
    "        print(n,\":\", mod_C_n, w_pi_n, sum_w_c)\n",
    "        if n not in result_w:\n",
    "            result_w[n] = 1/(mod_C_n + 1) * (w_pi_n + sum_w_c)\n",
    "    # else: n is a leaf node\n",
    "    else:\n",
    "        #optimize using lbfgs eqn 3.8, 3.9 or 8\n",
    "        lmbda = 1\n",
    "        eps = 1e-4\n",
    "        maxfn = 1000\n",
    "        w_n = w_all_n[n]\n",
    "        print(len(w_n))\n",
    "        lr_function = objective_lr(small_df, n, lmbda, eps, maxfn) ##\n",
    "#         g = function_g() ##\n",
    "        print(\"over\")\n",
    "    print(\"ove\")\n",
    "print(\"ov\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
