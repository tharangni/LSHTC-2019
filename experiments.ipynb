{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/gcdart/MulticlassClassifier/blob/master/src/ml/LogisticRegression.java\n",
    "# https://www.kaggle.com/c/lshtc/discussion/6911#38233 - preprocessing: multilabels comma should not have spaces\n",
    "# https://www.kaggle.com/c/lshtc/discussion/14048 - dataset statistics\n",
    "## reading the LWIKI, SWIKI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import igraph as ig\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm #always use this instead of `import tqdm`\n",
    "# from sklearn.datasets import fetch_rcv1\n",
    "from scipy.sparse import *\n",
    "# np.random.seed(123)\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from collections import OrderedDict\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import preprocessing, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Memory\n",
    "from sklearn.datasets import load_svmlight_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.utils.hierarchy import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2c_table, c2p_table, node2id, id2node, pi_parents, T_leaves, N_all_nodes = lookup_table(\"swiki/data/cat_hier.txt\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg = hierarchy2graph(p2c_table, node2id, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node2id[2397997]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6009, 6010, 6011, 6012, 4337, 5649, 6013, 6014]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[node2id[x] for x in p2c_table[2397997]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25, 36, 37, 44]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[node2id[x] for x in c2p_table[2397997]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6920, 6921, 6916, 6917, 6918, 6919, 6922, 6923]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gg.incident(node2id[2397997]) # incident is different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25, 36, 37, 44, 4337, 5649, 6009, 6010, 6011, 6012, 6013, 6014]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gg.neighbors(node2id[2397997])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_degree = gg.degree(mode=\"in\")\n",
    "in_degree_nodes = np.where(np.array(in_degree)==0)[0]\n",
    "device = 'cpu'\n",
    "n=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_degree_nodes = [id2node[x] for x in in_degree_nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2143406]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_degree_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2143406"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2node[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "node2vec={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50312"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(node2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg.diameter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(in_degree_nodes), len(T_leaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = Memory(\"./mycache\")\n",
    "@mem.cache\n",
    "def get_data(filename):\n",
    "    \n",
    "    fname = str(Path(filename))\n",
    "    fe, ex = os.path.splitext(fname) \n",
    "\n",
    "    try:\n",
    "        data = load_svmlight_file(fname, n_features = 2085162 , multilabel=True, offset = 100000, length = 2800000)\n",
    "    except:\n",
    "        # Required: if the input data isn't in the correct libsvm format\n",
    "        outfile = str(Path(\"{}_small{}\".format(fe, ex)))\n",
    "#         outfile = str(Path(\"{}_remapped{}\".format(fe, ex)))\n",
    "        if not os.path.isfile(outfile):\n",
    "            logging.info(\"Remapping data to LibSVM format...\")\n",
    "            f = preprocess_libsvm(fname, outfile)\n",
    "        else:\n",
    "            logging.info(\"Using already remapped data...\")\n",
    "            f = outfile\n",
    "        data = load_svmlight_file(f, multilabel=True, n_features = 2085162 , offset = 100000, length = 2800000)\n",
    "        \n",
    "    return data[0], data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_libsvm(input_file, output_file):\n",
    "    # converts file to the required libsvm format.\n",
    "    # this is very brute force but can be made faster [IMPROVE]\n",
    "\n",
    "    file = open(output_file, \"w+\")\n",
    "    with open(input_file, \"r\") as f:\n",
    "        head = [next(f) for x in range(500)] # retrieve only `n` docs\n",
    "        for i, line in enumerate(tqdm(head)): # change to f/head depending on your needs\n",
    "            instance = line.strip().split()\n",
    "            labels = instance[0]\n",
    "            doc_dict = OrderedDict()\n",
    "            temp_dict = {}\n",
    "            temp_string = ''\n",
    "\n",
    "            for pair in instance[1:]:\n",
    "                feat = pair.split(\":\")\n",
    "                if int(feat[0]) not in temp_dict:\n",
    "                    temp_dict[int(feat[0])] = int(feat[1])\n",
    "\n",
    "            for key in sorted(temp_dict.keys()):\n",
    "                doc_dict[key] = temp_dict[key]\n",
    "\n",
    "            for feat, tf in doc_dict.items():\n",
    "                temp_string = temp_string + \"{}:{} \".format(feat, tf)        \n",
    "            file.write(\"{} {}\\n\".format(labels, temp_string))\n",
    "        file.close()\n",
    "\n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_extractor(labels):\n",
    "\n",
    "    leaf_labels = set()\n",
    "    labels_per_doc = []\n",
    "\n",
    "    for i in labels:\n",
    "        labels_per_doc.append(len(i))\n",
    "        for j in i:\n",
    "            leaf_labels.add(int(j))\n",
    "    \n",
    "    return leaf_labels, labels_per_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = Memory(\"./mycache\")\n",
    "@mem.cache\n",
    "def rr_reader(filename):\n",
    "    '''\n",
    "    create a dataframe from the data-label pair\n",
    "    '''\n",
    "\n",
    "    num_entries = 200000\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    with open(filename, \"r\") as f:\n",
    "#         head = [next(f) for x in range(num_entries)] # retrieve only `n` docs\n",
    "        for i, line in enumerate(tqdm(f)): # change to f/head depending on your needs\n",
    "            instance = line.strip().split()\n",
    "            labels = instance[0]\n",
    "            doc_dict = OrderedDict()\n",
    "            temp_dict = {}\n",
    "\n",
    "            for pair in instance[1:]:\n",
    "                feat = pair.split(\":\")\n",
    "                if int(feat[0]) not in temp_dict:\n",
    "                    temp_dict[int(feat[0])] = int(feat[1])\n",
    "\n",
    "            for key in sorted(temp_dict.keys()):\n",
    "                doc_dict[key] = temp_dict[key]\n",
    "                \n",
    "            temp_df = pd.DataFrame(data = [ labels, doc_dict ]).T\n",
    "            df = df.append(temp_df, ignore_index=True)\n",
    "    \n",
    "    df.columns = [\"labels\", \"feat_tf\"]\n",
    "    df[\"labels\"] = df[\"labels\"].apply( lambda x: list(map(int, x.split(\",\")))  )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = rr_reader(\"swiki/data/train.txt\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dta, labls = get_data(\"swiki/data/train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: fucking hell- represent documents in an embedding space -.-\n",
    "# after that we can take `each x_i` -.-\n",
    "def data_parser(dataset, dta):\n",
    "    # embeds the doc to a D-dim space\n",
    "    #create doc_vector for each instance\n",
    "#     tfidfer = TfidfVectorizer()\n",
    "    idx = list(dataset.index)\n",
    "    df_x = []\n",
    "#     for ix in tqdm(idx):\n",
    "#         twoDarray = list(dataset.loc[ix, \"feat_tf\"].items())        \n",
    "#         doc_items = []\n",
    "#         for item in twoDarray:\n",
    "#             if item[0] not in doc_items:\n",
    "#                 if item[1] > 1:\n",
    "#                     for _ in range(item[1]):\n",
    "#                         doc_items.append(str(item[0]))\n",
    "#                 else:\n",
    "#                     doc_items.append(str(item[0]))\n",
    "\n",
    "#         tfidfer.fit(doc_items)\n",
    "#         df_x.append(tfidfer.transform(doc_items))\n",
    "#             df_x.append(doc_items)\n",
    "    for ix in tqdm(idx):\n",
    "        df_x.append(dta[ix].todense())\n",
    "    dg_x = np.stack(df_x)\n",
    "    return dg_x"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def user_func(x):\n",
    "    sol = 0\n",
    "    for i in whatiwant:\n",
    "        for j in x:\n",
    "            if i == j:\n",
    "                sol = j\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "    return sol"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df[\"partition\"] = df[\"labels\"].apply(lambda x: user_func(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def largest_feat_n(df):\n",
    "    return max(df[\"feat_tf\"].apply(lambda x: len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whatiwant = [14661, 71999, 292915, 188756, 131368, 130762, 352578, 395447, 27512, 157031, 33692, 13402, 393382, 390846, 395447, 276114]\n",
    "whatiwant = [14661, 52361, 401434,316934, 369064]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SGDClassifier()\n",
    "# lb = preprocessing.LabelBinarizer(sparse_output=True)\n",
    "# mb = preprocessing.MultiLabelBinarizer(sparse_output=True)\n",
    "# le = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# SGD Classifier\n",
    "for category in whatiwant[1:]:\n",
    "    print('**Processing {} tag...**'.format(category))\n",
    "    \n",
    "    # Training sgd model on train data\n",
    "    clf.fit(x_train, y_train[str(category)])\n",
    "    \n",
    "    # calculating test accuracy\n",
    "    prediction = clf.predict(x_test)\n",
    "    print('Test set Micro F1 is {}'.format(f1_score(y_test[str(category)], prediction, average=\"micro\")))\n",
    "    print('Test set Precision is {}'.format(precision_score(y_test[str(category)], prediction, average=\"macro\")))\n",
    "    print('Test set Recall is {}'.format(recall_score(y_test[str(category)], prediction, average=\"micro\")))\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Using pipeline for applying logistic regression and one vs rest classifier\n",
    "LogReg_pipeline = Pipeline([\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1)),\n",
    "            ])\n",
    "for category in whatiwant[1:]:\n",
    "    print('**Processing {} tag...**'.format(category))\n",
    "    \n",
    "    # Training logistic regression model on train data\n",
    "    LogReg_pipeline.fit(x_train, y_train[str(category)])\n",
    "    \n",
    "    # calculating test accuracy\n",
    "    prediction = LogReg_pipeline.predict(x_test)\n",
    "    print('Test accuracy is {}'.format(accuracy_score(y_test[str(category)], prediction)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def lookup_table(filename, subset):\n",
    "        \n",
    "    p2c_table = {}\n",
    "    c2p_table = {}\n",
    "    node2id = OrderedDict()\n",
    "    id2node = OrderedDict()\n",
    "    i = 0\n",
    "    \n",
    "    with open(filename, \"r\") as f:\n",
    "        if not subset:\n",
    "            head = f\n",
    "        elif isinstance(subset, int):\n",
    "            head = [next(f) for x in range(subset)] # retrieve only `n` docs\n",
    "        else:\n",
    "            raise ValueError(\"Incorrect subset type. Enter only False (boolean) or int. Encountered {} type.\".format(type(subset)))\n",
    "        for _, line in enumerate(tqdm(head)):\n",
    "            split_line = line.strip().split()\n",
    "            parent_node = int(split_line[0])\n",
    "            child_node = list(map(int, split_line[1:]))\n",
    "            \n",
    "            # map to the respective dicts -> parent:child relationship\n",
    "            # parent2child lookup table\n",
    "            if parent_node not in p2c_table:\n",
    "                p2c_table[parent_node] = [child_node[0]]\n",
    "            else:\n",
    "                p2c_table[parent_node].append(child_node[0])\n",
    "                \n",
    "            #child2parent lookup table\n",
    "            if child_node[0] not in c2p_table:\n",
    "                c2p_table[child_node[0]] = [parent_node]\n",
    "            else:\n",
    "                c2p_table[child_node[0]].append(parent_node)\n",
    "                \n",
    "            # map parent/child node to a node<->id\n",
    "            if parent_node not in node2id:\n",
    "                p_id = i\n",
    "                node2id[parent_node] = p_id\n",
    "                id2node[p_id] = parent_node\n",
    "                i+=1\n",
    "            else:\n",
    "                p_id = node2id[parent_node]\n",
    "                \n",
    "            if child_node[0] not in node2id:\n",
    "                c_id = i\n",
    "                node2id[child_node[0]] = c_id\n",
    "                id2node[c_id] = child_node[0]      \n",
    "                i+=1\n",
    "            else:\n",
    "                c_id = node2id[child_node[0]]\n",
    "\n",
    "    pi_parents = set(p2c_table.keys())        \n",
    "    T_leaves = (c2p_table.keys() - p2c_table.keys()) \n",
    "    N_all_nodes = pi_parents.union(T_leaves)\n",
    "    \n",
    "    return p2c_table, c2p_table, node2id, id2node, list(pi_parents), list(T_leaves), list(N_all_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchy2graph(p2c_table, node2id):\n",
    "\n",
    "    edges = []\n",
    "    for parent, children in p2c_table.items():\n",
    "        p_id = node2id[parent]\n",
    "        for child in children:\n",
    "            c_id = node2id[child]\n",
    "            edges.append((p_id, c_id))\n",
    "    vertices = [k for k, v in node2id.items()]\n",
    "    g = ig.Graph(n=len(node2id), edges=edges, directed=True, vertex_attrs={\"name\": vertices})\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hier(filename):\n",
    "    \n",
    "    N = set()\n",
    "    pi = set()\n",
    "    T = set()\n",
    "#     this is wrong\n",
    "    with open(filename, \"r\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            words = line.strip().split()\n",
    "            pi.add(int(words[0])) #adding parent node\n",
    "            T.add(int(words[1])) #adding ALL leaf nodes in the hierarchy\n",
    "            for w in words:\n",
    "                N.add(int(w))\n",
    "\n",
    "    return N, pi, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p2c, c2p, n2i, i2n, pi, T, N = lookup_table(\"swiki/data/cat_hier.txt\", subset = False)\n",
    "p2c_s, c2p_s, n2i_s, i2n_s, pi_s, T_s, N_s = lookup_table(\"swiki/data/cat_hier.txt\", subset = 25)\n",
    "# w_all_n, rev_all_n = weightparameter(N, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pi) + len(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pi) , len(T), len(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nn, pii, Ti = read_hier(\"swiki/data/cat_hier.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Nn), len(Ti)) # there is one node without any parent\n",
    "print(Nn.difference(Ti)) # this is the node: this is probably the root node i guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(pii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = hierarchy2graph(p2c_s, n2i_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.get_diameter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = next(g.bfsiter(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(l.neighbors())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind= g.degree(type=\"in\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.neighbors(n2i[2143406])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.uniform(1,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i2n_s[np.where(np.array(ind) == 0)[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchy_vectors(graph_obj, ix2node, node2ix, p2c, n):\n",
    "    \n",
    "    # maybe even return the depth level of the node while returning the vector?\n",
    "    # ensure that all the vectors are unique\n",
    "    node2vec = {}\n",
    "    depth_level = [1e-5, 1e-4, 1e-4, 1e-3, 1e-3, 1e-2, 1e-2, 1e-2, 1e-2, 1e-2, 1e-2]\n",
    "    \n",
    "    # 1. find the root node. in degree = 0\n",
    "    in_degree = graph_obj.degree(type = \"in\")\n",
    "    root_node = ix2node[np.where(np.array(in_degree)==0)[0][0]]\n",
    "    \n",
    "    # 2. generate random vector for root\n",
    "    root_vector = np.random.randn(n,)\n",
    "    \n",
    "    for parent, children in p2c.items():\n",
    "        if parent == root_node:\n",
    "            node2vec[parent] = root_vector\n",
    "\n",
    "        # 3. find children\n",
    "        # 4. generate random vectors for children of parent (neighbours) at uniform randomness\n",
    "        for each_child in children:\n",
    "            i = len(graph_obj.get_shortest_paths(node2ix[root_node], node2ix[each_child])[0]) - 2\n",
    "            rand = random.uniform(1, 9)*depth_level[i]\n",
    "            if each_child not in node2vec:\n",
    "                curr_vector = node2vec[parent] + rand\n",
    "                node2vec[each_child] = curr_vector\n",
    "\n",
    "    return node2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = hierarchy2graph(p2c, n2i)\n",
    "n2v = hierarchy_vectors(g, i2n, n2i, p2c, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(g.get_shortest_paths(0, 69)[0])-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g.get_diameter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2c_s[2143406]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(n2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2c_s, c2p_s, n2i_s, i2n_s, pi_s, T_s, N_s = lookup_table(\"swiki/data/cat_hier.txt\", subset = 100)\n",
    "g = hierarchy2graph(p2c_s, n2i_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.get_diameter() # returns indices of the smallest node number at each level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.neighbors(70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i2n[70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = g.layout(\"kk\")\n",
    "g.vs[\"label\"] = g.vs[\"name\"]\n",
    "ig.plot(g, layout = layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"image.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightparameter(N_all_nodes, size_n):\n",
    "    \n",
    "    # randomly initialize weights for all nodes instead of taking user input [assumption]\n",
    "    w_all_n = {}\n",
    "    rev_mapping_w = {}\n",
    "    for n in N_all_nodes:\n",
    "        temp_rand = np.random.randn(size_n,1)\n",
    "        if n not in w_all_n:\n",
    "            w_all_n[n] = temp_rand\n",
    "        if temp_rand not in rev_mapping_w:\n",
    "            rev_mapping_w[temp_rand] = n\n",
    "    return w_all_n, rev_mapping_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_gradient(w_n, w_node, x, data):\n",
    "    \n",
    "    w_pi_n = w_all_n[c2p[w_node][0]]\n",
    "    \n",
    "    C = 1 # by default\n",
    "    one = w_n - w_pi_n\n",
    "    norm_w = np.linalg.norm(x = one, ord = 'fro', keepdims = False)\n",
    "    \n",
    "#     g = []\n",
    "#     g = g.reshape((-1,1))\n",
    "    y_in = -1 * np.ones((w_n.shape))\n",
    "\n",
    "    for i, j in enumerate(data.index):\n",
    "        if w_node in data.loc[j, \"labels\"]:\n",
    "            y_in[i] = 1\n",
    "            print(\"yes\", w_node, data.loc[j, \"labels\"])\n",
    "\n",
    "    y_w_x = np.dot(y_in, np.dot(np.transpose(w_n), np.transpose(x)))\n",
    "\n",
    "    # eqn 3.8\n",
    "    obj = C * np.sum(np.log(1+np.exp(-y_w_x)), axis = 1, keepdims = True)\n",
    "    assert w_n.shape == obj.shape\n",
    "    \n",
    "#     y_x = np.dot(y_in.T, x.T)\n",
    "#     two = C*(1/(1+np.exp(y_w_x)))*y_x\n",
    "#     assert(one.shape == two.shape)\n",
    "#     g.append(one - two)\n",
    "\n",
    "    min_wn = norm_w + obj\n",
    "    \n",
    "    return min_wn      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_lr(data, w_node, lmbda, eps, maxfn, fff):\n",
    "    m = 5\n",
    "    f = 0\n",
    "    xtol = 1e-30\n",
    "    w_n = w_all_n[w_node]\n",
    "    maxfn = 0\n",
    "    init_x = [2, 2]\n",
    "    largest_n = largest_feat_n(data)\n",
    "    n = max(len(w_n), largest_n)\n",
    "    \n",
    "    x = fff\n",
    "    g = np.zeros(x.shape)\n",
    "    while(maxfn < 10):\n",
    "        \n",
    "#         f = function_gradient(w_n, w_node, x, data)\n",
    "#         try:\n",
    "        optimize.minimize(fun = function_gradient, x0 = init_x, args=(w_n), method='L-BFGS-B', tol=xtol, options={'disp': True, 'eps': eps })\n",
    "#         except:\n",
    "#             print(\"didn't converge\")\n",
    "            \n",
    "        maxfn +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_n = largest_feat_n(small_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# result_w = np.zeros((len(N_s),1))\n",
    "# while(1):\n",
    "result_w = {}\n",
    "\n",
    "for n in N_s:    \n",
    "    print(n)\n",
    "    # check if the node has a parent\n",
    "    if n not in c2p:\n",
    "        w_pi_n = 0 # if the node has no parent then it's the root node. assign it w=0\n",
    "    else:\n",
    "        w_pi_n = w_all_n[n] \n",
    "    \n",
    "    # if n is not leaf node\n",
    "    if n not in T_s:\n",
    "        # update w_n using eqn 3.3 or 4\n",
    "        mod_C_n = len(p2c[n]) # |C_n|\n",
    "        sum_w_c = 0 \n",
    "        for c in p2c[n]:\n",
    "            sum_w_c += w_all_n[c]\n",
    "            \n",
    "        if n not in result_w:\n",
    "            result_w[n] = 1/(mod_C_n + 1) * (w_pi_n + sum_w_c)\n",
    "    # else: n is a leaf node\n",
    "    else:\n",
    "        #optimize using lbfgs eqn 3.8, 3.9 or 8\n",
    "        lmbda = 1\n",
    "        eps = 1e-4\n",
    "        maxfn = 10\n",
    "        w_n = w_all_n[n]\n",
    "        lr_function = objective_lr(small_df, n, lmbda, eps, maxfn, gg) ##\n",
    "#         g = function_g() ##\n",
    "        print(\"over\")\n",
    "    print(\"ove\")\n",
    "print(\"ov\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, x_test = train_test_split(train_data, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pi), len(T), len(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
