{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/gcdart/MulticlassClassifier/blob/master/src/ml/LogisticRegression.java\n",
    "# https://www.kaggle.com/c/lshtc/discussion/6911#38233 - preprocessing: multilabels comma should not have spaces\n",
    "# https://www.kaggle.com/c/lshtc/discussion/14048 - dataset statistics\n",
    "## reading the LWIKI, SWIKI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.datasets import fetch_rcv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Memory\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "mem = Memory(\"./mycache\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_list = []"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# test set takes 6:19s to load\n",
    "with open(\"large_lshtc/data/train.txt\", \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "#     for i, line in enumerate(tqdm(f)):\n",
    "        if line not in train_list:\n",
    "            train_list.append(line.strip())\n",
    "        if (i%10000==1000):\n",
    "            print(i)\n",
    "            break"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "len(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@mem.cache\n",
    "def get_data(filename):\n",
    "    \n",
    "    try:\n",
    "        data = load_svmlight_file(filename, multilabel=True)\n",
    "    \n",
    "    except:\n",
    "        # when there is an error since the dataset isn't in the correct libsvm format. *sigh*\n",
    "        # so we need to ValueError: Feature indices in SVMlight/LibSVM data file should be sorted and unique get rid of that\n",
    "        # *SIGH*\n",
    "        \n",
    "        # *call function to preprocess*\n",
    "    return data[0], data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"swiki/data/train_remapped.txt\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "57767it [06:16, 68.20it/s] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-f2c416406b18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mtemp_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp_string\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"{}:{} \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mtemp_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtemp_string\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mthe_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mthe_file\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtemp_string\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open(\"swiki/data/train.txt\", \"r\") as f:\n",
    "    the_file = ''\n",
    "    for i, line in enumerate(tqdm(f)):\n",
    "        instance = line.strip().split()\n",
    "        labels = instance[0]\n",
    "        doc_dict = OrderedDict()\n",
    "        temp_dict = {}\n",
    "        temp_string = ''\n",
    "        \n",
    "        for pair in instance[1:]:\n",
    "            feat = pair.split(\":\")\n",
    "            if int(feat[0]) not in temp_dict:\n",
    "                temp_dict[int(feat[0])] = int(feat[1])\n",
    "        \n",
    "        for key in sorted(temp_dict.keys()):\n",
    "            doc_dict[key] = temp_dict[key]\n",
    "        \n",
    "        for feat, tf in doc_dict.items():\n",
    "            temp_string = temp_string + \"{}:{} \".format(feat, tf)        \n",
    "        temp_string = labels + \" \" + temp_string\n",
    "        the_file = the_file + temp_string + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "57767it [06:30, 68.20it/s]"
     ]
    }
   ],
   "source": [
    "file.write(the_file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rcv1 = fetch_rcv1(data_home=\"./mycache\", subset = \"train\", download_if_missing=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = pd.DataFrame(rcv1.data.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "[Memory] Calling __main__-C%3A-users-harshasivajit-Documents-master-ai-rr13-__ipython-input__.get_data...\n",
      "get_data('swiki/data/train.txt')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Feature indices in SVMlight/LibSVM data file should be sorted and unique.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-76496f513d8f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# x contains all the feat:value data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# y contains all the labels (per line)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"swiki/data/train.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m# a, b = get_data(\"test-remapped/test-remapped.csv\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\harshasivajit\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cached_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\harshasivajit\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m_cached_call\u001b[1;34m(self, args, kwargs, shelving)\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmust_call\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 534\u001b[1;33m             \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    535\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmmap_mode\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m                 \u001b[1;31m# Memmap the output at the first call to be consistent with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\harshasivajit\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    732\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_verbose\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    733\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 734\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    735\u001b[0m         self.store_backend.dump_item(\n\u001b[0;32m    736\u001b[0m             [func_id, args_id], output, verbose=self._verbose)\n",
      "\u001b[1;32m<ipython-input-3-285dc5b74327>\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mmem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_svmlight_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmultilabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\harshasivajit\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\datasets\\svmlight_format.py\u001b[0m in \u001b[0;36mload_svmlight_file\u001b[1;34m(f, n_features, dtype, multilabel, zero_based, query_id, offset, length)\u001b[0m\n\u001b[0;32m    154\u001b[0m     \"\"\"\n\u001b[0;32m    155\u001b[0m     return tuple(load_svmlight_files([f], n_features, dtype, multilabel,\n\u001b[1;32m--> 156\u001b[1;33m                                      zero_based, query_id, offset, length))\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\harshasivajit\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\datasets\\svmlight_format.py\u001b[0m in \u001b[0;36mload_svmlight_files\u001b[1;34m(files, n_features, dtype, multilabel, zero_based, query_id, offset, length)\u001b[0m\n\u001b[0;32m    297\u001b[0m     r = [_open_and_load(f, dtype, multilabel, bool(zero_based), bool(query_id),\n\u001b[0;32m    298\u001b[0m                         offset=offset, length=length)\n\u001b[1;32m--> 299\u001b[1;33m          for f in files]\n\u001b[0m\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     if (zero_based is False or\n",
      "\u001b[1;32mc:\\users\\harshasivajit\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\datasets\\svmlight_format.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    297\u001b[0m     r = [_open_and_load(f, dtype, multilabel, bool(zero_based), bool(query_id),\n\u001b[0;32m    298\u001b[0m                         offset=offset, length=length)\n\u001b[1;32m--> 299\u001b[1;33m          for f in files]\n\u001b[0m\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     if (zero_based is False or\n",
      "\u001b[1;32mc:\\users\\harshasivajit\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\datasets\\svmlight_format.py\u001b[0m in \u001b[0;36m_open_and_load\u001b[1;34m(f, dtype, multilabel, zero_based, query_id, offset, length)\u001b[0m\n\u001b[0;32m    185\u001b[0m             \u001b[0mactual_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindptr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m                 _load_svmlight_file(f, dtype, multilabel, zero_based, query_id,\n\u001b[1;32m--> 187\u001b[1;33m                                     offset, length)\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[1;31m# convert from array.array, give data the right dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32msklearn\\datasets\\_svmlight_format.pyx\u001b[0m in \u001b[0;36msklearn.datasets._svmlight_format._load_svmlight_file\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Feature indices in SVMlight/LibSVM data file should be sorted and unique."
     ]
    }
   ],
   "source": [
    "# x contains all the feat:value data\n",
    "# y contains all the labels (per line)\n",
    "x, y = get_data(\"swiki/data/train.txt\")\n",
    "# a, b = get_data(\"test-remapped/test-remapped.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape #num documents X num features (training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{} size\".format(x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see how many unique labels are there and if it verifies with the paper\n",
    "# ooooh i only get the leaf labels with this. or like atleast when i add it to a set.\n",
    "# how do i get class labels? what are class labels first!\n",
    "leaf_labels = set()\n",
    "class_labels = []\n",
    "count_y = []\n",
    "for i in y:\n",
    "    count_y.append(len(i))\n",
    "    for j in i:\n",
    "        leaf_labels.add(j)\n",
    "        class_labels.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(leaf_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these leaf labels range from 1 - 445729 (training set) this number is the max label id\n",
    "list(leaf_labels)[325055] #[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just the plain brute force counting of all labels for all documents.\n",
    "# same as count_y\n",
    "len((class_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg num_labels per instance(training doc)\n",
    "sum(count_y)/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape\n",
    "# len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaf_t = set()\n",
    "class_t = []\n",
    "\n",
    "for i in b:\n",
    "    for j in i:\n",
    "        leaf_t.add(j)\n",
    "        class_t.append(j)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "leaf_t.pop() # pops the 0 from label set since it's not required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(class_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "904334/2 # <- num of labels are doubled basically due to the 0 in every document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hierarchy.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hier(filename):\n",
    "    \n",
    "    unique_h = set()\n",
    "    \n",
    "    with open(filename, \"r\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            words = line.strip().split()\n",
    "            for w in words:\n",
    "                unique_h.add(int(w))\n",
    "\n",
    "    return unique_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = read_hier(\"hierarchy/hierarchy.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(u)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
