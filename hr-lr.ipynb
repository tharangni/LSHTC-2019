{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/gcdart/MulticlassClassifier/blob/master/src/ml/LogisticRegression.java\n",
    "# https://www.kaggle.com/c/lshtc/discussion/6911#38233 - preprocessing: multilabels comma should not have spaces\n",
    "# https://www.kaggle.com/c/lshtc/discussion/14048 - dataset statistics\n",
    "## reading the LWIKI, SWIKI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "from tqdm import tqdm #always use this instead of `import tqdm`\n",
    "# from sklearn.datasets import fetch_rcv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from collections import OrderedDict\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Memory\n",
    "from sklearn.datasets import load_svmlight_file"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_list = []"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# test set takes 6:19s to load\n",
    "with open(\"large_lshtc/data/train.txt\", \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "#     for i, line in enumerate(tqdm(f)):\n",
    "        if line not in train_list:\n",
    "            train_list.append(line.strip())\n",
    "        if (i%10000==1000):\n",
    "            print(i)\n",
    "            break"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "len(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:00, 598.41it/s]\n"
     ]
    }
   ],
   "source": [
    "file = open(\"swiki/data/dum_remapped.txt\", \"w+\")\n",
    "\n",
    "with open(\"swiki/data/dum.txt\", \"r\") as f:\n",
    "    for _, line in enumerate(tqdm(f)):\n",
    "        instance = line.strip().split()\n",
    "        labels = instance[0]\n",
    "        print(labels)\n",
    "        doc_dict = OrderedDict()\n",
    "        temp_dict = {}\n",
    "        temp_string = ''\n",
    "\n",
    "        for pair in instance[1:]:\n",
    "            feat = pair.split(\":\")\n",
    "            if int(feat[0]) not in temp_dict:\n",
    "                temp_dict[int(feat[0])] = int(feat[1])\n",
    "\n",
    "        for key in sorted(temp_dict.keys()):\n",
    "            doc_dict[key] = temp_dict[key]\n",
    "\n",
    "        for feat, tf in doc_dict.items():\n",
    "            temp_string = temp_string + \"{}:{} \".format(feat, tf)        \n",
    "        file.write(\"{} {}\\n\".format(labels, temp_string))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_libsvm(input_file, output_file):\n",
    "    \n",
    "    file = open(output_file, \"w\")\n",
    "\n",
    "    with open(input_file, \"r\") as f:\n",
    "        the_file = ''\n",
    "        for i, line in enumerate(tqdm(f)):\n",
    "            instance = line.strip().split()\n",
    "            labels = instance[0]\n",
    "            doc_dict = OrderedDict()\n",
    "            temp_dict = {}\n",
    "            temp_string = ''\n",
    "\n",
    "            for pair in instance[1:]:\n",
    "                feat = pair.split(\":\")\n",
    "                if int(feat[0]) not in temp_dict:\n",
    "                    temp_dict[int(feat[0])] = int(feat[1])\n",
    "                else:\n",
    "                    print(feat[0])\n",
    "\n",
    "            for key in sorted(temp_dict.keys()):\n",
    "                doc_dict[key] = temp_dict[key]\n",
    "\n",
    "            for feat, tf in doc_dict.items():\n",
    "                temp_string = temp_string + \"{}:{} \".format(feat, tf)        \n",
    "            temp_string = labels + \" \" + temp_string\n",
    "            the_file = the_file + temp_string + \"\\n\"\n",
    "\n",
    "    file.write(the_file)\n",
    "    file.close()\n",
    "    \n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = Memory(\"./mycache\")\n",
    "@mem.cache\n",
    "def get_data(filename):\n",
    "    \n",
    "    try:\n",
    "        print(\"try\")\n",
    "        data = load_svmlight_file(filename, multilabel=True)\n",
    "    \n",
    "    except:\n",
    "        # when there is an error since the dataset isn't in the correct libsvm format. *sigh*\n",
    "        # so we need to ValueError: Feature indices in SVMlight/LibSVM data file should be sorted and unique get rid of that\n",
    "        # *SIGH*\n",
    "                \n",
    "        temp = filename.split(\".\")\n",
    "        outfile = temp[0] + \"_remapped.\" + temp[1]\n",
    "        \n",
    "        if not os.path.isfile(outfile):\n",
    "            logging.warning(\"Remapping data to LibSVM format...\")\n",
    "            f = preprocess_libsvm(filename, outfile)\n",
    "        else:\n",
    "            logging.warning(\"Using already remapped data...\")\n",
    "            f = outfile\n",
    "            \n",
    "        data = load_svmlight_file(f, multilabel=True)\n",
    "        \n",
    "    return data[0], data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "fff = \"..\\swiki\\data\\dummy.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe, ex = os.path.splitext(fff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.txt'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rcv1 = fetch_rcv1(data_home=\"./mycache\", subset = \"train\", download_if_missing=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = pd.DataFrame(rcv1.data.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:[MemorizedFunc(func=<function get_data at 0x0000016EEF7EF268>, location=./mycache\\joblib)]: Clearing function cache identified by __main__-C%3A-Users-harshasivajit-Documents-master-ai-rr13-__ipython-input__\\get_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "[Memory] Calling __main__-C%3A-Users-harshasivajit-Documents-master-ai-rr13-__ipython-input__.get_data...\n",
      "get_data('swiki/data/dummy.txt')\n",
      "try\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Using already remapped data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________get_data - 0.0s, 0.0min\n"
     ]
    }
   ],
   "source": [
    "# x contains all the feat:value data\n",
    "# y contains all the labels (per line)\n",
    "# x, y = get_data(\"swiki/data/train.txt\")\n",
    "# a, b = get_data(\"swiki/data/test_remapped.txt\")\n",
    "x, y = get_data(\"swiki/data/dummy.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 2076509)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape #num documents X num features (training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 2076509) size\n"
     ]
    }
   ],
   "source": [
    "print(\"{} size\".format(x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(85048.0, 175937.0, 189243.0),\n",
       " (302422.0, 378629.0),\n",
       " (13402.0, 33692.0, 393382.0),\n",
       " (130762.0,),\n",
       " (27512.0, 157031.0, 352578.0, 395447.0),\n",
       " (276114.0, 390846.0, 395447.0),\n",
       " (14661.0, 71999.0, 131368.0, 188756.0, 292915.0),\n",
       " (106615.0, 228092.0),\n",
       " (228506.0, 420589.0),\n",
       " (207086.0, 307963.0),\n",
       " (119558.0, 159586.0)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see how many unique labels are there and if it verifies with the paper\n",
    "# ooooh i only get the leaf labels with this. or like atleast when i add it to a set.\n",
    "# how do i get class labels? what are class labels first!\n",
    "leaf_labels = set()\n",
    "class_labels = []\n",
    "count_y = []\n",
    "for i in y:\n",
    "    count_y.append(len(i))\n",
    "    for j in i:\n",
    "        leaf_labels.add(j)\n",
    "        class_labels.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(leaf_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these leaf labels range from 1 - 445729 (training set) this number is the max label id\n",
    "list(leaf_labels)[325055] #[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just the plain brute force counting of all labels for all documents.\n",
    "# same as count_y\n",
    "len((class_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg num_labels per instance(training doc)\n",
    "sum(count_y)/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape\n",
    "# len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaf_t = set()\n",
    "class_t = []\n",
    "\n",
    "for i in b:\n",
    "    for j in i:\n",
    "        leaf_t.add(j)\n",
    "        class_t.append(j)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "leaf_t.pop() # pops the 0 from label set since it's not required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(class_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "904334/2 # <- num of labels are doubled basically due to the 0 in every document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hierarchy.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hier(filename):\n",
    "    \n",
    "    unique_h = set()\n",
    "    \n",
    "    with open(filename, \"r\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            words = line.strip().split()\n",
    "            for w in words:\n",
    "                unique_h.add(int(w))\n",
    "\n",
    "    return unique_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u = read_hier(\"hierarchy/hierarchy.txt\")\n",
    "u = read_hier(\"swiki/data/cat_hier.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
