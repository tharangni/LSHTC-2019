{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import math\n",
    "import heapq\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from hierarchy import *\n",
    "from processing import *\n",
    "from label_utils import *\n",
    "from data_reading import *\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, classification_report\n",
    "from tqdm import tqdm_notebook as tqdm \n",
    "\n",
    "logging.basicConfig(level=logging.INFO )\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=5)\n",
    "torch.set_flush_denormal(False)\n",
    "num_gpus = torch.cuda.device_count()\n",
    "device = torch.device(\"cuda\" if (torch.cuda.is_available() and num_gpus > 0) else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 300\n",
    "MAX_VALUE = torch.finfo(torch.float).max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_data(y):\n",
    "\n",
    "    class_labels = np.unique(y)\n",
    "    num_tasks = len(class_labels)\n",
    "    num_examples = y.shape[0]\n",
    "    if num_tasks == 1:\n",
    "        raise ValueError(\"The number of classes has to be greater than one.\")\n",
    "    elif num_tasks == 2:\n",
    "        if 1 in class_labels and -1 in class_labels:\n",
    "            num_tasks = 1\n",
    "            class_labels = np.array([-1, 1])\n",
    "        elif 1 in class_labels and 0 in class_labels:\n",
    "            num_tasks = 1\n",
    "            class_labels = np.array([0, 1])\n",
    "        else:\n",
    "            raise ValueError(\"Unable to decide postive label\")\n",
    "\n",
    "    lbin = LabelBinarizer(neg_label=-1, pos_label=1)\n",
    "    lbin.fit(class_labels)\n",
    "    y_bin = lbin.transform(y)\n",
    "    return y_bin, class_labels, num_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetIterator:\n",
    "    def __init__(self, datafile, catfile, subsample, is_directed, fmt, split):\n",
    "        self.datafile = datafile\n",
    "        self.cat = HierarchyUtils(catfile, n_components, False)\n",
    "\n",
    "        if fmt == \"libsvm\":\n",
    "            self.lib_data = LIBSVM_Reader(self.datafile, True, n_components, subsample, split)\n",
    "            self.d_df = self.lib_data.data_df\n",
    "            self.d_df[\"doc_labels\"] = self.d_df[\"doc_labels\"].apply(lambda x: x[0])\n",
    "            self.d_df = self.d_df.loc[self.d_df['doc_labels'].isin(self.cat.N_all_nodes)]\n",
    "            \n",
    "            self.all_sub_x = self.lib_data.all_x[self.d_df.index, :]\n",
    "            \n",
    "            orderer = {}\n",
    "            for i, ind in enumerate(self.d_df.index):\n",
    "                orderer[ind] = i\n",
    "            \n",
    "            self.df = self.d_df.rename(index=orderer)\n",
    "            \n",
    "            self.MLmatrix = self.lib_data.label_matrix\n",
    "            self.ml_sub_matrix = self.MLmatrix[self.d_df.index, ]\n",
    "            self.MLbin = self.lib_data.binarizer\n",
    "            self.r_df = self.lib_data.rev_df\n",
    "            \n",
    "        elif fmt ==\"raw\":\n",
    "            # TODO: add split here too\n",
    "            self.raw_df = CSV_Reader(self.datafile, subsample)\n",
    "            self.df = self.raw_df.data_df\n",
    "            self.r_df = self.raw_df.rev_df\n",
    "        \n",
    "#         self.wn = self.cat.generate_vectors(device = device, neighbours = True)\n",
    "\n",
    "    def read_df(self, idx):\n",
    "        i = idx\n",
    "        return self.df.at[i, \"doc_id\"], self.df.at[i, \"doc_vector\"], self.df.at[i, \"doc_labels\"], i\n",
    "    \n",
    "    def __getitem__(self, _id):\n",
    "        return self.read_df(_id)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for _id in self.df.index:\n",
    "            yield self[_id]\n",
    "\n",
    "\n",
    "class DatasetModule(Dataset):\n",
    "\n",
    "    def __init__(self, root_location, cat_file, subsample, is_directed, fmt, split):\n",
    "        \n",
    "        self.iter = DatasetIterator(root_location, cat_file, subsample, is_directed, fmt, split)\n",
    "\n",
    "        self.small_mapper = self.iter.cat.node2id\n",
    "        self.y_bin = generate_binary_yin(self.iter.cat.N_all_nodes)\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    def encode_labels(self, labels):\n",
    "        \n",
    "        y_in = self.y_bin[self.small_mapper[int(labels)]]\n",
    "        y = torch.as_tensor(y_in>0, dtype=torch.float32, device=device)\n",
    "            \n",
    "        return y_in, y\n",
    "    \n",
    "    def encode_doc(self, doc_id_list):\n",
    "        doc_V = torch.empty((n_components, len(doc_id_list)), dtype=torch.float32, device=device)\n",
    "        for i, doc in enumerate(doc_id_list):\n",
    "            doc_V[:, i] = self.iter.data_df.at[doc, \"doc_vector\"]\n",
    "        return doc_V\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.iter.df)\n",
    "\n",
    "    def __load(self, idx):\n",
    "        doc_id, doc_vec, dec_labels, i = self.iter[idx]\n",
    "        \n",
    "        return doc_id, doc_vec, dec_labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.__load(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestsetModule(Dataset):\n",
    "    def __init__(self, location, subsample, fmt, split):\n",
    "        \n",
    "        self.location = location\n",
    "        \n",
    "        if fmt == \"libsvm\":\n",
    "            self.lib_data = LIBSVM_Reader(self.location, True, n_components, subsample, split)\n",
    "            self.df = self.lib_data.data_df\n",
    "            \n",
    "        elif fmt ==\"raw\":\n",
    "            # TODO: add split here too\n",
    "            self.raw_df = CSV_Reader(self.datafile, subsample)\n",
    "            self.df = self.raw_df.data_df\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.df.at[idx, \"doc_id\"], self.df.at[idx, \"doc_vector\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DatasetModule(\"../lshtc-small/train.txt\", \"../lshtc-small/sub_cat_hier.txt\", False, True, \"libsvm\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = DatasetModule(\"../lshtc-small/validation.txt\", \"../lshtc-small/sub_cat_hier.txt\", False, True, \"libsvm\", \"test\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "c, l = class_statistics(train_data.iter.data_df, \"dmoz\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = DatasetModule(\"../lshtc-small/test.txt\", \"../lshtc-small/sub_cat_hier.txt\", False, True, \"libsvm\", \"test\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_data = DatasetModule(\"../swiki/data/train_remapped.txt\", \"../swiki/data/cat_hier.txt\", False, True, \"libsvm\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_data = DatasetModule(\"../swiki/data/test_remapped.txt\", \"../swiki/data/cat_hier.txt\", False, False, \"libsvm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "batch size affects performance. higher batch size(99) vs. lower(40) for rcv1 didn't converge properly while training\n",
    "'''\n",
    "\n",
    "def my_collate(batch):\n",
    "    \n",
    "    label_id = [item[0] for item in batch]\n",
    "    doc_vecs = [item[1] for item in batch]\n",
    "    \n",
    "    y_in = [item[2] for item in batch]\n",
    "    y01 = [item[3] for item in batch]\n",
    "    \n",
    "    doc_vecs = torch.cat(doc_vecs, 0).t()\n",
    "    y_in = torch.cat(y_in, 0)\n",
    "    y01 = torch.cat(y01, 0)\n",
    "    \n",
    "    return [label_id, doc_vecs, y_in, y01]\n",
    "\n",
    "batch_size = 1\n",
    "validation_split = 0.01\n",
    "shuffle_dataset = True\n",
    "random_seed= 1273\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(train_data)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "# train_loader = DataLoader(train_data, batch_size= batch_size, shuffle=True)\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=1)\n",
    "validation_loader = DataLoader(valid_data, shuffle=True, batch_size=1)\n",
    "# validation_loader = DataLoader(train_data, sampler=valid_sampler, batch_size=1)\n",
    "test_loader = DataLoader(test_data, batch_size=1, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.version.cuda\n",
    "torch.backends.cudnn.version()\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RRLoss(nn.Module):\n",
    "    def __init__(self, n_node):\n",
    "        super().__init__()\n",
    "        self.n_node = n_node\n",
    "        self.H = train_data.iter.cat\n",
    "        \n",
    "    def forward(self, n_node_vec, pi_node_vec):\n",
    "\n",
    "        w_n, w_pi = n_node_vec, pi_node_vec\n",
    "        w_pi_no_grad = w_pi.detach()\n",
    "\n",
    "        param = torch.norm(w_n - w_pi_no_grad, 2)\n",
    "        param = (1e-6) * 0.5*param**2\n",
    "                \n",
    "        norm_sq = param\n",
    "\n",
    "        return norm_sq\n",
    "    \n",
    "    def non_leaf_update(self, Cn, w_pi_n, w_c):\n",
    "\n",
    "        W = (1/(len(Cn)+1)) * (w_pi_n + w_c)\n",
    "        return W\n",
    "    \n",
    "    \n",
    "class Node(nn.Module):\n",
    "    def __init__(self, node_n):\n",
    "        super().__init__()\n",
    "\n",
    "        self.node_n = node_n\n",
    "        \n",
    "#         w_n = train_data.iter.cat.W_nodes[self.node_n]\n",
    "        w_ = [n_components, 1]\n",
    "        w_n = torch.FloatTensor(*w_)\n",
    "        weights_n = nn.init.kaiming_normal_(w_n, mode=\"fan_out\")\n",
    "        \n",
    "        self.weight = nn.Parameter(weights_n)    \n",
    "        \n",
    "    def forward(self, x_i):\n",
    "        \n",
    "        out = x_i.mm(self.weight)\n",
    "#         out = F.sigmoid(out)      \n",
    "        return out\n",
    "    \n",
    "class HRLR(nn.Module):\n",
    "    def __init__(self, node_n):\n",
    "        super().__init__()\n",
    "        self.linear = Node(node_n)\n",
    "\n",
    "    def forward(self, x_i):\n",
    "        score = self.linear.forward(x_i)\n",
    "        return score\n",
    "    \n",
    "    def compute_loss_leaf(self, yin, output):\n",
    "        \n",
    "        m = nn.Sigmoid()\n",
    "        fwd_pass = -yin * output\n",
    "        exp = torch.exp(fwd_pass)\n",
    "#         loss = m(fwd_pass)\n",
    "        \n",
    "        if torch.isinf(exp).sum():\n",
    "            loss_inf = MAX_VALUE/1e30\n",
    "            exp = torch.tensor(loss_inf, device=device, dtype=torch.float32, requires_grad=True) #.expand(exp.shape)\n",
    "            \n",
    "        loss = torch.log1p(exp)\n",
    "        value = torch.sum(loss) \n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_leaf_update(non_leaf_, main_path):\n",
    "    H = train_data.iter.cat\n",
    "    node2id = H.node2id\n",
    "    id2node = H.id2node\n",
    "    \n",
    "    odd_even = OrderedDict()\n",
    "    \n",
    "    root = []\n",
    "    for nl in non_leaf_:\n",
    "        if nl not in H.child2parent_table:\n",
    "            root.append(nl) \n",
    "    non_leaf = set(non_leaf_) - set(root)\n",
    "    \n",
    "    \n",
    "    # internal nodes\n",
    "    for r in root:\n",
    "        r_ = node2id[r]\n",
    "        for n in non_leaf:\n",
    "            n_ = node2id[n]\n",
    "            oe = len(H.get_shortest_path(r_, n_)[0])\n",
    "            if oe == 0 :\n",
    "                pass\n",
    "            else:\n",
    "                if oe not in odd_even:\n",
    "                    odd_even[oe] = [n_]\n",
    "                else:\n",
    "                    odd_even[oe].append(n_)\n",
    "        \n",
    "        \n",
    "    logging.info(\"at even level..\")\n",
    "    \n",
    "    for level, nodes in odd_even.items():\n",
    "        \n",
    "        if level%2 == 0:\n",
    "            \n",
    "            for k in tqdm(nodes):\n",
    "                \n",
    "                rr = RRLoss(k)\n",
    "                kth_path = main_path.format(k)\n",
    "                _, kth_model, kth_opt, kth_loss = load_nth_model(kth_path)\n",
    "                \n",
    "                \n",
    "                Cn = H.parent2child_table[id2node[k]]\n",
    "                pi = H.child2parent_table[id2node[k]][0]\n",
    "\n",
    "                pi = node2id[pi]\n",
    "                pi_path = main_path.format(pi)\n",
    "                _, pi_model, _, _ = load_nth_model(pi_path)\n",
    "\n",
    "                C_n = 0\n",
    "                \n",
    "                for x in Cn: \n",
    "                    cn_path = main_path.format(node2id[x])\n",
    "                    _, cn_model, _, _ = load_nth_model(cn_path)\n",
    "                    C_n += cn_model.state_dict()['linear.weight'].data\n",
    "                \n",
    "                w_pi_n = pi_model.state_dict()['linear.weight'].data\n",
    "                w_c = C_n\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for name, param in kth_model.state_dict().items():\n",
    "                        param = rr.non_leaf_update(Cn, w_pi_n, w_c)\n",
    "                        kth_model.state_dict()[name] = param\n",
    "                        save_nth_models(k, kth_model, kth_opt, kth_loss, kth_path)\n",
    "    \n",
    "    \n",
    "    logging.info(\"at odd level..\")\n",
    "    \n",
    "    for level, nodes in odd_even.items():\n",
    "    \n",
    "        if level%2 == 1:\n",
    "            \n",
    "            for k in tqdm(nodes):\n",
    "                \n",
    "                rr = RRLoss(k)\n",
    "                kth_path = main_path.format(k)\n",
    "                _, kth_model, kth_opt, kth_loss = load_nth_model(kth_path)\n",
    "                \n",
    "                \n",
    "                Cn = H.parent2child_table[id2node[k]]\n",
    "                pi = H.child2parent_table[id2node[k]][0]\n",
    "\n",
    "                pi = node2id[pi]\n",
    "                pi_path = main_path.format(pi)\n",
    "                _, pi_model, _, _ = load_nth_model(pi_path)\n",
    "\n",
    "                C_n = 0\n",
    "                \n",
    "                for x in Cn: \n",
    "                    cn_path = main_path.format(node2id[x])\n",
    "                    _, cn_model, _, _ = load_nth_model(cn_path)\n",
    "                    C_n += cn_model.state_dict()['linear.weight'].data\n",
    "                \n",
    "                w_pi_n = pi_model.state_dict()['linear.weight'].data\n",
    "                w_c = C_n\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for name, param in kth_model.state_dict().items():\n",
    "                        param = rr.non_leaf_update(Cn, w_pi_n, w_c)\n",
    "                        kth_model.state_dict()[name] = param\n",
    "                        save_nth_models(k, kth_model, kth_opt, kth_loss, kth_path)        \n",
    "#     return odd_even"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_model(node_n):\n",
    "    '''\n",
    "    training performance is affected by large `n_tasks` size which basically increases the number of parameters to tune.\n",
    "    as parameter dimension increases, weight decay also needs to be increased. what is the relation between param dim \n",
    "    and weight decay?\n",
    "    '''\n",
    "\n",
    "    # Hyper Parameters \n",
    "    d_dim = n_components\n",
    "    num_classes = len(train_data.iter.MLbin.classes_) #batch_size\n",
    "\n",
    "    model = HRLR(node_n)\n",
    "    model = model.to(device)\n",
    "    \n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(params=model.parameters(), max_iter=20, history_size=20)\n",
    "    \n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_nth_models(n_node, model, opt, loss, path):    \n",
    "    \n",
    "    save_dict = {'n_node': n_node,\n",
    "               'model': model.state_dict(),\n",
    "               'optim': opt.state_dict(),\n",
    "               'n_loss': loss }\n",
    "        \n",
    "    torch.save(save_dict, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nth_model(path):\n",
    "    \n",
    "    checkpoint = torch.load(path)\n",
    "    n_node = checkpoint['n_node']\n",
    "    loss = checkpoint['n_loss']\n",
    "    \n",
    "    model, optimizer = reset_model(n_node)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    optimizer.load_state_dict(checkpoint['optim'])\n",
    "    \n",
    "    return n_node, model, optimizer, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_step = len(train_loader)\n",
    "total_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaves = train_data.iter.cat.T_leaves\n",
    "internal = train_data.iter.cat.pi_parents\n",
    "node2id = train_data.iter.cat.node2id\n",
    "id2node = train_data.iter.cat.id2node\n",
    "classes = list(node2id.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_node_models():\n",
    "    \n",
    "    g = {}\n",
    "    nth_model = {}\n",
    "    nth_optim = {}\n",
    "    nth_loss = {}\n",
    "    \n",
    "    for c_ids in tqdm(classes):\n",
    "        \n",
    "        torch.cuda.empty_cache()        \n",
    "        \n",
    "        nth_loss[c_ids] = []\n",
    "#         nth_model[c_ids], nth_optim[c_ids] = reset_model(c_ids)\n",
    "        \n",
    "        pre_ = \"../persist_models_lshtc/\"\n",
    "\n",
    "        if not os.path.isdir(pre_):\n",
    "            os.mkdir(pre_)\n",
    "\n",
    "        path = \"{}_all_classes_{}.pkl\".format(pre_, c_ids)\n",
    "        \n",
    "        if not os.path.isfile(path):\n",
    "            model, optimizer = reset_model(c_ids)\n",
    "            save_nth_models(c_ids, model, optimizer, nth_loss[c_ids], path)\n",
    "\n",
    "\n",
    "create_node_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(internal) + len(leaves) == len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def model_train(data, main_path):\n",
    "    \n",
    "    pat = []\n",
    "    per_node_loss = {}\n",
    "    cv = [74, 77, 79]\n",
    "    \n",
    "    doc_vec_all = torch.as_tensor(train_data.iter.all_sub_x, device=device, dtype=torch.float32)\n",
    "    \n",
    "    for orig in tqdm(leaves):\n",
    "        \n",
    "#         orig = id2node[orig]\n",
    "        n_id = node2id[orig]\n",
    "        pi_n =  node2id[train_data.iter.cat.child2parent_table[orig][0]]\n",
    "        pat.append(n_id)\n",
    "        \n",
    "        \n",
    "        leaf_path = main_path.format(n_id)\n",
    "        pi_path = main_path.format(pi_n)\n",
    "        \n",
    "        skipper = os.path.getsize(leaf_path)\n",
    "        \n",
    "        if skipper > 843537:\n",
    "            logging.info(\"Done: Class: {}, node_id: {}\".format(orig, n_id))\n",
    "            _, current_leaf_model, current_optimizer, per_node_loss[n_id] = load_nth_model(leaf_path)\n",
    "            pass\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            logging.info(\"Leaf: Class: {}, node_id: {}\".format(orig, n_id))\n",
    "\n",
    "            _, current_leaf_model, current_optimizer, per_node_loss[n_id] = load_nth_model(leaf_path)\n",
    "            _, parent_model, _, _ = load_nth_model(pi_path)\n",
    "\n",
    "            torch.cuda.empty_cache()        \n",
    "\n",
    "            for j ,(docid, doc_vec, doc_labels) in enumerate(data):\n",
    "#                 yin = torch.zeros((1,len(classes)), dtype= torch.float32, device=device)\n",
    "#                 yin[:, node2id[int(doc_labels)]] = 1\n",
    "                \n",
    "                if orig == doc_labels:\n",
    "                    y_in = 1\n",
    "                else:\n",
    "                    y_in = -1\n",
    "                \n",
    "                C = 1e-3\n",
    "\n",
    "                def closure():\n",
    "                    current_optimizer.zero_grad()\n",
    "\n",
    "                    w_n = current_leaf_model.state_dict()['linear.weight'].data\n",
    "                    pi__n = parent_model.state_dict()['linear.weight'].data\n",
    "\n",
    "                    rr = RRLoss(n_id)\n",
    "                    L2 = rr.forward(w_n, pi__n)\n",
    "\n",
    "                    output = current_leaf_model.forward(doc_vec)\n",
    "\n",
    "                    loss = current_leaf_model.compute_loss_leaf(y_in, output)\n",
    "                    loss = C * loss\n",
    "                    loss.add_(L2)\n",
    "                    loss.backward()\n",
    "                    per_node_loss[n_id].append(loss)\n",
    "                    return loss\n",
    "                current_optimizer.step(closure)\n",
    "\n",
    "            save_nth_models(n_id, current_leaf_model, current_optimizer, per_node_loss[n_id], leaf_path)\n",
    "            \n",
    "    non_leaf_update(internal, main_path)\n",
    "    \n",
    "    return per_node_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_path = \"../persist_models_lshtc/_all_classes_{}.pkl\"\n",
    "nth_loss = model_train(train_loader, main_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(nrows=5, ncols=5, figsize=(15,15))\n",
    "\n",
    "for a, (key, value) in zip(ax.flatten(), nth_loss.items()):\n",
    "    a.plot(value)\n",
    "#     a.yaxis.set_ticklabels([])\n",
    "    a.set_title(\"n_id: {}\".format(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    m_W = torch.zeros((len(classes), n_components))\n",
    "    for n_node in tqdm(classes):\n",
    "\n",
    "#         n_node = node2id[orig]\n",
    "        node_path = main_path.format(n_node)\n",
    "\n",
    "        _, current_node_model, _, _ = load_nth_model(node_path)\n",
    "        w_n = current_node_model.state_dict()['linear.weight'].data\n",
    "        m_W[n_node, :] = w_n.squeeze()\n",
    "        torch.cuda.empty_cache()        \n",
    "m_W = m_W.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_th = []\n",
    "for it, (docid, doc_vec, doclabels) in enumerate(tqdm(train_loader)):\n",
    "        nid = node2id[int(doclabels)]\n",
    "        m_W = m_W.to(device)\n",
    "        score = (doc_vec.mm(m_W))\n",
    "        score = (score.cpu().numpy())\n",
    "        theta_th.append(score[0, nid])\n",
    "        \n",
    "for it, (docid, doc_vec, doclabels) in enumerate(tqdm(validation_loader)):\n",
    "        nid = node2id[int(doclabels)]\n",
    "        m_W = m_W.to(device)\n",
    "        score = (doc_vec.mm(m_W))\n",
    "        score = (score.cpu().numpy())\n",
    "        theta_th.append(score[0, nid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(theta_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def y_validate(data, m_W, theta_th):\n",
    "    \n",
    "    # for each document representation\n",
    "    # for each node classifier N\n",
    "    # calucate score per classifier for document:\n",
    "    # score[n] = model[n].weight.t().mm(doc_vec) -> 1x1 vector\n",
    "    # label will be: max value at score[n] -> n \n",
    "    \n",
    "    y_true_num = np.zeros((len(data), len(classes)))\n",
    "    y_pred = np.zeros((len(data), len(classes)))\n",
    "    \n",
    "    for it, (docid, doc_vec, doclabels) in enumerate(tqdm(data)):\n",
    "        \n",
    "        nid = node2id[int(doclabels)]\n",
    "        y_true_num[it, nid] = 1\n",
    "        \n",
    "        m_W = m_W.to(device)\n",
    "        \n",
    "        score = F.normalize(doc_vec.mm(m_W))\n",
    "        score = (score.cpu().numpy().squeeze().tolist())\n",
    "        print(nid, score[nid])\n",
    "#         for j, i in enumerate(score):\n",
    "#             if i < 0.03 :\n",
    "#                 y_pred[it, j] = 1\n",
    "            \n",
    "\n",
    "    return y_pred, y_true_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yp_, yt_ = y_validate(validation_loader, m_W, theta_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yp_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(yt_, yp_, average=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_test_predict(data, nth_model):\n",
    "    \n",
    "    # predictions for test set\n",
    "    model_W = []\n",
    "    \n",
    "    for n_node, n_model in tqdm(nth_model.items()):\n",
    "        model_W.append(n_model.state_dict()['linear.weight'].data)\n",
    "    \n",
    "    model_W = torch.stack(model_W, 0).squeeze()\n",
    "    \n",
    "    \n",
    "    yp = []\n",
    "    for _, doc_vec in tqdm(data):\n",
    "        \n",
    "        doc_vec = torch.as_tensor(doc_vec, dtype=torch.float32, device=device)\n",
    "        \n",
    "        score = doc_vec.mm(model_W.t())\n",
    "        temp = score.cpu().numpy()\n",
    "        batch_ = np.array(temp > 500).astype(int)\n",
    "\n",
    "        yp.append(batch_)\n",
    "    \n",
    "    y_p = np.vstack(yp)\n",
    "    \n",
    "    del yp\n",
    "    \n",
    "    return y_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_test_p = y_test_predict(test_loader, nth_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_test_true_dict = {}\n",
    "lim = y_test_p.shape[0]\n",
    "\n",
    "with open(\"../DMOZ/DMOZGS\", \"r\") as test_true:\n",
    "    ans = test_true.readlines()\n",
    "    \n",
    "for i, line in enumerate(ans[:lim]):\n",
    "    each_line = line.strip().split(' ')\n",
    "    int_el = list(map(int ,each_line))\n",
    "    y_test_true_dict[i] = int_el\n",
    "    \n",
    "y_test_true_np = np.zeros_like(y_test_p)\n",
    "\n",
    "for i, true_node in tqdm(y_test_true_dict.items()):\n",
    "    for t in true_node:\n",
    "        class_id = node2id[t]\n",
    "        y_test_true_np[i, class_id] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_true_np.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_p.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_true_np.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_true_np, y_test_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
