{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:summarizer.preprocessing.cleaner:'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from scripts.src.hierarchy import *\n",
    "from scripts.src.processing import *\n",
    "from scripts.src.label_utils import *\n",
    "from scripts.src.data_reading import *\n",
    "\n",
    "logging.basicConfig(level=logging.INFO )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "n = 32 # wn vector size  --> ~log_{2}(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if (torch.cuda.is_available() and num_gpus > 0) else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetIterator:\n",
    "    def __init__(self, datafile, catfile, subsample):\n",
    "        self.datafile = datafile\n",
    "        self.cat = HierarchyUtils(catfile, False, False)\n",
    "        self.wn = self.cat.generate_vectors(device = device, neighbours = True)\n",
    "        self.lib_data = LIBSVM_Reader(self.datafile, True, n_components, subsample)\n",
    "        self.df = self.lib_data.data_df\n",
    "    \n",
    "    def read_df(self, idx):\n",
    "        i = self.df.index[self.df[\"doc_id\"] == idx][0]\n",
    "        return self.df.at[i, \"doc_vector\"], self.df.at[i, \"doc_labels\"], i\n",
    "\n",
    "    def __getitem__(self, _id):\n",
    "        return self.read_df(_id)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for _id in self.df[\"doc_id\"]:\n",
    "            yield self[_id]\n",
    "\n",
    "class DatasetModule(Dataset):\n",
    "\n",
    "    def __init__(self, root_location, cat_file, subsample):\n",
    "        self.iter = DatasetIterator(root_location, cat_file, subsample)\n",
    "        self.lmbda = self.lambda_param()\n",
    "        \n",
    "    def lambda_param(self):\n",
    "        w_n = list2tensor(self.iter.wn[0].values())\n",
    "        w_pi = list2tensor(self.iter.wn[1].values())\n",
    "\n",
    "        norm2 = torch.norm(w_n-w_pi, 2)\n",
    "        lmbda = 0.5*norm2**2\n",
    "        return lmbda\n",
    "\n",
    "    def encode_labels(self, labels):\n",
    "        yin = torch.ones(len(self.iter.cat.node2id), dtype=torch.float32)*-1\n",
    "        y_in = []\n",
    "        label_vector = []\n",
    "        for l in labels:\n",
    "            yin[self.iter.cat.node2id[l]] = 1.\n",
    "            if l in self.iter.cat.T_leaves:\n",
    "                indicator = torch.Tensor([1])\n",
    "            else:\n",
    "                indicator = torch.Tensor([-1])\n",
    "            y_in.append(indicator)\n",
    "            label_vector.append(self.iter.wn[0][l])\n",
    "#         label_vector = label_vector/len(labels)\n",
    "        return label_vector, y_in\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.iter.df[\"doc_id\"])\n",
    "\n",
    "    def __load(self, idx):\n",
    "        doc_vec, doc_labels, _id = self.iter[idx]\n",
    "        return doc_vec, doc_labels, _id, self.encode_labels(doc_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.__load(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35it [00:00, 69805.34it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 332.43it/s]\n",
      "1157it [00:00, 27585.95it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 1157/1157 [00:00<00:00, 32954.92it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = DatasetModule(\"rcv1.tar/RCV1_1/rcv1.train.ltc.svm\", \"rcv1.tar/RCV1_1/rcv1.topic.hierarchy\", 1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "test_data = DatasetModule(\"rcv1.tar/RCV1_1/rcv1.test.ltc.svm\", \"rcv1.tar/RCV1_1/rcv1.topic.hierarchy\", True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_data = DatasetModule(\"swiki/data/train_remapped.txt\", \"swiki/data/cat_hier.txt\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_data = DatasetModule(\"DMOZ/train.txt\", \"DMOZ/cat_hier.txt\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, shuffle = True, pin_memory=True, batch_size=batch_size)\n",
    "# valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(train_loader)\n",
    "doc_vec, doc_labels, _id, wn =  train_iter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 300])\n",
      "**************************************************\n",
      "torch.Size([1, 117, 1])\n",
      "--------------------------------------------------\n",
      "tensor([[-1.]])\n",
      "__________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(doc_vec.shape)\n",
    "print(\"*\"*50)\n",
    "print(wn[0][0].shape)\n",
    "print(\"-\"*50)\n",
    "print(wn[1][0])\n",
    "print(\"_\"*50)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_data.iter.df[\"doc_labels\"][16289]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.version.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def gather_outputs(data, model):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    logging.info(\"Evaluating ...\")\n",
    "    with torch.no_grad():\n",
    "        for index, (doc_vec, doc_labels, _id, label_vecs) in enumerate(data):\n",
    "            \n",
    "            seq = doc_vec\n",
    "            print(seq)\n",
    "            label_vec = label_vecs[0].squeeze()\n",
    "            yin = label_vecs[1]\n",
    "            output = model(seq, label_vec, yin)\n",
    "            \n",
    "            print(output)\n",
    "\n",
    "            threshold = torch.mean(output).item()\n",
    "#             threshold = 0.5\n",
    "\n",
    "            output[output >= threshold] = 1\n",
    "            output[output < threshold] = -1\n",
    "            \n",
    "            y_pred.append(output.cpu().view(-1).numpy())\n",
    "            y_true.append(yin.numpy())\n",
    "            \n",
    "            if (index+1) % 500 == 0:\n",
    "                print(output, yin)\n",
    "                test_f1 = f1_score(np.asarray(y_true), np.array(y_pred), average=\"micro\")\n",
    "                print(\"Index: {}/{} F1: {}\".format(index+1, len(data), test_f1))\n",
    "    \n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    test_f1 = f1_score(y_true, y_pred, average=\"micro\")\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(nn.Module):\n",
    "    def __init__(self, weight_dims):\n",
    "        super().__init__()\n",
    "        weights = torch.FloatTensor(*weight_dims)\n",
    "        weights = nn.init.xavier_uniform_(weights)\n",
    "        self.w = nn.Parameter(weights)    \n",
    "        \n",
    "    def forward(self, x_i):\n",
    "        return x_i.matmul(self.w)\n",
    "\n",
    "    \n",
    "class HRLR(nn.Module):\n",
    "    def __init__(self, input_dim, n_classes):\n",
    "        super().__init__()\n",
    "        self.linear = Node([input_dim, n_classes])\n",
    "        self.linear2 = Node([n_classes, 1])\n",
    "#         self.all_nodes = []\n",
    "#         self.leaf_nodes = []\n",
    "        \n",
    "        \n",
    "#         self.all_nodes.append(Node(input_dim, n_classes))\n",
    "        # loop through the graph structure and create the classifier structure\n",
    "        # store all of it all_nodes, but store references for all of the leaf nodes in the leaf nodes list\n",
    "    \n",
    "    def forward(self, x_i, w_n):\n",
    "        # for each node, compute forward, and do a -1 +1 threshold to get classes\n",
    "        x_i = self.linear(x_i)\n",
    "        self.fwd_pass = (x_i)*w_n\n",
    "        \n",
    "        return self.fwd_pass\n",
    "    \n",
    "    def compute_loss_leaf(self, yin):\n",
    "        loss = torch.log1p(torch.exp(-yin *self.fwd_pass))\n",
    "        return loss\n",
    "    \n",
    "    def compute_loss_non_leaf(self, yin):\n",
    "        loss = 1 - yin*self.fwd_pass\n",
    "        return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3556,  0.3884,  0.1371, -0.1389,  0.7216,  1.2512,  1.4120,  0.1044,\n",
       "         0.4370, -1.6375,  0.1126, -0.0317,  0.3667,  0.0707, -0.8993,  0.2248,\n",
       "         0.2193, -0.4157, -0.4864,  0.6615,  0.4523,  0.1966, -0.1696, -0.6285,\n",
       "        -0.2683, -0.9878, -0.5152, -0.0196,  0.7373,  0.8922,  1.0400,  0.2518,\n",
       "         0.1831,  1.1204,  0.4340,  0.6054, -0.3266, -0.0249,  0.8212, -0.3411,\n",
       "         0.2321, -0.8666,  0.5052, -0.1058, -0.0667, -0.2336,  0.3096,  0.4164,\n",
       "        -0.7387,  0.9758,  1.2439, -0.2728,  0.7941, -0.3048, -1.8721,  0.5797,\n",
       "         0.4761,  0.4195, -0.8803,  0.4809,  0.3571,  0.2510, -2.0544,  0.0513,\n",
       "        -0.3072, -1.2391,  0.7394,  0.0330,  0.4287, -0.2258,  0.5285, -0.7058,\n",
       "         0.0596,  0.5355, -0.6405,  0.4584, -0.2550,  0.1812,  0.5672, -0.5959,\n",
       "         0.4224, -0.2579,  0.1563, -0.2494, -0.6159,  0.0459, -0.0295, -0.4158,\n",
       "         1.4801,  0.5479,  0.3834, -0.4931, -0.4449,  0.6343,  1.0761, -0.8354,\n",
       "         0.2257, -0.1109, -0.0970, -0.1259,  0.1188, -0.6512, -0.1546,  0.8117,\n",
       "         1.4662, -0.4327, -0.0106,  0.4423, -0.0050, -0.6880,  0.1067,  0.1440,\n",
       "         0.7529,  0.6061, -0.3519, -0.1994, -0.9465],\n",
       "       grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_i = torch.rand(300,) #doc vec\n",
    "w_n = torch.FloatTensor(3,)\n",
    "yin = torch.Tensor([-1, 1, -1])\n",
    "\n",
    "linear = Node([300, 117])\n",
    "lin = Node([117, 1])\n",
    "linear(x_i) #pass doc vec to node nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 117, 117])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-linear(doc_vec)*(label_vec[0])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.6941, 0.6952, 0.6911,  ..., 0.6967, 0.6949, 0.6920],\n",
      "         [0.6929, 0.6926, 0.6936,  ..., 0.6923, 0.6927, 0.6934],\n",
      "         [0.6932, 0.6932, 0.6931,  ..., 0.6932, 0.6932, 0.6931],\n",
      "         ...,\n",
      "         [0.6923, 0.6913, 0.6949,  ..., 0.6900, 0.6916, 0.6942],\n",
      "         [0.6925, 0.6917, 0.6945,  ..., 0.6907, 0.6919, 0.6940],\n",
      "         [0.6940, 0.6949, 0.6915,  ..., 0.6961, 0.6946, 0.6922]]],\n",
      "       grad_fn=<Log1PBackward>)\n"
     ]
    }
   ],
   "source": [
    "print((torch.log1p(torch.exp(-linear(doc_vec)*(label_vec[0])*wn[1][0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrr = HRLR(n_components, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.7286e-04, -5.8906e-04, -1.2482e-04, -3.4745e-03, -2.6662e-03,\n",
       "         -1.9736e-03, -6.3880e-04, -6.6700e-08, -9.8846e-04,  1.5042e-04,\n",
       "         -2.9052e-03,  1.7922e-03, -7.8385e-04,  5.2867e-04,  5.0290e-05,\n",
       "          6.9366e-05, -2.2596e-03, -5.3449e-03,  2.2916e-03,  2.6406e-04,\n",
       "          4.8162e-04, -3.4864e-04, -3.9629e-03, -2.0648e-03,  4.5326e-03,\n",
       "          6.8529e-03,  8.1558e-04,  1.3225e-04, -9.3767e-04, -4.2121e-04,\n",
       "         -3.0751e-04, -1.6425e-03,  2.7530e-04, -7.4484e-03,  2.6502e-04,\n",
       "          3.9637e-04,  1.5146e-03, -3.9889e-03, -5.0928e-03, -3.2761e-03,\n",
       "          1.9460e-03, -1.8938e-03,  1.6054e-03, -2.9268e-03,  4.4918e-03,\n",
       "         -2.9239e-03, -4.3428e-03,  9.2111e-04, -1.7196e-03,  3.2554e-03,\n",
       "          3.5910e-03, -4.0279e-03, -9.3613e-03, -4.6862e-04,  5.1331e-03,\n",
       "          2.7828e-03, -2.5167e-04, -1.2669e-03, -2.1076e-03,  1.3230e-03,\n",
       "         -4.0168e-03,  3.2876e-03,  7.1112e-03,  1.5776e-04, -7.9771e-04,\n",
       "         -2.8265e-03,  3.5170e-03,  4.4878e-03,  8.5408e-03,  2.2418e-04,\n",
       "         -6.8242e-03, -1.7886e-03, -1.4859e-03,  5.0252e-03, -3.7704e-04,\n",
       "          4.8190e-03,  1.1019e-02,  1.2768e-02, -1.1220e-03,  1.5182e-03,\n",
       "         -2.2502e-04,  1.5820e-03,  9.2983e-05, -3.0878e-03, -4.6179e-04,\n",
       "         -9.7469e-03, -3.2401e-03,  9.5971e-04,  7.1965e-03,  1.0692e-04,\n",
       "         -8.8357e-04, -3.3211e-03, -8.7853e-04,  2.3267e-03,  6.3192e-03,\n",
       "         -7.5962e-03,  2.3409e-03, -7.4118e-04,  4.0277e-03,  3.4119e-03,\n",
       "         -4.6020e-04, -1.7894e-03, -1.3348e-03,  9.5399e-03,  9.5652e-04,\n",
       "          3.7704e-05,  3.9571e-03, -6.4537e-03,  1.8576e-03,  9.1491e-04,\n",
       "          1.7963e-03, -5.7894e-04, -3.5577e-03,  2.5757e-03,  2.3983e-03,\n",
       "         -4.1108e-03, -1.9898e-03]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrr.forward(doc_vec, wn[0][0].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6930, 0.6929, 0.6931, 0.6914, 0.6918, 0.6922, 0.6928, 0.6931, 0.6927,\n",
       "         0.6932, 0.6917, 0.6940, 0.6928, 0.6934, 0.6932, 0.6932, 0.6920, 0.6905,\n",
       "         0.6943, 0.6933, 0.6934, 0.6930, 0.6912, 0.6921, 0.6954, 0.6966, 0.6936,\n",
       "         0.6932, 0.6927, 0.6929, 0.6930, 0.6923, 0.6933, 0.6894, 0.6933, 0.6933,\n",
       "         0.6939, 0.6912, 0.6906, 0.6915, 0.6941, 0.6922, 0.6940, 0.6917, 0.6954,\n",
       "         0.6917, 0.6910, 0.6936, 0.6923, 0.6948, 0.6949, 0.6911, 0.6885, 0.6929,\n",
       "         0.6957, 0.6945, 0.6930, 0.6925, 0.6921, 0.6938, 0.6911, 0.6948, 0.6967,\n",
       "         0.6932, 0.6927, 0.6917, 0.6949, 0.6954, 0.6974, 0.6933, 0.6897, 0.6923,\n",
       "         0.6924, 0.6957, 0.6930, 0.6956, 0.6987, 0.6996, 0.6926, 0.6939, 0.6930,\n",
       "         0.6939, 0.6932, 0.6916, 0.6929, 0.6883, 0.6915, 0.6936, 0.6968, 0.6932,\n",
       "         0.6927, 0.6915, 0.6927, 0.6943, 0.6963, 0.6894, 0.6943, 0.6928, 0.6952,\n",
       "         0.6949, 0.6929, 0.6923, 0.6925, 0.6979, 0.6936, 0.6932, 0.6951, 0.6899,\n",
       "         0.6941, 0.6936, 0.6940, 0.6929, 0.6914, 0.6944, 0.6943, 0.6911, 0.6922]],\n",
       "       grad_fn=<Log1PBackward>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrr.compute_loss_leaf(wn[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([117])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_i.matmul(linear.w.data).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300, 117])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear.w.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, num_classes, False)\n",
    "        self.exp = torch.exp\n",
    "        \n",
    "    def forward(self, x, labels, yin):\n",
    "        x1 = self.linear1(x)\n",
    "        y  = x1.mul(labels)\n",
    "        exp_ = self.exp(-y*yin)\n",
    "        return torch.log(1+exp_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters \n",
    "input_size = n_components #128 n_components\n",
    "# input_size = train_data.data[0].shape[1] #2085164 \n",
    "\n",
    "num_classes = len(train_data.iter.cat.N_all_nodes) #50312 --> n (16)\n",
    "num_epochs = 5 # TRAIN IT FOR A LOT OF EPOCHS in case of lbfgs (2nd order method) else less is more\n",
    "learning_rate = 0.0005 #1e-4, 0.0005\n",
    "\n",
    "model = HRLR(n_components, num_classes)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=train_data.lmbda)\n",
    "# optimizer = torch.optim.LBFGS(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2389)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.lmbda"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "logger = Logger('./logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_step = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1157"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = {\n",
    "    \"test_f1\": [],\n",
    "    \"loss\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1157/1157 [00:05<00:00, 201.59it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1157/1157 [00:05<00:00, 203.97it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1157/1157 [00:05<00:00, 199.49it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1157/1157 [00:05<00:00, 202.19it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1157/1157 [00:05<00:00, 201.59it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for doc_vec, doc_labels, _id, label_vecs in tqdm(train_loader):\n",
    "        label_vec = label_vecs[0]\n",
    "        yin = label_vecs[1]\n",
    "        for i, each_label_vec in enumerate(label_vec):\n",
    "            optimizer.zero_grad()\n",
    "            output = model.forward(doc_vec, each_label_vec.squeeze())\n",
    "            loss = model.compute_loss_leaf(yin[i]) \n",
    "            loss.mean().backward()\n",
    "            optimizer.step()\n",
    "        monitor[\"loss\"].append((loss.mean().item()))\n",
    "#         loss_per+=loss.item()\n",
    "#             else:\n",
    "#                 label_vec = label_vecs[0].squeeze()\n",
    "#                 yin = label_vecs[1].squeeze()\n",
    "                \n",
    "#                 model.zero_grad()\n",
    "                \n",
    "#                 output = model.forward(doc_vec, label_vec)\n",
    "\n",
    "#                 loss = model.compute_loss_non_leaf(yin) \n",
    "#                 loss.backward()\n",
    "\n",
    "#                 optimizer.step()\n",
    "                \n",
    "#                 loss_per += loss.item()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(monitor[\"loss\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6929802894592285,\n",
       " 0.6930485963821411,\n",
       " 0.693127453327179,\n",
       " 0.6931055188179016,\n",
       " 0.6929110884666443,\n",
       " 0.6930413246154785,\n",
       " 0.6930298805236816,\n",
       " 0.6932344436645508,\n",
       " 0.6931713223457336,\n",
       " 0.6930485367774963,\n",
       " 0.6931593418121338,\n",
       " 0.6931775808334351,\n",
       " 0.69309002161026,\n",
       " 0.6931564211845398,\n",
       " 0.6930827498435974,\n",
       " 0.6931803822517395,\n",
       " 0.6931025981903076,\n",
       " 0.6931264400482178,\n",
       " 0.6931087374687195,\n",
       " 0.6931004524230957,\n",
       " 0.6931522488594055,\n",
       " 0.6931512355804443,\n",
       " 0.6931491494178772,\n",
       " 0.6931407451629639,\n",
       " 0.6931425929069519,\n",
       " 0.6931487321853638,\n",
       " 0.6931255459785461,\n",
       " 0.6931558847427368,\n",
       " 0.6931547522544861,\n",
       " 0.6931408643722534,\n",
       " 0.6931384205818176,\n",
       " 0.6931542754173279,\n",
       " 0.6931434273719788,\n",
       " 0.6931484937667847,\n",
       " 0.6931480169296265,\n",
       " 0.6931470632553101,\n",
       " 0.6931478977203369,\n",
       " 0.6931471824645996,\n",
       " 0.6931461691856384,\n",
       " 0.693148136138916,\n",
       " 0.6931460499763489,\n",
       " 0.6931480765342712,\n",
       " 0.6931378841400146,\n",
       " 0.6931470036506653,\n",
       " 0.6931563019752502,\n",
       " 0.6931450963020325,\n",
       " 0.6931445598602295,\n",
       " 0.6931431293487549,\n",
       " 0.693148672580719,\n",
       " 0.693142831325531,\n",
       " 0.6931486129760742,\n",
       " 0.6931472420692444,\n",
       " 0.6931466460227966,\n",
       " 0.6931480765342712,\n",
       " 0.6931450963020325,\n",
       " 0.6931410431861877,\n",
       " 0.6931469440460205,\n",
       " 0.6931487917900085,\n",
       " 0.6931424140930176,\n",
       " 0.6931492686271667,\n",
       " 0.6931402087211609,\n",
       " 0.6931436657905579,\n",
       " 0.6931461691856384,\n",
       " 0.6931427121162415,\n",
       " 0.6931492686271667,\n",
       " 0.6931484937667847,\n",
       " 0.6931489109992981,\n",
       " 0.6931517720222473,\n",
       " 0.6931459903717041,\n",
       " 0.6931483149528503,\n",
       " 0.6931476593017578,\n",
       " 0.6931520700454712,\n",
       " 0.6931512951850891,\n",
       " 0.6931549906730652,\n",
       " 0.6931511163711548,\n",
       " 0.6931464672088623,\n",
       " 0.6931444406509399,\n",
       " 0.6931464672088623,\n",
       " 0.6931291222572327,\n",
       " 0.6931389570236206,\n",
       " 0.6931498050689697,\n",
       " 0.6931476593017578,\n",
       " 0.6931377053260803,\n",
       " 0.6931519508361816,\n",
       " 0.6931460499763489,\n",
       " 0.6931458711624146,\n",
       " 0.6931464076042175,\n",
       " 0.6931459307670593,\n",
       " 0.6931458711624146,\n",
       " 0.6931483745574951,\n",
       " 0.6931508183479309,\n",
       " 0.6931459903717041,\n",
       " 0.6931434273719788,\n",
       " 0.6931594610214233,\n",
       " 0.6931493282318115,\n",
       " 0.6931442618370056,\n",
       " 0.693148672580719,\n",
       " 0.693143367767334,\n",
       " 0.6931529641151428,\n",
       " 0.6931469440460205,\n",
       " 0.6931522488594055,\n",
       " 0.6931413412094116,\n",
       " 0.693149983882904,\n",
       " 0.6931484341621399,\n",
       " 0.6931434273719788,\n",
       " 0.6931332349777222,\n",
       " 0.6931405067443848,\n",
       " 0.6931349635124207,\n",
       " 0.6931418776512146,\n",
       " 0.6931452751159668,\n",
       " 0.6931511163711548,\n",
       " 0.6931397318840027,\n",
       " 0.6931492686271667,\n",
       " 0.6931483745574951,\n",
       " 0.6931476593017578,\n",
       " 0.6931508183479309,\n",
       " 0.6931320428848267,\n",
       " 0.6931483149528503,\n",
       " 0.6931493282318115,\n",
       " 0.693153977394104,\n",
       " 0.6931455731391907,\n",
       " 0.6931455731391907,\n",
       " 0.6931458115577698,\n",
       " 0.6931458115577698,\n",
       " 0.6931470632553101,\n",
       " 0.6931489706039429,\n",
       " 0.6931401491165161,\n",
       " 0.6931496262550354,\n",
       " 0.6931499242782593,\n",
       " 0.6931495070457458,\n",
       " 0.693149209022522,\n",
       " 0.6931520104408264,\n",
       " 0.6931424736976624,\n",
       " 0.6931439638137817,\n",
       " 0.6931443810462952,\n",
       " 0.6931446194648743,\n",
       " 0.6931504011154175,\n",
       " 0.6931489109992981,\n",
       " 0.6931588053703308,\n",
       " 0.6931425333023071,\n",
       " 0.6931557059288025,\n",
       " 0.6931496262550354,\n",
       " 0.6931441426277161,\n",
       " 0.6931291818618774,\n",
       " 0.6931503415107727,\n",
       " 0.6931498050689697,\n",
       " 0.6931541562080383,\n",
       " 0.693145751953125,\n",
       " 0.6931434869766235,\n",
       " 0.6931414604187012,\n",
       " 0.6931478381156921,\n",
       " 0.6931437849998474,\n",
       " 0.6931427121162415,\n",
       " 0.6931442022323608,\n",
       " 0.6931455135345459,\n",
       " 0.6931470632553101,\n",
       " 0.6931374669075012,\n",
       " 0.6931496262550354,\n",
       " 0.6931527256965637,\n",
       " 0.693146288394928,\n",
       " 0.6931529641151428,\n",
       " 0.6931455135345459,\n",
       " 0.6931454539299011,\n",
       " 0.6931456327438354,\n",
       " 0.6931298971176147,\n",
       " 0.6931480765342712,\n",
       " 0.6931253671646118,\n",
       " 0.6931313872337341,\n",
       " 0.6931328773498535,\n",
       " 0.6931422352790833,\n",
       " 0.6931333541870117,\n",
       " 0.6931416988372803,\n",
       " 0.6931391954421997,\n",
       " 0.6931508779525757,\n",
       " 0.6931489706039429,\n",
       " 0.6931408643722534,\n",
       " 0.6931402683258057,\n",
       " 0.693136990070343,\n",
       " 0.6931496858596802,\n",
       " 0.6931296586990356,\n",
       " 0.6931485533714294,\n",
       " 0.6931447982788086,\n",
       " 0.6931468844413757,\n",
       " 0.6931542754173279,\n",
       " 0.6931431889533997,\n",
       " 0.6931480765342712,\n",
       " 0.6931331753730774,\n",
       " 0.6931460499763489,\n",
       " 0.6931450366973877,\n",
       " 0.6931395530700684,\n",
       " 0.6931487321853638,\n",
       " 0.6931532025337219,\n",
       " 0.6931474208831787,\n",
       " 0.693145215511322,\n",
       " 0.6931362748146057,\n",
       " 0.6931309700012207,\n",
       " 0.6931514143943787,\n",
       " 0.6931458711624146,\n",
       " 0.6931544542312622,\n",
       " 0.6931268572807312,\n",
       " 0.6931465268135071,\n",
       " 0.6931493282318115,\n",
       " 0.6931556463241577,\n",
       " 0.6931462287902832,\n",
       " 0.6931584477424622,\n",
       " 0.6931436657905579,\n",
       " 0.6931530833244324,\n",
       " 0.6931446194648743,\n",
       " 0.6931449174880981,\n",
       " 0.6931388974189758,\n",
       " 0.6931300759315491,\n",
       " 0.6931521892547607,\n",
       " 0.6931504607200623,\n",
       " 0.6931431293487549,\n",
       " 0.6931416988372803,\n",
       " 0.6931504607200623,\n",
       " 0.6931523084640503,\n",
       " 0.693146288394928,\n",
       " 0.6931517720222473,\n",
       " 0.6931489109992981,\n",
       " 0.6931495070457458,\n",
       " 0.6931546330451965,\n",
       " 0.693133533000946,\n",
       " 0.6931489109992981,\n",
       " 0.693145215511322,\n",
       " 0.6931408643722534,\n",
       " 0.6931249499320984,\n",
       " 0.6931207776069641,\n",
       " 0.6931431293487549,\n",
       " 0.6931484937667847,\n",
       " 0.6931429505348206,\n",
       " 0.6931233406066895,\n",
       " 0.693148672580719,\n",
       " 0.6931442618370056,\n",
       " 0.6931486129760742,\n",
       " 0.6931411027908325,\n",
       " 0.6931511163711548,\n",
       " 0.6931557059288025,\n",
       " 0.6931494474411011,\n",
       " 0.6931586861610413,\n",
       " 0.6931456327438354,\n",
       " 0.6931450963020325,\n",
       " 0.6931430697441101,\n",
       " 0.6931602954864502,\n",
       " 0.6931365728378296,\n",
       " 0.6931354403495789,\n",
       " 0.6931403279304504,\n",
       " 0.6931269764900208,\n",
       " 0.6931416392326355,\n",
       " 0.6931526064872742,\n",
       " 0.6931344866752625,\n",
       " 0.6931623220443726,\n",
       " 0.6931406259536743,\n",
       " 0.6931431293487549,\n",
       " 0.6931496262550354,\n",
       " 0.6931341886520386,\n",
       " 0.6931462287902832,\n",
       " 0.693145215511322,\n",
       " 0.6931513547897339,\n",
       " 0.6931375861167908,\n",
       " 0.6931458711624146,\n",
       " 0.6931582689285278,\n",
       " 0.6931511163711548,\n",
       " 0.6931465268135071,\n",
       " 0.6931321620941162,\n",
       " 0.6931421160697937,\n",
       " 0.6931475400924683,\n",
       " 0.6931456327438354,\n",
       " 0.6931414008140564,\n",
       " 0.6931602954864502,\n",
       " 0.6931439638137817,\n",
       " 0.6931565999984741,\n",
       " 0.6931418776512146,\n",
       " 0.6931506395339966,\n",
       " 0.6931456923484802,\n",
       " 0.6931557059288025,\n",
       " 0.693149209022522,\n",
       " 0.6931395530700684,\n",
       " 0.6931423544883728,\n",
       " 0.6931286454200745,\n",
       " 0.6931456327438354,\n",
       " 0.6931385397911072,\n",
       " 0.6931282877922058,\n",
       " 0.6931517720222473,\n",
       " 0.6931394934654236,\n",
       " 0.6931459903717041,\n",
       " 0.6931512355804443,\n",
       " 0.6931441426277161,\n",
       " 0.6931319236755371,\n",
       " 0.6931238174438477,\n",
       " 0.6931451559066772,\n",
       " 0.6931449174880981,\n",
       " 0.6931491494178772,\n",
       " 0.6931493282318115,\n",
       " 0.6931432485580444,\n",
       " 0.693157970905304,\n",
       " 0.6931570768356323,\n",
       " 0.6931451559066772,\n",
       " 0.6931406259536743,\n",
       " 0.6931460499763489,\n",
       " 0.6931505799293518,\n",
       " 0.6931560039520264,\n",
       " 0.6931431889533997,\n",
       " 0.6931530833244324,\n",
       " 0.693142831325531,\n",
       " 0.6931424736976624,\n",
       " 0.6931415796279907,\n",
       " 0.6931451559066772,\n",
       " 0.6931423544883728,\n",
       " 0.6931533813476562,\n",
       " 0.6931567788124084,\n",
       " 0.6931437849998474,\n",
       " 0.6931447982788086,\n",
       " 0.6931343674659729,\n",
       " 0.6931418776512146,\n",
       " 0.6931394934654236,\n",
       " 0.6931531429290771,\n",
       " 0.6931519508361816,\n",
       " 0.6931445598602295,\n",
       " 0.6931416988372803,\n",
       " 0.6931448578834534,\n",
       " 0.6931418180465698,\n",
       " 0.6931493282318115,\n",
       " 0.6931553483009338,\n",
       " 0.6931471824645996,\n",
       " 0.6931439638137817,\n",
       " 0.6931437253952026,\n",
       " 0.693149745464325,\n",
       " 0.6931499242782593,\n",
       " 0.6931469440460205,\n",
       " 0.6931450366973877,\n",
       " 0.6931347250938416,\n",
       " 0.6931544542312622,\n",
       " 0.6931477189064026,\n",
       " 0.6931463479995728,\n",
       " 0.6931503415107727,\n",
       " 0.6931532621383667,\n",
       " 0.6931854486465454,\n",
       " 0.6931446194648743,\n",
       " 0.6931388974189758,\n",
       " 0.6931443214416504,\n",
       " 0.6931391954421997,\n",
       " 0.6931526064872742,\n",
       " 0.6931367516517639,\n",
       " 0.6931454539299011,\n",
       " 0.693140983581543,\n",
       " 0.693143367767334,\n",
       " 0.6931431889533997,\n",
       " 0.6931453347206116,\n",
       " 0.6931532025337219,\n",
       " 0.6931445002555847,\n",
       " 0.693143367767334,\n",
       " 0.6931521892547607,\n",
       " 0.693134069442749,\n",
       " 0.6931479573249817,\n",
       " 0.6931531429290771,\n",
       " 0.6931450963020325,\n",
       " 0.6931276321411133,\n",
       " 0.6931434869766235,\n",
       " 0.6931393146514893,\n",
       " 0.6931643486022949,\n",
       " 0.6931506991386414,\n",
       " 0.6931331157684326,\n",
       " 0.6931533217430115,\n",
       " 0.6931559443473816,\n",
       " 0.6931617856025696,\n",
       " 0.6931447982788086,\n",
       " 0.6931459307670593,\n",
       " 0.6931675672531128,\n",
       " 0.6931402683258057,\n",
       " 0.6931973099708557,\n",
       " 0.6931427121162415,\n",
       " 0.693144679069519,\n",
       " 0.6931521892547607,\n",
       " 0.6931456923484802,\n",
       " 0.6931212544441223,\n",
       " 0.6931292414665222,\n",
       " 0.693143904209137,\n",
       " 0.6931521892547607,\n",
       " 0.693142831325531,\n",
       " 0.693157434463501,\n",
       " 0.6931547522544861,\n",
       " 0.6931460499763489,\n",
       " 0.6931460499763489,\n",
       " 0.6931427121162415,\n",
       " 0.6931217908859253,\n",
       " 0.6931388974189758,\n",
       " 0.6931493282318115,\n",
       " 0.6931506991386414,\n",
       " 0.6931572556495667,\n",
       " 0.6931539177894592,\n",
       " 0.69314044713974,\n",
       " 0.6931542158126831,\n",
       " 0.6931465268135071,\n",
       " 0.6931627988815308,\n",
       " 0.6931511759757996,\n",
       " 0.6931461691856384,\n",
       " 0.6931512951850891,\n",
       " 0.6931580901145935,\n",
       " 0.6931524872779846,\n",
       " 0.6931499242782593,\n",
       " 0.6931473016738892,\n",
       " 0.693130373954773,\n",
       " 0.6931344866752625,\n",
       " 0.6931633353233337,\n",
       " 0.6931498050689697,\n",
       " 0.693140983581543,\n",
       " 0.6931416392326355,\n",
       " 0.6931508183479309,\n",
       " 0.6931286454200745,\n",
       " 0.6931496858596802,\n",
       " 0.6931565999984741,\n",
       " 0.6931506395339966,\n",
       " 0.6931475400924683,\n",
       " 0.6931286454200745,\n",
       " 0.6931484341621399,\n",
       " 0.6931445002555847,\n",
       " 0.6931625008583069,\n",
       " 0.6931496858596802,\n",
       " 0.6931138038635254,\n",
       " 0.6931509971618652,\n",
       " 0.6931396126747131,\n",
       " 0.6931436061859131,\n",
       " 0.693143904209137,\n",
       " 0.6931503415107727,\n",
       " 0.6931469440460205,\n",
       " 0.6931462287902832,\n",
       " 0.693126916885376,\n",
       " 0.6931446194648743,\n",
       " 0.6931511759757996,\n",
       " 0.6931646466255188,\n",
       " 0.6931484937667847,\n",
       " 0.6931838393211365,\n",
       " 0.6931472420692444,\n",
       " 0.6931453347206116,\n",
       " 0.6931512355804443,\n",
       " 0.6931517720222473,\n",
       " 0.6931360363960266,\n",
       " 0.6931596994400024,\n",
       " 0.6931498050689697,\n",
       " 0.6931455135345459,\n",
       " 0.693139374256134,\n",
       " 0.6931390166282654,\n",
       " 0.6931511163711548,\n",
       " 0.6931582093238831,\n",
       " 0.6931484937667847,\n",
       " 0.6931448578834534,\n",
       " 0.6931330561637878,\n",
       " 0.6931402087211609,\n",
       " 0.6931501030921936,\n",
       " 0.6931520700454712,\n",
       " 0.6931557655334473,\n",
       " 0.693141520023346,\n",
       " 0.6931380033493042,\n",
       " 0.6931498646736145,\n",
       " 0.6931554079055786,\n",
       " 0.6931614875793457,\n",
       " 0.6931456923484802,\n",
       " 0.6931419968605042,\n",
       " 0.6931434869766235,\n",
       " 0.6931427121162415,\n",
       " 0.6931298971176147,\n",
       " 0.6931470036506653,\n",
       " 0.6931602954864502,\n",
       " 0.6931418180465698,\n",
       " 0.6931596398353577,\n",
       " 0.6931349039077759,\n",
       " 0.693148672580719,\n",
       " 0.6931524276733398,\n",
       " 0.6931428909301758,\n",
       " 0.6931443810462952,\n",
       " 0.6931394338607788,\n",
       " 0.6931570172309875,\n",
       " 0.6931430697441101,\n",
       " 0.6931269764900208,\n",
       " 0.6931388974189758,\n",
       " 0.6931307911872864,\n",
       " 0.6931436061859131,\n",
       " 0.6931573152542114,\n",
       " 0.693147599697113,\n",
       " 0.6931546926498413,\n",
       " 0.6931407451629639,\n",
       " 0.6931421160697937,\n",
       " 0.6931482553482056,\n",
       " 0.6931213140487671,\n",
       " 0.6931549906730652,\n",
       " 0.693148672580719,\n",
       " 0.693143904209137,\n",
       " 0.6931376457214355,\n",
       " 0.6931517124176025,\n",
       " 0.6931560635566711,\n",
       " 0.6931352615356445,\n",
       " 0.6931411027908325,\n",
       " 0.6931531429290771,\n",
       " 0.6931440830230713,\n",
       " 0.6931600570678711,\n",
       " 0.6931441426277161,\n",
       " 0.6931505799293518,\n",
       " 0.6931452751159668,\n",
       " 0.6931513547897339,\n",
       " 0.6931596398353577,\n",
       " 0.693152904510498,\n",
       " 0.6931427121162415,\n",
       " 0.6931447982788086,\n",
       " 0.693142831325531,\n",
       " 0.6931656002998352,\n",
       " 0.6931465268135071,\n",
       " 0.6931444406509399,\n",
       " 0.6931390166282654,\n",
       " 0.6931513547897339,\n",
       " 0.693153977394104,\n",
       " 0.6931403279304504,\n",
       " 0.6931508779525757,\n",
       " 0.6931428909301758,\n",
       " 0.6931610703468323,\n",
       " 0.6931537985801697,\n",
       " 0.6931459307670593,\n",
       " 0.6931416988372803,\n",
       " 0.6931440830230713,\n",
       " 0.6931428909301758,\n",
       " 0.6931406259536743,\n",
       " 0.693149983882904,\n",
       " 0.6931544542312622,\n",
       " 0.6931517124176025,\n",
       " 0.6931542754173279,\n",
       " 0.6931453347206116,\n",
       " 0.6931415796279907,\n",
       " 0.6931450366973877,\n",
       " 0.693153440952301,\n",
       " 0.6931508779525757,\n",
       " 0.6931177377700806,\n",
       " 0.6931443214416504,\n",
       " 0.6931586265563965,\n",
       " 0.693145751953125,\n",
       " 0.6931522488594055,\n",
       " 0.6931474208831787,\n",
       " 0.6931719183921814,\n",
       " 0.6931518912315369,\n",
       " 0.6931562423706055,\n",
       " 0.6931256055831909,\n",
       " 0.6931533217430115,\n",
       " 0.6931546926498413,\n",
       " 0.6931541562080383,\n",
       " 0.6931391954421997,\n",
       " 0.693141520023346,\n",
       " 0.6931459307670593,\n",
       " 0.6931400299072266,\n",
       " 0.6931483745574951,\n",
       " 0.6931503415107727,\n",
       " 0.6931386590003967,\n",
       " 0.6931507587432861,\n",
       " 0.6931446194648743,\n",
       " 0.6931619644165039,\n",
       " 0.6931384205818176,\n",
       " 0.6931512355804443,\n",
       " 0.6931537985801697,\n",
       " 0.6931273341178894,\n",
       " 0.6931318640708923,\n",
       " 0.6931170225143433,\n",
       " 0.6931517720222473,\n",
       " 0.6931442022323608,\n",
       " 0.6931562423706055,\n",
       " 0.6931617259979248,\n",
       " 0.6931498646736145,\n",
       " 0.6931389570236206,\n",
       " 0.693152904510498,\n",
       " 0.6931441426277161,\n",
       " 0.6931521892547607,\n",
       " 0.6931424140930176,\n",
       " 0.6931623220443726,\n",
       " 0.6931478381156921,\n",
       " 0.693148136138916,\n",
       " 0.6931455731391907,\n",
       " 0.6931532025337219,\n",
       " 0.6931498646736145,\n",
       " 0.6931490898132324,\n",
       " 0.6931437849998474,\n",
       " 0.6931797862052917,\n",
       " 0.693140983581543,\n",
       " 0.6931278109550476,\n",
       " 0.6931524276733398,\n",
       " 0.6931375861167908,\n",
       " 0.6931442022323608,\n",
       " 0.6931423544883728,\n",
       " 0.6931242346763611,\n",
       " 0.693104088306427,\n",
       " 0.693139910697937,\n",
       " 0.6931577324867249,\n",
       " 0.6931520104408264,\n",
       " 0.6931447982788086,\n",
       " 0.6931459903717041,\n",
       " 0.693129301071167,\n",
       " 0.6931561827659607,\n",
       " 0.6931464672088623,\n",
       " 0.6931470632553101,\n",
       " 0.693144679069519,\n",
       " 0.6931294202804565,\n",
       " 0.6931379437446594,\n",
       " 0.6931276917457581,\n",
       " 0.6931552290916443,\n",
       " 0.6931484937667847,\n",
       " 0.6931490302085876,\n",
       " 0.6931428909301758,\n",
       " 0.6931360960006714,\n",
       " 0.6931467056274414,\n",
       " 0.6931518912315369,\n",
       " 0.6931560635566711,\n",
       " 0.6931275725364685,\n",
       " 0.6931569576263428,\n",
       " 0.6931201219558716,\n",
       " 0.6931466460227966,\n",
       " 0.6931536197662354,\n",
       " 0.6931149959564209,\n",
       " 0.6931455731391907,\n",
       " 0.6931518912315369,\n",
       " 0.69312983751297,\n",
       " 0.6931450366973877,\n",
       " 0.6931447982788086,\n",
       " 0.6931317448616028,\n",
       " 0.693153977394104,\n",
       " 0.693147599697113,\n",
       " 0.6931392550468445,\n",
       " 0.6931434273719788,\n",
       " 0.6931332349777222,\n",
       " 0.6931393146514893,\n",
       " 0.6931418180465698,\n",
       " 0.6931598782539368,\n",
       " 0.693142294883728,\n",
       " 0.6931394934654236,\n",
       " 0.6931395530700684,\n",
       " 0.6931511163711548,\n",
       " 0.6931412816047668,\n",
       " 0.6931491494178772,\n",
       " 0.693145215511322,\n",
       " 0.6931466460227966,\n",
       " 0.6931281685829163,\n",
       " 0.6931479573249817,\n",
       " 0.6931524872779846,\n",
       " 0.6931476593017578,\n",
       " 0.6931494474411011,\n",
       " 0.69312584400177,\n",
       " 0.6931498050689697,\n",
       " 0.6931625604629517,\n",
       " 0.6931504607200623,\n",
       " 0.6931514739990234,\n",
       " 0.6931470036506653,\n",
       " 0.6931399703025818,\n",
       " 0.6931535005569458,\n",
       " 0.6931511759757996,\n",
       " 0.6931374669075012,\n",
       " 0.6931068301200867,\n",
       " 0.693150520324707,\n",
       " 0.693165123462677,\n",
       " 0.6931538581848145,\n",
       " 0.6931467056274414,\n",
       " 0.6931402087211609,\n",
       " 0.6931537985801697,\n",
       " 0.6931729912757874,\n",
       " 0.693141520023346,\n",
       " 0.6931491494178772,\n",
       " 0.6931397914886475,\n",
       " 0.6931402683258057,\n",
       " 0.6931400895118713,\n",
       " 0.6931535601615906,\n",
       " 0.6931484341621399,\n",
       " 0.6931406259536743,\n",
       " 0.6931442022323608,\n",
       " 0.6931625008583069,\n",
       " 0.693148672580719,\n",
       " 0.6931470632553101,\n",
       " 0.6931183934211731,\n",
       " 0.6931394338607788,\n",
       " 0.6931389570236206,\n",
       " 0.6931507587432861,\n",
       " 0.6931825280189514,\n",
       " 0.6931211948394775,\n",
       " 0.6931458115577698,\n",
       " 0.693152666091919,\n",
       " 0.6931657195091248,\n",
       " 0.6931436061859131,\n",
       " 0.6931511163711548,\n",
       " 0.6931489706039429,\n",
       " 0.6931653618812561,\n",
       " 0.6931447982788086,\n",
       " 0.6931523084640503,\n",
       " 0.6931503415107727,\n",
       " 0.6931504011154175,\n",
       " 0.6931518316268921,\n",
       " 0.6931456923484802,\n",
       " 0.6931376457214355,\n",
       " 0.6931352019309998,\n",
       " 0.6931472420692444,\n",
       " 0.6931556463241577,\n",
       " 0.6931525468826294,\n",
       " 0.6931537985801697,\n",
       " 0.6931476593017578,\n",
       " 0.69315505027771,\n",
       " 0.6931418776512146,\n",
       " 0.6931466460227966,\n",
       " 0.6931465268135071,\n",
       " 0.6931381225585938,\n",
       " 0.69315505027771,\n",
       " 0.6931443214416504,\n",
       " 0.6931447982788086,\n",
       " 0.6931385397911072,\n",
       " 0.6931505799293518,\n",
       " 0.6931544542312622,\n",
       " 0.6931554675102234,\n",
       " 0.6931704878807068,\n",
       " 0.6931259036064148,\n",
       " 0.6931427717208862,\n",
       " 0.693160355091095,\n",
       " 0.6931383013725281,\n",
       " 0.6931381225585938,\n",
       " 0.693165123462677,\n",
       " 0.6931344866752625,\n",
       " 0.6931453347206116,\n",
       " 0.6931447982788086,\n",
       " 0.6931496262550354,\n",
       " 0.6931424736976624,\n",
       " 0.6931401491165161,\n",
       " 0.69314044713974,\n",
       " 0.6931485533714294,\n",
       " 0.6931394934654236,\n",
       " 0.6931585073471069,\n",
       " 0.6931536197662354,\n",
       " 0.6930931210517883,\n",
       " 0.693168580532074,\n",
       " 0.6931489109992981,\n",
       " 0.6931406259536743,\n",
       " 0.693115770816803,\n",
       " 0.6931735277175903,\n",
       " 0.6931406855583191,\n",
       " 0.6931422352790833,\n",
       " 0.6931373476982117,\n",
       " 0.6931450963020325,\n",
       " 0.6931554675102234,\n",
       " 0.6931414604187012,\n",
       " 0.6931447982788086,\n",
       " 0.6931406259536743,\n",
       " 0.69315105676651,\n",
       " 0.6931077837944031,\n",
       " 0.6931513547897339,\n",
       " 0.6931542754173279,\n",
       " 0.6931472420692444,\n",
       " 0.6931383609771729,\n",
       " 0.6931755542755127,\n",
       " 0.6931450366973877,\n",
       " 0.6931616067886353,\n",
       " 0.6931484341621399,\n",
       " 0.6931616067886353,\n",
       " 0.6931304335594177,\n",
       " 0.69315505027771,\n",
       " 0.6931375861167908,\n",
       " 0.6931502223014832,\n",
       " 0.6931558847427368,\n",
       " 0.6931164264678955,\n",
       " 0.6931414604187012,\n",
       " 0.693147599697113,\n",
       " 0.6931406259536743,\n",
       " 0.6931375861167908,\n",
       " 0.6931450366973877,\n",
       " 0.6931487321853638,\n",
       " 0.6931454539299011,\n",
       " 0.6931436061859131,\n",
       " 0.6931525468826294,\n",
       " 0.6931586861610413,\n",
       " 0.6931436061859131,\n",
       " 0.6931555867195129,\n",
       " 0.693154513835907,\n",
       " 0.6931403875350952,\n",
       " 0.6931281089782715,\n",
       " 0.6931467056274414,\n",
       " 0.6931422352790833,\n",
       " 0.6931478381156921,\n",
       " 0.6931630969047546,\n",
       " 0.6931722164154053,\n",
       " 0.6931381821632385,\n",
       " 0.6931369304656982,\n",
       " 0.6931551694869995,\n",
       " 0.6931473612785339,\n",
       " 0.6931456923484802,\n",
       " 0.6931213140487671,\n",
       " 0.6931083798408508,\n",
       " 0.69316565990448,\n",
       " 0.6931524276733398,\n",
       " 0.6931423544883728,\n",
       " 0.6931185722351074,\n",
       " 0.6931622624397278,\n",
       " 0.69316166639328,\n",
       " 0.6931657791137695,\n",
       " 0.6931428909301758,\n",
       " 0.6931101083755493,\n",
       " 0.6931993365287781,\n",
       " 0.6931639909744263,\n",
       " 0.6931442022323608,\n",
       " 0.6931602954864502,\n",
       " 0.6931445598602295,\n",
       " 0.6931409239768982,\n",
       " 0.693152904510498,\n",
       " 0.6931440830230713,\n",
       " 0.6931073069572449,\n",
       " 0.6931389570236206,\n",
       " 0.6931406855583191,\n",
       " 0.6931403279304504,\n",
       " 0.6931551694869995,\n",
       " 0.6931387186050415,\n",
       " 0.693150520324707,\n",
       " 0.6931443214416504,\n",
       " 0.6931552886962891,\n",
       " 0.6931248307228088,\n",
       " 0.6931529641151428,\n",
       " 0.6931450963020325,\n",
       " 0.6931124925613403,\n",
       " 0.6931450963020325,\n",
       " 0.693211019039154,\n",
       " 0.6931520700454712,\n",
       " 0.6931234002113342,\n",
       " 0.6931672692298889,\n",
       " 0.6931431293487549,\n",
       " 0.6931141018867493,\n",
       " 0.6931555271148682,\n",
       " 0.6931583881378174,\n",
       " 0.6931561827659607,\n",
       " 0.693181037902832,\n",
       " 0.6931487321853638,\n",
       " 0.693148136138916,\n",
       " 0.6931381225585938,\n",
       " 0.6931254863739014,\n",
       " 0.6931601166725159,\n",
       " 0.6931120157241821,\n",
       " 0.6931332945823669,\n",
       " 0.6931317448616028,\n",
       " 0.6931226253509521,\n",
       " 0.6931549906730652,\n",
       " 0.6931470036506653,\n",
       " 0.6931479573249817,\n",
       " 0.6931484937667847,\n",
       " 0.6931431293487549,\n",
       " 0.6931614875793457,\n",
       " 0.6931462287902832,\n",
       " 0.6931643486022949,\n",
       " 0.6931406259536743,\n",
       " 0.6931562423706055,\n",
       " 0.6931552290916443,\n",
       " 0.6931358575820923,\n",
       " 0.6931442022323608,\n",
       " 0.6931562423706055,\n",
       " 0.6931565999984741,\n",
       " 0.6931593418121338,\n",
       " 0.6931578516960144,\n",
       " 0.6931455135345459,\n",
       " 0.6931370496749878,\n",
       " 0.6931644082069397,\n",
       " 0.6930951476097107,\n",
       " 0.693130373954773,\n",
       " 0.6931601166725159,\n",
       " 0.6931445002555847,\n",
       " 0.6931594610214233,\n",
       " 0.6931419968605042,\n",
       " 0.6931410431861877,\n",
       " 0.6931498050689697,\n",
       " 0.6931383609771729,\n",
       " 0.6931495070457458,\n",
       " 0.6931845545768738,\n",
       " 0.6931325197219849,\n",
       " 0.6931288242340088,\n",
       " 0.6931396126747131,\n",
       " 0.6931388974189758,\n",
       " 0.6931189894676208,\n",
       " 0.693152666091919,\n",
       " 0.6931445002555847,\n",
       " 0.6931441426277161,\n",
       " 0.693143367767334,\n",
       " 0.6931445002555847,\n",
       " 0.6931522488594055,\n",
       " 0.693137526512146,\n",
       " 0.6931450963020325,\n",
       " 0.693136990070343,\n",
       " 0.6931471228599548,\n",
       " 0.6931297779083252,\n",
       " 0.6931452751159668,\n",
       " 0.6931406855583191,\n",
       " 0.6931377053260803,\n",
       " 0.6931425929069519,\n",
       " 0.6931406855583191,\n",
       " 0.6931650042533875,\n",
       " 0.6931465268135071,\n",
       " 0.6931195259094238,\n",
       " 0.6931305527687073,\n",
       " 0.6931443214416504,\n",
       " 0.6931389570236206,\n",
       " 0.693175733089447,\n",
       " 0.6931482553482056,\n",
       " 0.6931569576263428,\n",
       " 0.6931662559509277,\n",
       " 0.6931443810462952,\n",
       " 0.6931401491165161,\n",
       " 0.6931650042533875,\n",
       " 0.6931449174880981,\n",
       " 0.6931452751159668,\n",
       " 0.6931536197662354,\n",
       " 0.6931478381156921,\n",
       " 0.6931527256965637,\n",
       " 0.6931447982788086,\n",
       " 0.693140983581543,\n",
       " 0.6931585073471069,\n",
       " 0.6931635737419128,\n",
       " 0.6931451559066772,\n",
       " 0.693138837814331,\n",
       " 0.6931552290916443,\n",
       " 0.6931588649749756,\n",
       " 0.6931484341621399,\n",
       " 0.6931444406509399,\n",
       " 0.6931414008140564,\n",
       " 0.693153977394104,\n",
       " 0.6931608319282532,\n",
       " 0.6931617259979248,\n",
       " 0.6931483149528503,\n",
       " 0.693170964717865,\n",
       " 0.6931496262550354,\n",
       " 0.693161129951477,\n",
       " 0.6931400895118713,\n",
       " 0.6931500434875488,\n",
       " 0.6931455135345459,\n",
       " 0.6931558847427368,\n",
       " 0.6931465268135071,\n",
       " 0.6931387186050415,\n",
       " 0.6931360960006714,\n",
       " 0.6931623220443726,\n",
       " 0.6931478381156921,\n",
       " 0.6931374073028564,\n",
       " 0.6931591629981995,\n",
       " 0.6931423544883728,\n",
       " 0.6931249499320984,\n",
       " 0.6931520104408264,\n",
       " 0.6931552886962891,\n",
       " 0.6931356191635132,\n",
       " 0.6931496858596802,\n",
       " 0.6931437253952026,\n",
       " 0.6930974721908569,\n",
       " 0.6931444406509399,\n",
       " 0.6931403875350952,\n",
       " 0.6931346654891968,\n",
       " 0.6931595802307129,\n",
       " 0.693087100982666,\n",
       " 0.6931368112564087,\n",
       " 0.6931570768356323,\n",
       " 0.6931416392326355,\n",
       " 0.6931206583976746,\n",
       " 0.6931535005569458,\n",
       " 0.6931592226028442,\n",
       " 0.6931455135345459,\n",
       " 0.6931511163711548,\n",
       " 0.693145215511322,\n",
       " 0.6930741667747498,\n",
       " 0.6931414008140564,\n",
       " 0.6931609511375427,\n",
       " 0.693142294883728,\n",
       " 0.6932151913642883,\n",
       " 0.693138599395752,\n",
       " 0.6931288242340088,\n",
       " 0.6931365728378296,\n",
       " 0.6931419968605042,\n",
       " 0.6931366324424744,\n",
       " 0.6931407451629639,\n",
       " 0.6931397318840027,\n",
       " 0.6931628584861755,\n",
       " 0.6931686401367188,\n",
       " 0.6931185722351074,\n",
       " 0.6931518316268921,\n",
       " 0.6931359767913818,\n",
       " 0.6931532621383667,\n",
       " 0.6931523084640503,\n",
       " 0.6931568384170532,\n",
       " 0.6931008100509644,\n",
       " 0.693139910697937,\n",
       " 0.6931387186050415,\n",
       " 0.6931418776512146,\n",
       " 0.6931446194648743,\n",
       " 0.6931368708610535,\n",
       " 0.6931586861610413,\n",
       " 0.6931423544883728,\n",
       " 0.6931575536727905,\n",
       " 0.693160891532898,\n",
       " 0.6931383013725281,\n",
       " 0.6931496262550354,\n",
       " 0.6931113600730896,\n",
       " 0.693147599697113,\n",
       " 0.6931517124176025,\n",
       " 0.6931360960006714,\n",
       " 0.6931459903717041,\n",
       " 0.693159818649292,\n",
       " 0.693145751953125,\n",
       " 0.6931401491165161,\n",
       " 0.6931468844413757,\n",
       " 0.6931388974189758,\n",
       " 0.6931501030921936,\n",
       " 0.6931139826774597,\n",
       " 0.6931449770927429,\n",
       " ...]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monitor[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vec*label_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_true, y_pred = gather_outputs(test_data, model)\n",
    "test_f1 = f1_score(y_true, y_pred, average=\"micro\")\n",
    "logging.info(\"Test F1: {}\".format(test_f1))\n",
    "monitor[\"test_f1\"].append(test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(monitor[\"test_f1\"]);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
