{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wn_tensors = wn_tensors.to(device)\n",
    "# binary_yin = binary_yin.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/pinocookie/pytorch-dataset-and-dataloader/data\n",
    "# https://discuss.pytorch.org/t/runtimeerror-multi-target-not-supported-newbie/10216/4\n",
    "\n",
    "# Build the Dataset. We are going to generate a simple data set and then we will read it.\n",
    "# Build the DataLoader.\n",
    "# Build the model.\n",
    "# Define the loss function and the optimizer.\n",
    "# Train the model.\n",
    "# Generate predictions.\n",
    "# Plot the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import igraph as ig\n",
    "import collections, gc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from scripts.utils.hierarchy import *\n",
    "from scripts.utils.processing import *\n",
    "from scripts.utils.data_reading import *\n",
    "\n",
    "logging.basicConfig(level=logging.INFO )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 16 # wn vector size  --> ~log_{2}(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N, T_leaves & PI_parents have to be present globally! (list of all the labels)\n",
    "# one_hot_labels because I will keep accessing it for each document <1082>\n",
    "p2c_table, c2p_table, node2id, id2node, PI_parents, T_leaves, N = lookup_table(\"swiki/data/cat_hier.txt\", subset = False)\n",
    "graph_obj = hierarchy2graph(p2c_table, node2id)\n",
    "node2vec = hierarchy_vectors(graph_obj, id2node, p2c_table, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "order_mapping = generate_order_mapping(N)\n",
    "binary_yin = generate_binary_yin(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if (torch.cuda.is_available() and num_gpus > 0) else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def too_hot_mapping(label):\n",
    "\n",
    "    # order_mapping, wn_tensors & binary_yin HAVE TO BE A GLOBAL OBJECT\n",
    "    \n",
    "#     doc_labels = list(map(int, list(label)))\n",
    "    w_n = []\n",
    "    w_pi = []\n",
    "    y_in = []\n",
    "    \n",
    "    try:\n",
    "        int_rep = order_mapping[label]\n",
    "        w_n.append(torch.from_numpy(node2vec[label]).float())\n",
    "        if label in T_leaves:\n",
    "            y_in.append(binary_yin[int_rep-1])\n",
    "            if label in c2p_table:\n",
    "                pi_n = c2p_table[label][0]\n",
    "                w_pi.append(torch.from_numpy(node2vec[pi_n]).float())\n",
    "    except:\n",
    "        print(\"wait whaat?\")\n",
    "    \n",
    "    w_n = list2tensor(w_n).to(device)\n",
    "    w_pi = list2tensor(w_pi).to(device)\n",
    "    y_in = list2tensor(y_in).to(device)\n",
    "    \n",
    "    return w_n, w_pi, y_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_non_leaf_wn(label_id):\n",
    "    '''\n",
    "    accepts label ids only which are non-leaf nodes\n",
    "    '''\n",
    "    \n",
    "    assert label_id in N, \"{} is not a node\".format(label_id)    \n",
    "\n",
    "    C_ids = p2c_table[label_id]\n",
    "    Cn = len(C_ids)\n",
    "\n",
    "    w_n = node2vec[label_id]\n",
    "    w_pi = node2vec[c2p_table[label_id][0]]\n",
    "    sum_wc = 0.0\n",
    "\n",
    "    for idx in C_ids:\n",
    "        w_c = node2vec[idx]\n",
    "        sum_wc += w_c\n",
    "\n",
    "    Wn = 1/(Cn +1) * (w_pi + sum_wc)\n",
    "\n",
    "    return Wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetSWIKI(Dataset):\n",
    "    \n",
    "    def __init__(self, file_path, reduce = True, n_components = 128):\n",
    "        self.w_n = {}\n",
    "        self.reduce = reduce\n",
    "        self.n_components = n_components\n",
    "        self.data, self.labels = lower_dim(file_path, reduce, n_components)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        if self.reduce:\n",
    "            document = torch.from_numpy(self.data[index]).to(device)\n",
    "        else:\n",
    "            document = torch.from_numpy(self.data[index].todense()).to(device)\n",
    "        \n",
    "        label = self.labels[index]\n",
    "        \n",
    "        w_n, w_pi, y_in = too_hot_mapping(label)  \n",
    "        self.w_n[label] = w_n\n",
    "        \n",
    "        return document, label, w_n, w_pi, y_in\n",
    "    \n",
    "    def update_wn(self, label_id):\n",
    "        \n",
    "        self.w_n[label_id] = update_non_leaf_wn(label_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = DatasetSWIKI(\"swiki/data/train_remapped_small.txt\", reduce=True, n_components = n_components)\n",
    "# valid_data = DatasetSWIKI(\"swiki/data/valid_remapped.txt\", reduce=True, n_components = n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data), train_data.w_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle = True)\n",
    "# valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_iter = iter(train_loader)\n",
    "\n",
    "doc, labs, w_n, w_pi, y_in = train_iter.next()\n",
    "\n",
    "print('docs shape on batch size = {}'.format(doc.shape))\n",
    "print('label shape on batch size = {}'.format(labs.shape))\n",
    "print('w_n shape on batch size = {}'.format(w_n.shape))\n",
    "print('w_pi shape on batch size = {}'.format(w_pi.shape))\n",
    "print('y_in shape on batch size = {}'.format(y_in.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters \n",
    "input_size = train_data.data.shape[1] #2085164 -> 128\n",
    "\n",
    "num_classes = n #50312 --> n (16)\n",
    "num_epochs = 30 # TRAIN IT FOR A LOT OF EPOCHS in case of lbfgs (2nd order method) else less is more\n",
    "learning_rate = 0.0002 #1e-4, 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, num_classes, False)\n",
    "        \n",
    "    def forward(self, x, wn):\n",
    "        x1 = self.linear1(x)\n",
    "        return x1*(wn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.SoftMarginLoss(reduction='mean') \n",
    "optimizer = torch.optim.LBFGS(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "logger = Logger('./logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_step = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.memory_cached()-torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.5*torch.sqrt(torch.sum((labels-pis)**2))**2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "w_xi = model(document, labels)\n",
    "torch.sum(torch.log(1+torch.exp(-y_ins*w_xi)), dim=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nn.parameter.Parameter(torch.norm(labels - pis, p=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training the Model\n",
    "losses = []\n",
    "l2 = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_iter = iter(train_loader)\n",
    "    for i, (document, all_labels, labels, pis, y_ins) in enumerate(train_iter):\n",
    "        \n",
    "        document = Variable(document).float().to(device) \n",
    "        \n",
    "        labels = Variable(labels).float().to(device).view(-1, n)\n",
    "        pis = pis.view(-1, n)\n",
    "        y_ins = y_ins.view(-1, n)\n",
    "                \n",
    "        l2_reg = nn.parameter.Parameter(0.5*torch.sqrt(torch.sum((labels-pis)**2))**2)\n",
    "        l2.append(l2_reg)\n",
    "\n",
    "        if type(optimizer) != torch.optim.LBFGS:\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            w_xi = model(document, labels)\n",
    "            loss1 = criterion(w_xi, y_ins) + l2_reg\n",
    "            \n",
    "            if (i+1) % 40 == 0: \n",
    "                print ('Epoch [{}/{}], step:[{}/{}], loss: {:.6f}'.format(epoch+1, num_epochs, i+1, total_step, loss1.item()))\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            losses.append(loss1.item())\n",
    "            loss1.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        else:\n",
    "            def closure():               \n",
    "                optimizer.zero_grad()\n",
    "                w_xi = model(document, labels)\n",
    "                loss1 = criterion(w_xi, y_ins) + l2_reg\n",
    "                \n",
    "                if (i+1) % 40 == 0: \n",
    "                    print ('Epoch [{}/{}], step:[{}/{}], loss: {:.6f}'.format(epoch+1, num_epochs, i+1, total_step, loss1.item()))\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                losses.append(loss1.item())\n",
    "                loss1.backward()\n",
    "                return loss1\n",
    "            optimizer.step(closure)\n",
    "            \n",
    "# optimise LR!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(losses);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umm = []\n",
    "for v in l2:\n",
    "    umm.append(v.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sorted(umm));"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.save(model.state_dict(), 'train_small_model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for documents, _, labels in valid_data:\n",
    "        docs = Variable(torch.from_numpy(documents)).float()\n",
    "        outputs = model(docs)\n",
    "        print(torch.sum(torch.where(outputs>0.0001, torch.tensor(1), torch.tensor(0)), dim=0))\n",
    "        print(torch.sum(torch.where(labels>0, torch.tensor(1), torch.tensor(0)), dim=0))\n",
    "\n",
    "        umm, predicted = torch.max(outputs.data, 1)\n",
    "        print(umm.shape)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "\n",
    "    print('Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'train_valid_model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import umap # fit should get a sparse matrix\n",
    "%time trans = umap.UMAP(n_neighbors=5, random_state=42, n_components=32, verbose=True).fit(train_data.data)\n",
    "trans.embedding_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
