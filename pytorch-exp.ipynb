{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wn_tensors = wn_tensors.to(device)\n",
    "# binary_yin = binary_yin.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/pinocookie/pytorch-dataset-and-dataloader/data\n",
    "# https://discuss.pytorch.org/t/runtimeerror-multi-target-not-supported-newbie/10216/4\n",
    "\n",
    "# Build the Dataset. We are going to generate a simple data set and then we will read it.\n",
    "# Build the DataLoader.\n",
    "# Build the model.\n",
    "# Define the loss function and the optimizer.\n",
    "# Train the model.\n",
    "# Generate predictions.\n",
    "# Plot the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import collections, gc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from sklearn import preprocessing, metrics\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "# from scripts.utils.logger import Logger\n",
    "from scripts.utils.data_reading import *\n",
    "from scripts.utils.processing import *\n",
    "\n",
    "logging.basicConfig(level=logging.INFO )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 16 # wn vector size  --> ~log_{2}(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "65333it [00:00, 257137.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# N, T_leaves & PI_parents have to be present globally! (list of all the labels)\n",
    "# one_hot_labels because I will keep accessing it for each document <1082>\n",
    "p2c_table, c2p_table, _, _, PI_parents, T_leaves, N = lookup_table(\"swiki/data/cat_hier.txt\", subset = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 50312/50312 [00:00<00:00, 189563.36it/s]\n"
     ]
    }
   ],
   "source": [
    "order_mapping = generate_order_mapping(N)\n",
    "wn_tensors = generate_wn(N, n)\n",
    "binary_yin = generate_binary_yin(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if (torch.cuda.is_available() and num_gpus > 0) else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def too_hot_mapping(label):\n",
    "\n",
    "    # order_mapping, wn_tensors & binary_yin HAVE TO BE A GLOBAL OBJECT\n",
    "    \n",
    "#     doc_labels = list(map(int, list(label)))\n",
    "    w_n = []\n",
    "    w_pi = []\n",
    "    y_in = []\n",
    "    \n",
    "    try:\n",
    "        int_rep = order_mapping[label]\n",
    "        w_n.append(wn_tensors[int_rep-1])\n",
    "        if label in T_leaves:\n",
    "            y_in.append(binary_yin[int_rep-1])\n",
    "            if label in c2p_table:\n",
    "                pi_n = order_mapping[c2p_table[label][0]]\n",
    "                w_pi.append(wn_tensors[pi_n-1])\n",
    "    except:\n",
    "        print(\"wait whaat?\")\n",
    "    \n",
    "    w_n = list2tensor(w_n).to(device)\n",
    "    w_pi = list2tensor(w_pi).to(device)\n",
    "    y_in = list2tensor(y_in).to(device)\n",
    "    \n",
    "    return w_n, w_pi, y_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetSWIKI(Dataset):\n",
    "    \n",
    "    def __init__(self, file_path, reduce = True, n_components = 128):\n",
    "        self.reduce = reduce\n",
    "        self.n_components = n_components\n",
    "        self.data, self.labels = lower_dim(file_path, reduce, n_components)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        if self.reduce:\n",
    "            document = torch.from_numpy(self.data[index]).to(device)\n",
    "        else:\n",
    "            document = torch.from_numpy(self.data[index].todense()).to(device)\n",
    "        \n",
    "        label = self.labels[index]\n",
    "        \n",
    "        w_n, w_pi, y_in = too_hot_mapping(label)        \n",
    "        \n",
    "        return document, label, w_n, w_pi, y_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = DatasetSWIKI(\"swiki/data/train_remapped_small.txt\", reduce=True, n_components = n_components)\n",
    "# valid_data = DatasetSWIKI(\"swiki/data/valid_remapped.txt\", reduce=True, n_components = n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1396"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle = True)\n",
    "# valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs shape on batch size = torch.Size([32, 128])\n",
      "label shape on batch size = torch.Size([32])\n",
      "w_n shape on batch size = torch.Size([32, 1, 16])\n",
      "w_pi shape on batch size = torch.Size([32, 1, 16])\n",
      "y_in shape on batch size = torch.Size([32, 1, 16])\n"
     ]
    }
   ],
   "source": [
    "train_iter = iter(train_loader)\n",
    "\n",
    "doc, labs, w_n, w_pi, y_in = train_iter.next()\n",
    "\n",
    "print('docs shape on batch size = {}'.format(doc.shape))\n",
    "print('label shape on batch size = {}'.format(labs.shape))\n",
    "print('w_n shape on batch size = {}'.format(w_n.shape))\n",
    "print('w_pi shape on batch size = {}'.format(w_pi.shape))\n",
    "print('y_in shape on batch size = {}'.format(y_in.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 9.8365, -5.0457, -1.7610, -0.1776, -2.7741,  2.3263, -0.2055,  1.7506,\n",
       "          1.1513,  0.3741, -2.5467, -1.2244,  0.4956,  0.6239,  1.1025,  0.4333,\n",
       "         -0.0221, -0.9534, -1.0785, -0.1622, -1.1985,  1.3402,  0.2410, -1.3047,\n",
       "          0.8091,  1.4383, -0.9604,  0.8942,  1.6953, -0.5882, -0.7627,  0.2241,\n",
       "          1.3576,  0.1927, -0.4254, -1.1699, -0.1532,  0.4774,  0.8600, -1.2309,\n",
       "          0.4896,  0.3407, -0.1808,  0.6861,  0.5701,  0.3511, -1.0711,  1.4789,\n",
       "          0.2393, -0.2528, -0.1849, -1.2790,  1.0678,  0.0774, -0.3383,  2.4926,\n",
       "          0.3256, -1.9476,  0.2631, -1.7924, -1.8929, -0.3645,  0.0732,  1.4308,\n",
       "          0.7945, -0.4393,  1.7312, -0.6344,  0.2225, -1.0386, -0.1363, -1.2747,\n",
       "         -0.8533,  0.0650,  0.3359, -2.2044, -0.1062,  1.2255, -0.3418,  0.1249,\n",
       "         -1.2310, -0.9614, -1.3562, -0.6249,  0.4060,  0.2473, -1.5463,  0.7643,\n",
       "          0.4451, -1.7043,  0.2260, -1.0866,  1.5432,  0.4001, -0.6890,  0.3798,\n",
       "         -0.1583, -0.6503, -0.1117,  0.5798, -1.6438,  1.1445,  1.6411, -0.3180,\n",
       "         -1.1951,  1.8735,  0.0816,  0.9931,  0.7761,  0.8295,  0.9851, -0.9963,\n",
       "          0.7158, -0.2670, -0.8541, -0.4770,  0.0660,  0.4014,  1.1393,  0.3514,\n",
       "         -1.1032,  0.2914,  0.5652,  1.4823, -0.3056, -1.9122,  1.3495, -1.7799],\n",
       "        dtype=torch.float64),\n",
       " 13402,\n",
       " tensor([[0.9069, 0.2945, 0.2527, 0.7020, 0.3367, 0.0900, 0.2541, 0.5268, 0.3378,\n",
       "          0.4858, 0.4239, 0.2036, 0.4931, 0.2289, 0.9206, 0.2354]]),\n",
       " tensor([[0.7469, 0.1345, 0.3135, 0.8404, 0.9329, 0.4151, 0.7060, 0.7008, 0.3826,\n",
       "          0.9310, 0.1312, 0.3248, 0.4052, 0.4325, 0.4348, 0.4708]]),\n",
       " tensor([[-1., -1., -1., -1., -1.,  1., -1., -1., -1.,  1., -1., -1.,  1., -1.,\n",
       "          -1., -1.]]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters \n",
    "input_size = train_data.data.shape[1] #2085164 -> 128\n",
    "\n",
    "num_classes = n #50312 --> n (16)\n",
    "num_epochs = 30 # TRAIN IT FOR A LOT OF EPOCHS!!!\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, num_classes, False)\n",
    "        \n",
    "    def forward(self, x, wn):\n",
    "        x1 = self.linear1(x)\n",
    "        return x1*(wn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(\n",
       "  (linear1): Linear(in_features=128, out_features=16, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.SoftMarginLoss(reduction='mean') \n",
    "L2Loss = nn.MSELoss()\n",
    "optimizer = torch.optim.LBFGS(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "logger = Logger('./logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_step = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.memory_cached()-torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0134, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.parameter.Parameter(torch.mean(0.5*torch.sqrt(torch.sum((labels-pis)**2, dim=1))**2))*1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "torch.Size([16, 128])\n",
      "tensor(2.2950, grad_fn=<NormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for m in model.parameters():\n",
    "    print(type(m))\n",
    "    print(m.shape)\n",
    "    print(m.norm(p=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], step:[40/44], loss: 0.701780\n",
      "Epoch [1/30], step:[40/44], loss: 0.701776\n",
      "Epoch [1/30], step:[40/44], loss: 0.701670\n",
      "Epoch [1/30], step:[40/44], loss: 0.701461\n",
      "Epoch [1/30], step:[40/44], loss: 0.701159\n",
      "Epoch [1/30], step:[40/44], loss: 0.700785\n",
      "Epoch [1/30], step:[40/44], loss: 0.700378\n",
      "Epoch [1/30], step:[40/44], loss: 0.699948\n",
      "Epoch [1/30], step:[40/44], loss: 0.699492\n",
      "Epoch [1/30], step:[40/44], loss: 0.699007\n",
      "Epoch [1/30], step:[40/44], loss: 0.698487\n",
      "Epoch [1/30], step:[40/44], loss: 0.697928\n",
      "Epoch [1/30], step:[40/44], loss: 0.697341\n",
      "Epoch [1/30], step:[40/44], loss: 0.696745\n",
      "Epoch [1/30], step:[40/44], loss: 0.696138\n",
      "Epoch [1/30], step:[40/44], loss: 0.695504\n",
      "Epoch [1/30], step:[40/44], loss: 0.694851\n",
      "Epoch [1/30], step:[40/44], loss: 0.694196\n",
      "Epoch [1/30], step:[40/44], loss: 0.693538\n",
      "Epoch [1/30], step:[40/44], loss: 0.692863\n",
      "Epoch [2/30], step:[40/44], loss: 0.689873\n",
      "Epoch [2/30], step:[40/44], loss: 0.689846\n",
      "Epoch [2/30], step:[40/44], loss: 0.689708\n",
      "Epoch [2/30], step:[40/44], loss: 0.689504\n",
      "Epoch [2/30], step:[40/44], loss: 0.689194\n",
      "Epoch [2/30], step:[40/44], loss: 0.688795\n",
      "Epoch [2/30], step:[40/44], loss: 0.688309\n",
      "Epoch [2/30], step:[40/44], loss: 0.687809\n",
      "Epoch [2/30], step:[40/44], loss: 0.687331\n",
      "Epoch [2/30], step:[40/44], loss: 0.686813\n",
      "Epoch [2/30], step:[40/44], loss: 0.686240\n",
      "Epoch [2/30], step:[40/44], loss: 0.685633\n",
      "Epoch [2/30], step:[40/44], loss: 0.684988\n",
      "Epoch [2/30], step:[40/44], loss: 0.684291\n",
      "Epoch [2/30], step:[40/44], loss: 0.683614\n",
      "Epoch [2/30], step:[40/44], loss: 0.682963\n",
      "Epoch [2/30], step:[40/44], loss: 0.682293\n",
      "Epoch [2/30], step:[40/44], loss: 0.681601\n",
      "Epoch [2/30], step:[40/44], loss: 0.680914\n",
      "Epoch [2/30], step:[40/44], loss: 0.680224\n",
      "Epoch [3/30], step:[40/44], loss: 0.662275\n",
      "Epoch [3/30], step:[40/44], loss: 0.662266\n",
      "Epoch [3/30], step:[40/44], loss: 0.662123\n",
      "Epoch [3/30], step:[40/44], loss: 0.661867\n",
      "Epoch [3/30], step:[40/44], loss: 0.661539\n",
      "Epoch [3/30], step:[40/44], loss: 0.661119\n",
      "Epoch [3/30], step:[40/44], loss: 0.660723\n",
      "Epoch [3/30], step:[40/44], loss: 0.660307\n",
      "Epoch [3/30], step:[40/44], loss: 0.659837\n",
      "Epoch [3/30], step:[40/44], loss: 0.659336\n",
      "Epoch [3/30], step:[40/44], loss: 0.658829\n",
      "Epoch [3/30], step:[40/44], loss: 0.658312\n",
      "Epoch [3/30], step:[40/44], loss: 0.657699\n",
      "Epoch [3/30], step:[40/44], loss: 0.657092\n",
      "Epoch [3/30], step:[40/44], loss: 0.656526\n",
      "Epoch [3/30], step:[40/44], loss: 0.655944\n",
      "Epoch [3/30], step:[40/44], loss: 0.655350\n",
      "Epoch [3/30], step:[40/44], loss: 0.654748\n",
      "Epoch [3/30], step:[40/44], loss: 0.654135\n",
      "Epoch [3/30], step:[40/44], loss: 0.653514\n",
      "Epoch [4/30], step:[40/44], loss: 0.633975\n",
      "Epoch [4/30], step:[40/44], loss: 0.633959\n",
      "Epoch [4/30], step:[40/44], loss: 0.633843\n",
      "Epoch [4/30], step:[40/44], loss: 0.633688\n",
      "Epoch [4/30], step:[40/44], loss: 0.633478\n",
      "Epoch [4/30], step:[40/44], loss: 0.633226\n",
      "Epoch [4/30], step:[40/44], loss: 0.632940\n",
      "Epoch [4/30], step:[40/44], loss: 0.632595\n",
      "Epoch [4/30], step:[40/44], loss: 0.632220\n",
      "Epoch [4/30], step:[40/44], loss: 0.631836\n",
      "Epoch [4/30], step:[40/44], loss: 0.631447\n",
      "Epoch [4/30], step:[40/44], loss: 0.631036\n",
      "Epoch [4/30], step:[40/44], loss: 0.630583\n",
      "Epoch [4/30], step:[40/44], loss: 0.630133\n",
      "Epoch [4/30], step:[40/44], loss: 0.629671\n",
      "Epoch [4/30], step:[40/44], loss: 0.629186\n",
      "Epoch [4/30], step:[40/44], loss: 0.628686\n",
      "Epoch [4/30], step:[40/44], loss: 0.628165\n",
      "Epoch [4/30], step:[40/44], loss: 0.627618\n",
      "Epoch [4/30], step:[40/44], loss: 0.627065\n",
      "Epoch [5/30], step:[40/44], loss: 0.662006\n",
      "Epoch [5/30], step:[40/44], loss: 0.662002\n",
      "Epoch [5/30], step:[40/44], loss: 0.661909\n",
      "Epoch [5/30], step:[40/44], loss: 0.661755\n",
      "Epoch [5/30], step:[40/44], loss: 0.661545\n",
      "Epoch [5/30], step:[40/44], loss: 0.661274\n",
      "Epoch [5/30], step:[40/44], loss: 0.660947\n",
      "Epoch [5/30], step:[40/44], loss: 0.660603\n",
      "Epoch [5/30], step:[40/44], loss: 0.660246\n",
      "Epoch [5/30], step:[40/44], loss: 0.659855\n",
      "Epoch [5/30], step:[40/44], loss: 0.659416\n",
      "Epoch [5/30], step:[40/44], loss: 0.658940\n",
      "Epoch [5/30], step:[40/44], loss: 0.658474\n",
      "Epoch [5/30], step:[40/44], loss: 0.657985\n",
      "Epoch [5/30], step:[40/44], loss: 0.657456\n",
      "Epoch [5/30], step:[40/44], loss: 0.656945\n",
      "Epoch [5/30], step:[40/44], loss: 0.656438\n",
      "Epoch [5/30], step:[40/44], loss: 0.655912\n",
      "Epoch [5/30], step:[40/44], loss: 0.655302\n",
      "Epoch [5/30], step:[40/44], loss: 0.654730\n",
      "Epoch [6/30], step:[40/44], loss: 0.638961\n",
      "Epoch [6/30], step:[40/44], loss: 0.638953\n",
      "Epoch [6/30], step:[40/44], loss: 0.638858\n",
      "Epoch [6/30], step:[40/44], loss: 0.638695\n",
      "Epoch [6/30], step:[40/44], loss: 0.638448\n",
      "Epoch [6/30], step:[40/44], loss: 0.638183\n",
      "Epoch [6/30], step:[40/44], loss: 0.637884\n",
      "Epoch [6/30], step:[40/44], loss: 0.637511\n",
      "Epoch [6/30], step:[40/44], loss: 0.637133\n",
      "Epoch [6/30], step:[40/44], loss: 0.636735\n",
      "Epoch [6/30], step:[40/44], loss: 0.636284\n",
      "Epoch [6/30], step:[40/44], loss: 0.635810\n",
      "Epoch [6/30], step:[40/44], loss: 0.635334\n",
      "Epoch [6/30], step:[40/44], loss: 0.634833\n",
      "Epoch [6/30], step:[40/44], loss: 0.634293\n",
      "Epoch [6/30], step:[40/44], loss: 0.633736\n",
      "Epoch [6/30], step:[40/44], loss: 0.633186\n",
      "Epoch [6/30], step:[40/44], loss: 0.632619\n",
      "Epoch [6/30], step:[40/44], loss: 0.632010\n",
      "Epoch [6/30], step:[40/44], loss: 0.631430\n",
      "Epoch [7/30], step:[40/44], loss: 0.665746\n",
      "Epoch [7/30], step:[40/44], loss: 0.665708\n",
      "Epoch [7/30], step:[40/44], loss: 0.665595\n",
      "Epoch [7/30], step:[40/44], loss: 0.665430\n",
      "Epoch [7/30], step:[40/44], loss: 0.665191\n",
      "Epoch [7/30], step:[40/44], loss: 0.664861\n",
      "Epoch [7/30], step:[40/44], loss: 0.664495\n",
      "Epoch [7/30], step:[40/44], loss: 0.664069\n",
      "Epoch [7/30], step:[40/44], loss: 0.663660\n",
      "Epoch [7/30], step:[40/44], loss: 0.663227\n",
      "Epoch [7/30], step:[40/44], loss: 0.662719\n",
      "Epoch [7/30], step:[40/44], loss: 0.662200\n",
      "Epoch [7/30], step:[40/44], loss: 0.661680\n",
      "Epoch [7/30], step:[40/44], loss: 0.661150\n",
      "Epoch [7/30], step:[40/44], loss: 0.660585\n",
      "Epoch [7/30], step:[40/44], loss: 0.659986\n",
      "Epoch [7/30], step:[40/44], loss: 0.659365\n",
      "Epoch [7/30], step:[40/44], loss: 0.658724\n",
      "Epoch [7/30], step:[40/44], loss: 0.658058\n",
      "Epoch [7/30], step:[40/44], loss: 0.657378\n",
      "Epoch [8/30], step:[40/44], loss: 0.655957\n",
      "Epoch [8/30], step:[40/44], loss: 0.655949\n",
      "Epoch [8/30], step:[40/44], loss: 0.655864\n",
      "Epoch [8/30], step:[40/44], loss: 0.655697\n",
      "Epoch [8/30], step:[40/44], loss: 0.655443\n",
      "Epoch [8/30], step:[40/44], loss: 0.655109\n",
      "Epoch [8/30], step:[40/44], loss: 0.654728\n",
      "Epoch [8/30], step:[40/44], loss: 0.654290\n",
      "Epoch [8/30], step:[40/44], loss: 0.653822\n",
      "Epoch [8/30], step:[40/44], loss: 0.653377\n",
      "Epoch [8/30], step:[40/44], loss: 0.652901\n",
      "Epoch [8/30], step:[40/44], loss: 0.652396\n",
      "Epoch [8/30], step:[40/44], loss: 0.651896\n",
      "Epoch [8/30], step:[40/44], loss: 0.651372\n",
      "Epoch [8/30], step:[40/44], loss: 0.650811\n",
      "Epoch [8/30], step:[40/44], loss: 0.650235\n",
      "Epoch [8/30], step:[40/44], loss: 0.649657\n",
      "Epoch [8/30], step:[40/44], loss: 0.649083\n",
      "Epoch [8/30], step:[40/44], loss: 0.648489\n",
      "Epoch [8/30], step:[40/44], loss: 0.647871\n",
      "Epoch [9/30], step:[40/44], loss: 0.657447\n",
      "Epoch [9/30], step:[40/44], loss: 0.657440\n",
      "Epoch [9/30], step:[40/44], loss: 0.657341\n",
      "Epoch [9/30], step:[40/44], loss: 0.657203\n",
      "Epoch [9/30], step:[40/44], loss: 0.657002\n",
      "Epoch [9/30], step:[40/44], loss: 0.656744\n",
      "Epoch [9/30], step:[40/44], loss: 0.656435\n",
      "Epoch [9/30], step:[40/44], loss: 0.656065\n",
      "Epoch [9/30], step:[40/44], loss: 0.655670\n",
      "Epoch [9/30], step:[40/44], loss: 0.655261\n",
      "Epoch [9/30], step:[40/44], loss: 0.654828\n",
      "Epoch [9/30], step:[40/44], loss: 0.654361\n",
      "Epoch [9/30], step:[40/44], loss: 0.653856\n",
      "Epoch [9/30], step:[40/44], loss: 0.653338\n",
      "Epoch [9/30], step:[40/44], loss: 0.652807\n",
      "Epoch [9/30], step:[40/44], loss: 0.652257\n",
      "Epoch [9/30], step:[40/44], loss: 0.651691\n",
      "Epoch [9/30], step:[40/44], loss: 0.651113\n",
      "Epoch [9/30], step:[40/44], loss: 0.650526\n",
      "Epoch [9/30], step:[40/44], loss: 0.649900\n",
      "Epoch [10/30], step:[40/44], loss: 0.675455\n",
      "Epoch [10/30], step:[40/44], loss: 0.675445\n",
      "Epoch [10/30], step:[40/44], loss: 0.675287\n",
      "Epoch [10/30], step:[40/44], loss: 0.675030\n",
      "Epoch [10/30], step:[40/44], loss: 0.674603\n",
      "Epoch [10/30], step:[40/44], loss: 0.674104\n",
      "Epoch [10/30], step:[40/44], loss: 0.673560\n",
      "Epoch [10/30], step:[40/44], loss: 0.672894\n",
      "Epoch [10/30], step:[40/44], loss: 0.672208\n",
      "Epoch [10/30], step:[40/44], loss: 0.671524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/30], step:[40/44], loss: 0.670789\n",
      "Epoch [10/30], step:[40/44], loss: 0.669994\n",
      "Epoch [10/30], step:[40/44], loss: 0.669203\n",
      "Epoch [10/30], step:[40/44], loss: 0.668447\n",
      "Epoch [10/30], step:[40/44], loss: 0.667655\n",
      "Epoch [10/30], step:[40/44], loss: 0.666843\n",
      "Epoch [10/30], step:[40/44], loss: 0.666043\n",
      "Epoch [10/30], step:[40/44], loss: 0.665224\n",
      "Epoch [10/30], step:[40/44], loss: 0.664428\n",
      "Epoch [10/30], step:[40/44], loss: 0.663645\n",
      "Epoch [11/30], step:[40/44], loss: 0.675747\n",
      "Epoch [11/30], step:[40/44], loss: 0.675734\n",
      "Epoch [11/30], step:[40/44], loss: 0.675646\n",
      "Epoch [11/30], step:[40/44], loss: 0.675503\n",
      "Epoch [11/30], step:[40/44], loss: 0.675274\n",
      "Epoch [11/30], step:[40/44], loss: 0.674955\n",
      "Epoch [11/30], step:[40/44], loss: 0.674545\n",
      "Epoch [11/30], step:[40/44], loss: 0.674150\n",
      "Epoch [11/30], step:[40/44], loss: 0.673725\n",
      "Epoch [11/30], step:[40/44], loss: 0.673230\n",
      "Epoch [11/30], step:[40/44], loss: 0.672747\n",
      "Epoch [11/30], step:[40/44], loss: 0.672250\n",
      "Epoch [11/30], step:[40/44], loss: 0.671709\n",
      "Epoch [11/30], step:[40/44], loss: 0.671140\n",
      "Epoch [11/30], step:[40/44], loss: 0.670584\n",
      "Epoch [11/30], step:[40/44], loss: 0.669999\n",
      "Epoch [11/30], step:[40/44], loss: 0.669391\n",
      "Epoch [11/30], step:[40/44], loss: 0.668790\n",
      "Epoch [11/30], step:[40/44], loss: 0.668185\n",
      "Epoch [11/30], step:[40/44], loss: 0.667561\n",
      "Epoch [12/30], step:[40/44], loss: 0.636496\n",
      "Epoch [12/30], step:[40/44], loss: 0.636479\n",
      "Epoch [12/30], step:[40/44], loss: 0.636275\n",
      "Epoch [12/30], step:[40/44], loss: 0.636093\n",
      "Epoch [12/30], step:[40/44], loss: 0.635829\n",
      "Epoch [12/30], step:[40/44], loss: 0.635501\n",
      "Epoch [12/30], step:[40/44], loss: 0.635128\n",
      "Epoch [12/30], step:[40/44], loss: 0.634706\n",
      "Epoch [12/30], step:[40/44], loss: 0.634259\n",
      "Epoch [12/30], step:[40/44], loss: 0.633792\n",
      "Epoch [12/30], step:[40/44], loss: 0.633299\n",
      "Epoch [12/30], step:[40/44], loss: 0.632774\n",
      "Epoch [12/30], step:[40/44], loss: 0.632217\n",
      "Epoch [12/30], step:[40/44], loss: 0.631662\n",
      "Epoch [12/30], step:[40/44], loss: 0.631104\n",
      "Epoch [12/30], step:[40/44], loss: 0.630528\n",
      "Epoch [12/30], step:[40/44], loss: 0.629899\n",
      "Epoch [12/30], step:[40/44], loss: 0.629212\n",
      "Epoch [12/30], step:[40/44], loss: 0.628554\n",
      "Epoch [12/30], step:[40/44], loss: 0.627910\n",
      "Epoch [13/30], step:[40/44], loss: 0.632621\n",
      "Epoch [13/30], step:[40/44], loss: 0.632615\n",
      "Epoch [13/30], step:[40/44], loss: 0.632512\n",
      "Epoch [13/30], step:[40/44], loss: 0.632312\n",
      "Epoch [13/30], step:[40/44], loss: 0.632031\n",
      "Epoch [13/30], step:[40/44], loss: 0.631640\n",
      "Epoch [13/30], step:[40/44], loss: 0.631233\n",
      "Epoch [13/30], step:[40/44], loss: 0.630825\n",
      "Epoch [13/30], step:[40/44], loss: 0.630394\n",
      "Epoch [13/30], step:[40/44], loss: 0.629935\n",
      "Epoch [13/30], step:[40/44], loss: 0.629435\n",
      "Epoch [13/30], step:[40/44], loss: 0.628942\n",
      "Epoch [13/30], step:[40/44], loss: 0.628445\n",
      "Epoch [13/30], step:[40/44], loss: 0.627940\n",
      "Epoch [13/30], step:[40/44], loss: 0.627419\n",
      "Epoch [13/30], step:[40/44], loss: 0.626866\n",
      "Epoch [13/30], step:[40/44], loss: 0.626290\n",
      "Epoch [13/30], step:[40/44], loss: 0.625712\n",
      "Epoch [13/30], step:[40/44], loss: 0.625138\n",
      "Epoch [13/30], step:[40/44], loss: 0.624554\n",
      "Epoch [14/30], step:[40/44], loss: 0.627099\n",
      "Epoch [14/30], step:[40/44], loss: 0.627079\n",
      "Epoch [14/30], step:[40/44], loss: 0.626916\n",
      "Epoch [14/30], step:[40/44], loss: 0.626720\n",
      "Epoch [14/30], step:[40/44], loss: 0.626417\n",
      "Epoch [14/30], step:[40/44], loss: 0.626131\n",
      "Epoch [14/30], step:[40/44], loss: 0.625808\n",
      "Epoch [14/30], step:[40/44], loss: 0.625350\n",
      "Epoch [14/30], step:[40/44], loss: 0.624922\n",
      "Epoch [14/30], step:[40/44], loss: 0.624495\n",
      "Epoch [14/30], step:[40/44], loss: 0.623993\n",
      "Epoch [14/30], step:[40/44], loss: 0.623448\n",
      "Epoch [14/30], step:[40/44], loss: 0.622925\n",
      "Epoch [14/30], step:[40/44], loss: 0.622391\n",
      "Epoch [14/30], step:[40/44], loss: 0.621827\n",
      "Epoch [14/30], step:[40/44], loss: 0.621222\n",
      "Epoch [14/30], step:[40/44], loss: 0.620659\n",
      "Epoch [14/30], step:[40/44], loss: 0.620049\n",
      "Epoch [14/30], step:[40/44], loss: 0.619442\n",
      "Epoch [14/30], step:[40/44], loss: 0.618864\n",
      "Epoch [15/30], step:[40/44], loss: 0.675035\n",
      "Epoch [15/30], step:[40/44], loss: 0.675022\n",
      "Epoch [15/30], step:[40/44], loss: 0.674904\n",
      "Epoch [15/30], step:[40/44], loss: 0.674659\n",
      "Epoch [15/30], step:[40/44], loss: 0.674274\n",
      "Epoch [15/30], step:[40/44], loss: 0.673856\n",
      "Epoch [15/30], step:[40/44], loss: 0.673415\n",
      "Epoch [15/30], step:[40/44], loss: 0.672890\n",
      "Epoch [15/30], step:[40/44], loss: 0.672349\n",
      "Epoch [15/30], step:[40/44], loss: 0.671796\n",
      "Epoch [15/30], step:[40/44], loss: 0.671207\n",
      "Epoch [15/30], step:[40/44], loss: 0.670613\n",
      "Epoch [15/30], step:[40/44], loss: 0.669994\n",
      "Epoch [15/30], step:[40/44], loss: 0.669352\n",
      "Epoch [15/30], step:[40/44], loss: 0.668712\n",
      "Epoch [15/30], step:[40/44], loss: 0.668079\n",
      "Epoch [15/30], step:[40/44], loss: 0.667432\n",
      "Epoch [15/30], step:[40/44], loss: 0.666747\n",
      "Epoch [15/30], step:[40/44], loss: 0.666075\n",
      "Epoch [15/30], step:[40/44], loss: 0.665414\n",
      "Epoch [16/30], step:[40/44], loss: 0.664683\n",
      "Epoch [16/30], step:[40/44], loss: 0.664676\n",
      "Epoch [16/30], step:[40/44], loss: 0.664585\n",
      "Epoch [16/30], step:[40/44], loss: 0.664414\n",
      "Epoch [16/30], step:[40/44], loss: 0.664161\n",
      "Epoch [16/30], step:[40/44], loss: 0.663800\n",
      "Epoch [16/30], step:[40/44], loss: 0.663359\n",
      "Epoch [16/30], step:[40/44], loss: 0.662926\n",
      "Epoch [16/30], step:[40/44], loss: 0.662478\n",
      "Epoch [16/30], step:[40/44], loss: 0.661999\n",
      "Epoch [16/30], step:[40/44], loss: 0.661502\n",
      "Epoch [16/30], step:[40/44], loss: 0.660991\n",
      "Epoch [16/30], step:[40/44], loss: 0.660438\n",
      "Epoch [16/30], step:[40/44], loss: 0.659879\n",
      "Epoch [16/30], step:[40/44], loss: 0.659308\n",
      "Epoch [16/30], step:[40/44], loss: 0.658710\n",
      "Epoch [16/30], step:[40/44], loss: 0.658130\n",
      "Epoch [16/30], step:[40/44], loss: 0.657537\n",
      "Epoch [16/30], step:[40/44], loss: 0.656905\n",
      "Epoch [16/30], step:[40/44], loss: 0.656285\n",
      "Epoch [17/30], step:[40/44], loss: 0.664024\n",
      "Epoch [17/30], step:[40/44], loss: 0.664018\n",
      "Epoch [17/30], step:[40/44], loss: 0.663932\n",
      "Epoch [17/30], step:[40/44], loss: 0.663748\n",
      "Epoch [17/30], step:[40/44], loss: 0.663427\n",
      "Epoch [17/30], step:[40/44], loss: 0.663020\n",
      "Epoch [17/30], step:[40/44], loss: 0.662560\n",
      "Epoch [17/30], step:[40/44], loss: 0.661999\n",
      "Epoch [17/30], step:[40/44], loss: 0.661344\n",
      "Epoch [17/30], step:[40/44], loss: 0.660618\n",
      "Epoch [17/30], step:[40/44], loss: 0.659876\n",
      "Epoch [17/30], step:[40/44], loss: 0.659055\n",
      "Epoch [17/30], step:[40/44], loss: 0.658270\n",
      "Epoch [17/30], step:[40/44], loss: 0.657527\n",
      "Epoch [17/30], step:[40/44], loss: 0.656751\n",
      "Epoch [17/30], step:[40/44], loss: 0.655932\n",
      "Epoch [17/30], step:[40/44], loss: 0.655066\n",
      "Epoch [17/30], step:[40/44], loss: 0.654226\n",
      "Epoch [17/30], step:[40/44], loss: 0.653399\n",
      "Epoch [17/30], step:[40/44], loss: 0.652545\n",
      "Epoch [18/30], step:[40/44], loss: 0.649094\n",
      "Epoch [18/30], step:[40/44], loss: 0.649065\n",
      "Epoch [18/30], step:[40/44], loss: 0.648987\n",
      "Epoch [18/30], step:[40/44], loss: 0.648824\n",
      "Epoch [18/30], step:[40/44], loss: 0.648582\n",
      "Epoch [18/30], step:[40/44], loss: 0.648286\n",
      "Epoch [18/30], step:[40/44], loss: 0.647970\n",
      "Epoch [18/30], step:[40/44], loss: 0.647628\n",
      "Epoch [18/30], step:[40/44], loss: 0.647252\n",
      "Epoch [18/30], step:[40/44], loss: 0.646822\n",
      "Epoch [18/30], step:[40/44], loss: 0.646368\n",
      "Epoch [18/30], step:[40/44], loss: 0.645906\n",
      "Epoch [18/30], step:[40/44], loss: 0.645414\n",
      "Epoch [18/30], step:[40/44], loss: 0.644914\n",
      "Epoch [18/30], step:[40/44], loss: 0.644417\n",
      "Epoch [18/30], step:[40/44], loss: 0.643905\n",
      "Epoch [18/30], step:[40/44], loss: 0.643370\n",
      "Epoch [18/30], step:[40/44], loss: 0.642802\n",
      "Epoch [18/30], step:[40/44], loss: 0.642264\n",
      "Epoch [18/30], step:[40/44], loss: 0.641733\n",
      "Epoch [19/30], step:[40/44], loss: 0.648408\n",
      "Epoch [19/30], step:[40/44], loss: 0.648402\n",
      "Epoch [19/30], step:[40/44], loss: 0.648317\n",
      "Epoch [19/30], step:[40/44], loss: 0.648144\n",
      "Epoch [19/30], step:[40/44], loss: 0.647842\n",
      "Epoch [19/30], step:[40/44], loss: 0.647534\n",
      "Epoch [19/30], step:[40/44], loss: 0.647176\n",
      "Epoch [19/30], step:[40/44], loss: 0.646725\n",
      "Epoch [19/30], step:[40/44], loss: 0.646221\n",
      "Epoch [19/30], step:[40/44], loss: 0.645711\n",
      "Epoch [19/30], step:[40/44], loss: 0.645178\n",
      "Epoch [19/30], step:[40/44], loss: 0.644566\n",
      "Epoch [19/30], step:[40/44], loss: 0.643983\n",
      "Epoch [19/30], step:[40/44], loss: 0.643429\n",
      "Epoch [19/30], step:[40/44], loss: 0.642824\n",
      "Epoch [19/30], step:[40/44], loss: 0.642174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/30], step:[40/44], loss: 0.641565\n",
      "Epoch [19/30], step:[40/44], loss: 0.640958\n",
      "Epoch [19/30], step:[40/44], loss: 0.640280\n",
      "Epoch [19/30], step:[40/44], loss: 0.639651\n",
      "Epoch [20/30], step:[40/44], loss: 0.668756\n",
      "Epoch [20/30], step:[40/44], loss: 0.668742\n",
      "Epoch [20/30], step:[40/44], loss: 0.668604\n",
      "Epoch [20/30], step:[40/44], loss: 0.668354\n",
      "Epoch [20/30], step:[40/44], loss: 0.668030\n",
      "Epoch [20/30], step:[40/44], loss: 0.667698\n",
      "Epoch [20/30], step:[40/44], loss: 0.667283\n",
      "Epoch [20/30], step:[40/44], loss: 0.666839\n",
      "Epoch [20/30], step:[40/44], loss: 0.666366\n",
      "Epoch [20/30], step:[40/44], loss: 0.665797\n",
      "Epoch [20/30], step:[40/44], loss: 0.665181\n",
      "Epoch [20/30], step:[40/44], loss: 0.664568\n",
      "Epoch [20/30], step:[40/44], loss: 0.663917\n",
      "Epoch [20/30], step:[40/44], loss: 0.663224\n",
      "Epoch [20/30], step:[40/44], loss: 0.662473\n",
      "Epoch [20/30], step:[40/44], loss: 0.661687\n",
      "Epoch [20/30], step:[40/44], loss: 0.660897\n",
      "Epoch [20/30], step:[40/44], loss: 0.660096\n",
      "Epoch [20/30], step:[40/44], loss: 0.659256\n",
      "Epoch [20/30], step:[40/44], loss: 0.658370\n",
      "Epoch [21/30], step:[40/44], loss: 0.672321\n",
      "Epoch [21/30], step:[40/44], loss: 0.672315\n",
      "Epoch [21/30], step:[40/44], loss: 0.672164\n",
      "Epoch [21/30], step:[40/44], loss: 0.671900\n",
      "Epoch [21/30], step:[40/44], loss: 0.671542\n",
      "Epoch [21/30], step:[40/44], loss: 0.671106\n",
      "Epoch [21/30], step:[40/44], loss: 0.670602\n",
      "Epoch [21/30], step:[40/44], loss: 0.670029\n",
      "Epoch [21/30], step:[40/44], loss: 0.669395\n",
      "Epoch [21/30], step:[40/44], loss: 0.668709\n",
      "Epoch [21/30], step:[40/44], loss: 0.668050\n",
      "Epoch [21/30], step:[40/44], loss: 0.667395\n",
      "Epoch [21/30], step:[40/44], loss: 0.666696\n",
      "Epoch [21/30], step:[40/44], loss: 0.666006\n",
      "Epoch [21/30], step:[40/44], loss: 0.665318\n",
      "Epoch [21/30], step:[40/44], loss: 0.664631\n",
      "Epoch [21/30], step:[40/44], loss: 0.663917\n",
      "Epoch [21/30], step:[40/44], loss: 0.663207\n",
      "Epoch [21/30], step:[40/44], loss: 0.662523\n",
      "Epoch [21/30], step:[40/44], loss: 0.661831\n",
      "Epoch [22/30], step:[40/44], loss: 0.657040\n",
      "Epoch [22/30], step:[40/44], loss: 0.657027\n",
      "Epoch [22/30], step:[40/44], loss: 0.656854\n",
      "Epoch [22/30], step:[40/44], loss: 0.656598\n",
      "Epoch [22/30], step:[40/44], loss: 0.656241\n",
      "Epoch [22/30], step:[40/44], loss: 0.655807\n",
      "Epoch [22/30], step:[40/44], loss: 0.655315\n",
      "Epoch [22/30], step:[40/44], loss: 0.654776\n",
      "Epoch [22/30], step:[40/44], loss: 0.654187\n",
      "Epoch [22/30], step:[40/44], loss: 0.653542\n",
      "Epoch [22/30], step:[40/44], loss: 0.652891\n",
      "Epoch [22/30], step:[40/44], loss: 0.652240\n",
      "Epoch [22/30], step:[40/44], loss: 0.651552\n",
      "Epoch [22/30], step:[40/44], loss: 0.650861\n",
      "Epoch [22/30], step:[40/44], loss: 0.650161\n",
      "Epoch [22/30], step:[40/44], loss: 0.649417\n",
      "Epoch [22/30], step:[40/44], loss: 0.648690\n",
      "Epoch [22/30], step:[40/44], loss: 0.647960\n",
      "Epoch [22/30], step:[40/44], loss: 0.647239\n",
      "Epoch [22/30], step:[40/44], loss: 0.646520\n",
      "Epoch [23/30], step:[40/44], loss: 0.668486\n",
      "Epoch [23/30], step:[40/44], loss: 0.668473\n",
      "Epoch [23/30], step:[40/44], loss: 0.668196\n",
      "Epoch [23/30], step:[40/44], loss: 0.667962\n",
      "Epoch [23/30], step:[40/44], loss: 0.667625\n",
      "Epoch [23/30], step:[40/44], loss: 0.667202\n",
      "Epoch [23/30], step:[40/44], loss: 0.666749\n",
      "Epoch [23/30], step:[40/44], loss: 0.666269\n",
      "Epoch [23/30], step:[40/44], loss: 0.665751\n",
      "Epoch [23/30], step:[40/44], loss: 0.665187\n",
      "Epoch [23/30], step:[40/44], loss: 0.664597\n",
      "Epoch [23/30], step:[40/44], loss: 0.663996\n",
      "Epoch [23/30], step:[40/44], loss: 0.663373\n",
      "Epoch [23/30], step:[40/44], loss: 0.662723\n",
      "Epoch [23/30], step:[40/44], loss: 0.662072\n",
      "Epoch [23/30], step:[40/44], loss: 0.661411\n",
      "Epoch [23/30], step:[40/44], loss: 0.660716\n",
      "Epoch [23/30], step:[40/44], loss: 0.659992\n",
      "Epoch [23/30], step:[40/44], loss: 0.659299\n",
      "Epoch [23/30], step:[40/44], loss: 0.658619\n",
      "Epoch [24/30], step:[40/44], loss: 0.641456\n",
      "Epoch [24/30], step:[40/44], loss: 0.641444\n",
      "Epoch [24/30], step:[40/44], loss: 0.641250\n",
      "Epoch [24/30], step:[40/44], loss: 0.640953\n",
      "Epoch [24/30], step:[40/44], loss: 0.640514\n",
      "Epoch [24/30], step:[40/44], loss: 0.639997\n",
      "Epoch [24/30], step:[40/44], loss: 0.639436\n",
      "Epoch [24/30], step:[40/44], loss: 0.638781\n",
      "Epoch [24/30], step:[40/44], loss: 0.638140\n",
      "Epoch [24/30], step:[40/44], loss: 0.637480\n",
      "Epoch [24/30], step:[40/44], loss: 0.636764\n",
      "Epoch [24/30], step:[40/44], loss: 0.636050\n",
      "Epoch [24/30], step:[40/44], loss: 0.635344\n",
      "Epoch [24/30], step:[40/44], loss: 0.634602\n",
      "Epoch [24/30], step:[40/44], loss: 0.633879\n",
      "Epoch [24/30], step:[40/44], loss: 0.633149\n",
      "Epoch [24/30], step:[40/44], loss: 0.632412\n",
      "Epoch [24/30], step:[40/44], loss: 0.631684\n",
      "Epoch [24/30], step:[40/44], loss: 0.630942\n",
      "Epoch [24/30], step:[40/44], loss: 0.630220\n",
      "Epoch [25/30], step:[40/44], loss: 0.630600\n",
      "Epoch [25/30], step:[40/44], loss: 0.630571\n",
      "Epoch [25/30], step:[40/44], loss: 0.630443\n",
      "Epoch [25/30], step:[40/44], loss: 0.630178\n",
      "Epoch [25/30], step:[40/44], loss: 0.629889\n",
      "Epoch [25/30], step:[40/44], loss: 0.629552\n",
      "Epoch [25/30], step:[40/44], loss: 0.629170\n",
      "Epoch [25/30], step:[40/44], loss: 0.628768\n",
      "Epoch [25/30], step:[40/44], loss: 0.628340\n",
      "Epoch [25/30], step:[40/44], loss: 0.627896\n",
      "Epoch [25/30], step:[40/44], loss: 0.627424\n",
      "Epoch [25/30], step:[40/44], loss: 0.626920\n",
      "Epoch [25/30], step:[40/44], loss: 0.626367\n",
      "Epoch [25/30], step:[40/44], loss: 0.625807\n",
      "Epoch [25/30], step:[40/44], loss: 0.625252\n",
      "Epoch [25/30], step:[40/44], loss: 0.624685\n",
      "Epoch [25/30], step:[40/44], loss: 0.624100\n",
      "Epoch [25/30], step:[40/44], loss: 0.623478\n",
      "Epoch [25/30], step:[40/44], loss: 0.622867\n",
      "Epoch [25/30], step:[40/44], loss: 0.622261\n",
      "Epoch [26/30], step:[40/44], loss: 0.641969\n",
      "Epoch [26/30], step:[40/44], loss: 0.641952\n",
      "Epoch [26/30], step:[40/44], loss: 0.641883\n",
      "Epoch [26/30], step:[40/44], loss: 0.641719\n",
      "Epoch [26/30], step:[40/44], loss: 0.641524\n",
      "Epoch [26/30], step:[40/44], loss: 0.641306\n",
      "Epoch [26/30], step:[40/44], loss: 0.641048\n",
      "Epoch [26/30], step:[40/44], loss: 0.640731\n",
      "Epoch [26/30], step:[40/44], loss: 0.640375\n",
      "Epoch [26/30], step:[40/44], loss: 0.640038\n",
      "Epoch [26/30], step:[40/44], loss: 0.639697\n",
      "Epoch [26/30], step:[40/44], loss: 0.639335\n",
      "Epoch [26/30], step:[40/44], loss: 0.638958\n",
      "Epoch [26/30], step:[40/44], loss: 0.638574\n",
      "Epoch [26/30], step:[40/44], loss: 0.638188\n",
      "Epoch [26/30], step:[40/44], loss: 0.637795\n",
      "Epoch [26/30], step:[40/44], loss: 0.637393\n",
      "Epoch [26/30], step:[40/44], loss: 0.636975\n",
      "Epoch [26/30], step:[40/44], loss: 0.636541\n",
      "Epoch [26/30], step:[40/44], loss: 0.636095\n",
      "Epoch [27/30], step:[40/44], loss: 0.655737\n",
      "Epoch [27/30], step:[40/44], loss: 0.655734\n",
      "Epoch [27/30], step:[40/44], loss: 0.655584\n",
      "Epoch [27/30], step:[40/44], loss: 0.655308\n",
      "Epoch [27/30], step:[40/44], loss: 0.654850\n",
      "Epoch [27/30], step:[40/44], loss: 0.654441\n",
      "Epoch [27/30], step:[40/44], loss: 0.653999\n",
      "Epoch [27/30], step:[40/44], loss: 0.653450\n",
      "Epoch [27/30], step:[40/44], loss: 0.652854\n",
      "Epoch [27/30], step:[40/44], loss: 0.652272\n",
      "Epoch [27/30], step:[40/44], loss: 0.651653\n",
      "Epoch [27/30], step:[40/44], loss: 0.650994\n",
      "Epoch [27/30], step:[40/44], loss: 0.650308\n",
      "Epoch [27/30], step:[40/44], loss: 0.649617\n",
      "Epoch [27/30], step:[40/44], loss: 0.648909\n",
      "Epoch [27/30], step:[40/44], loss: 0.648193\n",
      "Epoch [27/30], step:[40/44], loss: 0.647517\n",
      "Epoch [27/30], step:[40/44], loss: 0.646831\n",
      "Epoch [27/30], step:[40/44], loss: 0.646108\n",
      "Epoch [27/30], step:[40/44], loss: 0.645370\n",
      "Epoch [28/30], step:[40/44], loss: 0.639571\n",
      "Epoch [28/30], step:[40/44], loss: 0.639568\n",
      "Epoch [28/30], step:[40/44], loss: 0.639419\n",
      "Epoch [28/30], step:[40/44], loss: 0.639169\n",
      "Epoch [28/30], step:[40/44], loss: 0.638830\n",
      "Epoch [28/30], step:[40/44], loss: 0.638418\n",
      "Epoch [28/30], step:[40/44], loss: 0.637975\n",
      "Epoch [28/30], step:[40/44], loss: 0.637529\n",
      "Epoch [28/30], step:[40/44], loss: 0.637069\n",
      "Epoch [28/30], step:[40/44], loss: 0.636564\n",
      "Epoch [28/30], step:[40/44], loss: 0.636014\n",
      "Epoch [28/30], step:[40/44], loss: 0.635470\n",
      "Epoch [28/30], step:[40/44], loss: 0.634934\n",
      "Epoch [28/30], step:[40/44], loss: 0.634373\n",
      "Epoch [28/30], step:[40/44], loss: 0.633805\n",
      "Epoch [28/30], step:[40/44], loss: 0.633251\n",
      "Epoch [28/30], step:[40/44], loss: 0.632690\n",
      "Epoch [28/30], step:[40/44], loss: 0.632097\n",
      "Epoch [28/30], step:[40/44], loss: 0.631470\n",
      "Epoch [28/30], step:[40/44], loss: 0.630868\n",
      "Epoch [29/30], step:[40/44], loss: 0.632140\n",
      "Epoch [29/30], step:[40/44], loss: 0.632131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/30], step:[40/44], loss: 0.632031\n",
      "Epoch [29/30], step:[40/44], loss: 0.631872\n",
      "Epoch [29/30], step:[40/44], loss: 0.631655\n",
      "Epoch [29/30], step:[40/44], loss: 0.631380\n",
      "Epoch [29/30], step:[40/44], loss: 0.631049\n",
      "Epoch [29/30], step:[40/44], loss: 0.630691\n",
      "Epoch [29/30], step:[40/44], loss: 0.630315\n",
      "Epoch [29/30], step:[40/44], loss: 0.629889\n",
      "Epoch [29/30], step:[40/44], loss: 0.629441\n",
      "Epoch [29/30], step:[40/44], loss: 0.628983\n",
      "Epoch [29/30], step:[40/44], loss: 0.628512\n",
      "Epoch [29/30], step:[40/44], loss: 0.628045\n",
      "Epoch [29/30], step:[40/44], loss: 0.627573\n",
      "Epoch [29/30], step:[40/44], loss: 0.627095\n",
      "Epoch [29/30], step:[40/44], loss: 0.626601\n",
      "Epoch [29/30], step:[40/44], loss: 0.626077\n",
      "Epoch [29/30], step:[40/44], loss: 0.625542\n",
      "Epoch [29/30], step:[40/44], loss: 0.625014\n",
      "Epoch [30/30], step:[40/44], loss: 0.650448\n",
      "Epoch [30/30], step:[40/44], loss: 0.650438\n",
      "Epoch [30/30], step:[40/44], loss: 0.650221\n",
      "Epoch [30/30], step:[40/44], loss: 0.649948\n",
      "Epoch [30/30], step:[40/44], loss: 0.649536\n",
      "Epoch [30/30], step:[40/44], loss: 0.649005\n",
      "Epoch [30/30], step:[40/44], loss: 0.648452\n",
      "Epoch [30/30], step:[40/44], loss: 0.647847\n",
      "Epoch [30/30], step:[40/44], loss: 0.647289\n",
      "Epoch [30/30], step:[40/44], loss: 0.646699\n",
      "Epoch [30/30], step:[40/44], loss: 0.646054\n",
      "Epoch [30/30], step:[40/44], loss: 0.645406\n",
      "Epoch [30/30], step:[40/44], loss: 0.644727\n",
      "Epoch [30/30], step:[40/44], loss: 0.644056\n",
      "Epoch [30/30], step:[40/44], loss: 0.643354\n",
      "Epoch [30/30], step:[40/44], loss: 0.642632\n",
      "Epoch [30/30], step:[40/44], loss: 0.641902\n",
      "Epoch [30/30], step:[40/44], loss: 0.641191\n",
      "Epoch [30/30], step:[40/44], loss: 0.640490\n",
      "Epoch [30/30], step:[40/44], loss: 0.639792\n"
     ]
    }
   ],
   "source": [
    "# Training the Model\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_iter = iter(train_loader)\n",
    "    for i, (document, all_labels, labels, pis, y_ins) in enumerate(train_iter):\n",
    "        \n",
    "        document = Variable(document).float().to(device) \n",
    "        \n",
    "        labels = Variable(labels).float().to(device).view(-1, n)\n",
    "        pis = pis.view(-1, n)\n",
    "        y_ins = y_ins.view(-1, n)\n",
    "        \n",
    "        # todo: make this better i.e. better label embeddings that actually embed hierarchy\n",
    "        # ideas: skipgram/cbow uses context-word pair to create embeddings. try similarly for\n",
    "        # label hierarchy - i.e. parent-child relationship - use this to create label embeddings\n",
    "        # maybe this way l2_reg value will be low. right now it needs to be of atleast 1e-3 order\n",
    "        # to get nice curve. [OR] start with a fixed random vector initialization for the root node\n",
    "        # and then as you traverse through the tree add v.v.v.small amount of randomness to the children\n",
    "        # till you reach the leaf.\n",
    "        l2_reg = nn.parameter.Parameter(torch.mean(0.5*torch.sqrt(torch.sum((labels-pis)**2, dim=1))**2))*1e-3\n",
    "        \n",
    "        if type(optimizer) != torch.optim.LBFGS:            \n",
    "            optimizer.zero_grad()\n",
    "            w_xi = model(document, labels)\n",
    "            loss1 = criterion(w_xi, y_ins) + l2_reg\n",
    "            \n",
    "            if (i+1) % 40 == 0: \n",
    "                print ('Epoch [{}/{}], step:[{}/{}], loss: {:.6f}'.format(epoch+1, num_epochs, i+1, total_step, loss1.item()))\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            losses.append(loss1.item())\n",
    "            loss1.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        else:\n",
    "            def closure():               \n",
    "                optimizer.zero_grad()\n",
    "                w_xi = model(document, labels)\n",
    "                loss1 = criterion(w_xi, y_ins) \n",
    "                \n",
    "                if (i+1) % 40 == 0: \n",
    "                    print ('Epoch [{}/{}], step:[{}/{}], loss: {:.6f}'.format(epoch+1, num_epochs, i+1, total_step, loss1.item()))\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                losses.append(loss1.item())\n",
    "                loss1.backward()\n",
    "                return loss1\n",
    "            optimizer.step(closure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(losses);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.t().mm(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses[-469]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.save(model.state_dict(), 'train_small_model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for documents, _, labels in valid_data:\n",
    "        docs = Variable(torch.from_numpy(documents)).float()\n",
    "        outputs = model(docs)\n",
    "        print(torch.sum(torch.where(outputs>0.0001, torch.tensor(1), torch.tensor(0)), dim=0))\n",
    "        print(torch.sum(torch.where(labels>0, torch.tensor(1), torch.tensor(0)), dim=0))\n",
    "\n",
    "        umm, predicted = torch.max(outputs.data, 1)\n",
    "        print(umm.shape)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "\n",
    "    print('Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'train_valid_model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.utils as vutils\n",
    "from tensorboardX import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "c1 = plt.Circle((0.2, 0.5), 0.2, color='r')\n",
    "c2 = plt.Circle((0.8, 0.5), 0.2, color='r')\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.add_patch(c1)\n",
    "ax.add_patch(c2)\n",
    "plt.axis('scaled')\n",
    "\n",
    "\n",
    "# from tensorboardX import SummaryWriter\n",
    "# writer = SummaryWriter()\n",
    "writer.add_figure('matplotlib', fig)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #         Forward + Backward + Optimize\n",
    "#         def closure():\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(document)\n",
    "#             loss = criterion(outputs, torch.max(labels, 1)[0])\n",
    "# #             print('loss:', loss.item())\n",
    "#             loss.backward()\n",
    "#             return loss\n",
    "#         optimizer.step(closure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import umap # fit should get a sparse matrix\n",
    "%time trans = umap.UMAP(n_neighbors=5, random_state=42, n_components=32, verbose=True).fit(train_data.data)\n",
    "trans.embedding_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "167593,441685 160318:1 227881:1 255720:1 265934:1 432905:2 515946:1 538188:1 586136:1 610561:1 692683:1 \n",
    "                                        735075:1 828325:1 874107:1 898766:1 1087064:1 1354716:1 1432746:1 \n",
    "                                        1454292:1 1463839:1 1626714:1 1715083:1 1839104:1 1864180:1 2023750:1 \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
