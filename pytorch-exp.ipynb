{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/pinocookie/pytorch-dataset-and-dataloader/data\n",
    "# https://discuss.pytorch.org/t/runtimeerror-multi-target-not-supported-newbie/10216/4\n",
    "\n",
    "# Build the Dataset. We are going to generate a simple data set and then we will read it.\n",
    "# Build the DataLoader.\n",
    "# Build the model.\n",
    "# Define the loss function and the optimizer.\n",
    "# Train the model.\n",
    "# Generate predictions.\n",
    "# Plot the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import logging\n",
    "import numpy as np\n",
    "import igraph as ig\n",
    "import collections, gc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "from scripts.utils.hierarchy import *\n",
    "from scripts.utils.processing import *\n",
    "from scripts.utils.data_reading import *\n",
    "\n",
    "logging.basicConfig(level=logging.INFO )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 16 # wn vector size  --> ~log_{2}(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if (torch.cuda.is_available() and num_gpus > 0) else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "65333it [00:00, 247599.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# N, T_leaves & PI_parents have to be present globally! (list of all the labels)\n",
    "# one_hot_labels because I will keep accessing it for each document <1082>\n",
    "p2c_table, c2p_table, node2id, id2node, PI_parents, T_leaves, N = lookup_table(\"swiki/data/cat_hier.txt\", subset = False)\n",
    "graph_obj = hierarchy2graph(p2c_table, node2id)\n",
    "node2vec, w_pi_vec = hierarchy_vectors(graph_obj, id2node, p2c_table, c2p_table, n, device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50312/50312 [00:00<00:00, 153526.58it/s]\n"
     ]
    }
   ],
   "source": [
    "order_mapping = generate_order_mapping(N, False)\n",
    "rev_order_mapping = generate_order_mapping(N, True)\n",
    "binary_yin = generate_binary_yin(N, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_non_leaf_wn(label_id):\n",
    "    '''\n",
    "    accepts label ids only which are non-leaf nodes\n",
    "    '''\n",
    "    \n",
    "    assert label_id in N, \"{} is not a node\".format(label_id)    \n",
    "\n",
    "    C_ids = p2c_table[label_id]\n",
    "    Cn = len(C_ids)\n",
    "\n",
    "    w_n = node2vec[label_id]\n",
    "    w_pi = node2vec[c2p_table[label_id][0]]\n",
    "    sum_wc = 0.0\n",
    "\n",
    "    for idx in C_ids:\n",
    "        w_c = node2vec[idx]\n",
    "        sum_wc += w_c\n",
    "\n",
    "    Wn = 1/(Cn +1) * (w_pi + sum_wc)\n",
    "\n",
    "    return Wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetSWIKI(Dataset):\n",
    "    \n",
    "    def __init__(self, file_path, reduce = True, n_components = 128):\n",
    "        self.reduce = reduce\n",
    "        \n",
    "        self.n_components = n_components\n",
    "        \n",
    "        self.data, self.labels = lower_dim(file_path, reduce, n_components)\n",
    "             \n",
    "        self.labels = torch.as_tensor(self.labels, device = device, dtype = torch.float32)\n",
    "        \n",
    "        self.w_vec = node2vec\n",
    "        \n",
    "        self.w_pi_vec = w_pi_vec\n",
    "        \n",
    "        self.y_in = binary_yin\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        document = torch.as_tensor(self.data[index].todense(), device = device, dtype = torch.float32)\n",
    "       \n",
    "        label = self.labels[index]\n",
    "        \n",
    "        w_n = self.w_vec[label.item()]\n",
    "        \n",
    "        w_pi = self.w_pi_vec[label.item()]\n",
    "        \n",
    "        y_in = self.y_in[order_mapping[label.item()]]\n",
    "        \n",
    "        l2_reg = torch.sqrt(torch.sum((w_n-w_pi)**2)).to(device)\n",
    "\n",
    "        return document, label, w_n, y_in, l2_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = DatasetSWIKI(\"swiki/data/valid_remapped.txt\", reduce=False, n_components = n_components)\n",
    "valid_data = DatasetSWIKI(\"swiki/data/valid_small_remapped.txt\", reduce=False, n_components = n_components)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_data = DatasetSWIKI(\"swiki/data/test_remapped.txt\", reduce=False, n_components = n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252757"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 2085161), (1, 2085161))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.data[0].shape, valid_data.data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 251"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle = True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs shape on batch size = torch.Size([128, 1, 2085161]), torch.float32, cuda:0\n",
      "label shape on batch size = torch.Size([128]), torch.float32, cuda:0\n",
      "w_n shape on batch size = torch.Size([128, 16]), torch.float32, cuda:0\n",
      "y_in shape on batch size = torch.Size([128, 16]), torch.float32, cuda:0\n",
      "l2-reg shape on batch size = torch.Size([128]), torch.float32, cuda:0\n"
     ]
    }
   ],
   "source": [
    "train_iter = iter(train_loader)\n",
    "\n",
    "doc, labs, w_n, y_in, l2 = train_iter.next()\n",
    "\n",
    "print('docs shape on batch size = {}, {}, {}'.format(doc.shape, doc.dtype, doc.device))\n",
    "print('label shape on batch size = {}, {}, {}'.format(labs.shape, labs.dtype, labs.device))\n",
    "print('w_n shape on batch size = {}, {}, {}'.format(w_n.shape, w_n.dtype, w_n.device))\n",
    "print('y_in shape on batch size = {}, {}, {}'.format(y_in.shape, y_in.dtype, y_in.device))\n",
    "print('l2-reg shape on batch size = {}, {}, {}'.format(l2.shape, l2.dtype, l2.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters \n",
    "# input_size = train_data.data.shape[1] #128 n_components\n",
    "input_size = train_data.data[0].shape[1] #2085164 \n",
    "\n",
    "num_classes = n #50312 --> n (16)\n",
    "num_epochs = 10 # TRAIN IT FOR A LOT OF EPOCHS in case of lbfgs (2nd order method) else less is more\n",
    "learning_rate = 0.0001 #1e-4, 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7402"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.cudnn.version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, num_classes, False)\n",
    "        \n",
    "    def forward(self, x, wn):\n",
    "        x1 = self.linear1(x)\n",
    "        y =  torch.mul(-x1, wn)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.SoftMarginLoss(reduction='mean').to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "logger = Logger('./logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_step = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1007"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"batch_{}_{}_{}_train_valid_model.pt\".format(batch_size, learning_rate, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    losses = checkpoint['losses']\n",
    "    step = checkpoint['step']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 99/1007 [04:03<36:59,  2.44s/it]INFO:root:Epoch [1/10], step:[100/1007], loss: 0.684101\n",
      " 20%|█▉        | 199/1007 [08:07<32:49,  2.44s/it]INFO:root:Epoch [1/10], step:[200/1007], loss: 0.679254\n",
      " 25%|██▌       | 254/1007 [10:22<30:43,  2.45s/it]"
     ]
    }
   ],
   "source": [
    "# Training the Model\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "losses = []\n",
    "epoch = 0\n",
    "for step in range(num_epochs):  \n",
    "    for document, _, labels, y_ins, l2_reg in tqdm(train_loader):\n",
    "\n",
    "        document = document.reshape(batch_size,-1)\n",
    "        \n",
    "        optimizer.param_groups[0]['weight_decay'] = torch.mean(l2_reg)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        w_xi = model.forward(document, labels)\n",
    "        loss1 = criterion(w_xi, y_ins)\n",
    "        \n",
    "        if (epoch+1) % 100 == 0: \n",
    "            logging.info('Epoch [{}/{}], step:[{}/{}], loss: {:.6f}'.format(step+1, num_epochs, epoch+1, total_step, loss1.item()))\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "        losses.append(loss1.item())\n",
    "        loss1.backward()\n",
    "        optimizer.step()\n",
    "        epoch += 1\n",
    "#         if type(optimizer) != torch.optim.LBFGS:\n",
    "#         else:\n",
    "#             def closure():               \n",
    "#                 optimizer.zero_grad()\n",
    "#                 w_xi = model(document, labels)\n",
    "#                 loss1 = criterion(w_xi, y_ins) + l2_reg\n",
    "                \n",
    "#                 if (step+1) % 100 == 0: \n",
    "#                     print ('Epoch [{}/{}], step:[{}/{}], loss: {:.6f}'.format(epoch+1, num_epochs, step+1, total_step, loss1.item()))\n",
    "#                     torch.cuda.empty_cache()\n",
    "                \n",
    "#                 losses.append(loss1.item())\n",
    "#                 loss1.backward()\n",
    "#                 return loss1\n",
    "#             optimizer.step(closure)\n",
    "\n",
    "    torch.save({\n",
    "    'epoch': step,\n",
    "    'step': epoch,\n",
    "    'losses': losses,\n",
    "    'node2vec': node2vec,\n",
    "    'learning_rate': learning_rate,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': loss1}, checkpoint_path)\n",
    "    plt.plot(losses);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save node2vec also as a pkl file or something to retrieve original classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(losses);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converter(input_, mode_in=\"binary\", mode_out=\"decimal\"):\n",
    "    \n",
    "    if mode_in == \"binary\" and mode_out == \"decimal\":\n",
    "        a2str = np.array2string(input_, separator='')\n",
    "        width = len(a2str)-1\n",
    "        a2str = a2str[1:width]\n",
    "        decimal = int(a2str, 2)\n",
    "    \n",
    "    return decimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for document, all_label, labels, _, y_ins, _ in valid_data:\n",
    "        \n",
    "        # change document size\n",
    "        if document.shape[1] != input_size:\n",
    "            docx = document.view(-1)\n",
    "            zeros = torch.zeros((input_size-docx.shape[0],), device=device, dtype=torch.float32)\n",
    "            document = torch.cat((docx, zeros),0).view(-1,)\n",
    "        \n",
    "        output = model(document, labels)\n",
    "        \n",
    "        \n",
    "        print((output))\n",
    "        trans_out = torch.where(torch.tanh(output)>=0, torch.tensor(1, device=device), torch.tensor(0, device=device)).view(-1,)\n",
    "        trans_yin = torch.where(y_ins>0, torch.tensor(1, device=device), torch.tensor(0, device=device))\n",
    "        o = trans_out.cpu().numpy()\n",
    "        print(trans_out)\n",
    "        dec_o = converter(input_ = o)\n",
    "        y = trans_yin.cpu().numpy()\n",
    "        print(rev_order_mapping[dec_o])\n",
    "        print(int(all_label.cpu().numpy()))\n",
    "        # todo: multilabel prediction\n",
    "        # todo: see how much it differs from the original (root node) or something else\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'test_valid_model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import umap # fit should get a sparse matrix\n",
    "%time trans = umap.UMAP(n_neighbors=5, random_state=42, n_components=32, verbose=True).fit(train_data.data)\n",
    "trans.embedding_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
